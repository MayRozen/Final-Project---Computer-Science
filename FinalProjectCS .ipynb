{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "cell_execution_strategy": "setup"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# extract_data\n"
      ],
      "metadata": {
        "id": "CeCXF8rG9JdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# הנתיב לתיקייה שנוצרה בהרצה הקודמת\n",
        "destination_folder = \"/content/vctk_full\"\n",
        "\n",
        "# בדיקה אם התיקייה קיימת, ואז מחיקה\n",
        "if os.path.exists(destination_folder):\n",
        "    shutil.rmtree(destination_folder)\n",
        "    print(f\" התיקייה '{destination_folder}' נמחקה בהצלחה.\")\n",
        "else:\n",
        "    print(f\"ℹ התיקייה '{destination_folder}' לא קיימת, אין מה למחוק.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsB_qTqinrA3",
        "outputId": "b226832c-eee5-4fe2-cc74-628ce23ad6e6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ℹ התיקייה '/content/vctk_full' לא קיימת, אין מה למחוק.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# List contents to verify paths (optional)\n",
        "root_path = '/content/drive/My Drive/'\n",
        "print(\"Contents of 'My Drive':\", os.listdir(root_path))\n",
        "\n",
        "subfolder_path = '/content/drive/My Drive/Colab Notebooks/'\n",
        "print(\"Contents of 'Colab Notebooks':\", os.listdir(subfolder_path))\n",
        "\n",
        "# Define paths\n",
        "zip_file = \"/content/drive/My Drive/Colab Notebooks/archive.zip\"  # Path to your ZIP file\n",
        "destination_folder = \"/content/vctk_samples\"  # Where to extract selected data\n",
        "wanted_speakers = [\"p225\", \"p226\", \"p227\", \"p228\"]  # Select specific speakers\n",
        "\n",
        "# Check if the ZIP file exists\n",
        "if os.path.isfile(zip_file):\n",
        "    print(\" ZIP file found:\", zip_file)\n",
        "else:\n",
        "    raise FileNotFoundError(f\" ZIP file not found: {zip_file}\")\n",
        "\n",
        "# Create destination folder if it doesn't exist\n",
        "os.makedirs(destination_folder, exist_ok=True)\n",
        "\n",
        "# Selectively extract only desired speaker folders from the ZIP\n",
        "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "    extracted_files = 0\n",
        "    for file in zip_ref.namelist():\n",
        "        if any(f\"VCTK-Corpus/wav48/{spk}/\" in file or f\"VCTK-Corpus/txt/{spk}/\" in file for spk in wanted_speakers):\n",
        "            # Ensure directory structure is preserved\n",
        "            target_path = os.path.join(destination_folder, file)\n",
        "            os.makedirs(os.path.dirname(target_path), exist_ok=True)\n",
        "            with zip_ref.open(file) as source, open(target_path, 'wb') as target:\n",
        "                shutil.copyfileobj(source, target)\n",
        "            extracted_files += 1\n",
        "\n",
        "print(f\"\\n Extraction complete: {extracted_files} files were extracted.\")\n",
        "print(f\" Extracted data is available in: {destination_folder}\")\n"
      ],
      "metadata": {
        "id": "9YxlgeQE9ufN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bb786d7-6d3d-406e-ea02-48e7349ad61d",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Contents of 'My Drive': ['Colab Notebooks']\n",
            "Contents of 'Colab Notebooks': ['archive.zip', 'Untitled', 'FinalProjectCS.ipynb']\n",
            " ZIP file found: /content/drive/My Drive/Colab Notebooks/archive.zip\n",
            "\n",
            " Extraction complete: 2684 files were extracted.\n",
            " Extracted data is available in: /content/vctk_samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#data_preprocessing"
      ],
      "metadata": {
        "id": "Yvl1fT6PAs-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# generate_fake_data"
      ],
      "metadata": {
        "id": "zB_QfTQcEA-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy==1.24.3 pandas==2.2.2 networkx==2.8.8 TTS==0.15.2"
      ],
      "metadata": {
        "id": "n0ByRuEDgQME",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97130b97-ae50-47db-9873-0a82ddb30a65"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.24.3 in /usr/local/lib/python3.11/dist-packages (1.24.3)\n",
            "Requirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: networkx==2.8.8 in /usr/local/lib/python3.11/dist-packages (2.8.8)\n",
            "Requirement already satisfied: TTS==0.15.2 in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.2)\n",
            "Requirement already satisfied: cython==0.29.30 in /usr/local/lib/python3.11/dist-packages (from TTS==0.15.2) (0.29.30)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from TTS==0.15.2) (1.15.2)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.11/dist-packages (from TTS==0.15.2) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (from TTS==0.15.2) (2.6.0+cu124)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (from TTS==0.15.2) (0.13.1)\n",
            "Requirement already satisfied: librosa==0.10.0.* in /usr/local/lib/python3.11/dist-packages (from TTS==0.15.2) (0.10.0.post2)\n",
            "Requirement already satisfied: inflect==5.6.0 in /usr/local/lib/python3.11/dist-packages (from TTS==0.15.2) (5.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from TTS==0.15.2) (4.67.1)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from TTS==0.15.2) (0.3.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from TTS==0.15.2) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2021.04.0 in /usr/local/lib/python3.11/dist-packages (from TTS==0.15.2) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from TTS==0.15.2) (3.11.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from TTS==0.15.2) (24.2)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (from TTS==0.15.2) (3.1.0)\n",
            "Requirement already satisfied: pysbd in /usr/local/lib/python3.11/dist-packages (from TTS==0.15.2) (0.3.4)\n",
            "Requirement already satisfied: umap-learn==0.5.1 in /usr/local/lib/python3.11/dist-packages (from TTS==0.15.2) (0.5.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from TTS==0.15.2) (3.10.0)\n",
            "Requirement already satisfied: trainer in /usr/local/lib/python3.11/dist-packages (from TTS==0.15.2) (0.0.36)\n",
            "Requirement already satisfied: coqpit>=0.0.16 in /usr/local/lib/python3.11/dist-packages (from TTS==0.15.2) (0.0.17)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.11/dist-packages (from TTS==0.15.2) (0.42.1)\n",
            "Requirement already satisfied: pypinyin in /usr/local/lib/python3.11/dist-packages (from TTS==0.15.2) (0.54.0)\n",
            "Requirement already satisfied: mecab-python3==1.0.6 in /usr/local/lib/python3.11/dist-packages (from TTS==0.15.2) (1.0.6)\n",
            "Requirement already satisfied: unidic-lite==1.0.8 in /usr/local/lib/python3.11/dist-packages (from TTS==0.15.2) (1.0.8)\n",
            "Requirement already satisfied: gruut==2.2.3 in /usr/local/lib/python3.11/dist-packages (from gruut[de,es,fr]==2.2.3->TTS==0.15.2) (2.2.3)\n",
            "Requirement already satisfied: jamo in /usr/local/lib/python3.11/dist-packages (from TTS==0.15.2) (0.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from TTS==0.15.2) (3.9.1)\n",
            "Requirement already satisfied: g2pkk>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from TTS==0.15.2) (0.1.2)\n",
            "Requirement already satisfied: bangla==0.0.2 in /usr/local/lib/python3.11/dist-packages (from TTS==0.15.2) (0.0.2)\n",
            "Requirement already satisfied: bnnumerizer in /usr/local/lib/python3.11/dist-packages (from TTS==0.15.2) (0.0.2)\n",
            "Requirement already satisfied: bnunicodenormalizer==0.1.1 in /usr/local/lib/python3.11/dist-packages (from TTS==0.15.2) (0.1.1)\n",
            "Requirement already satisfied: k-diffusion in /usr/local/lib/python3.11/dist-packages (from TTS==0.15.2) (0.1.1.post1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from TTS==0.15.2) (0.8.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from TTS==0.15.2) (4.51.3)\n",
            "Requirement already satisfied: encodec in /usr/local/lib/python3.11/dist-packages (from TTS==0.15.2) (0.1.1)\n",
            "Requirement already satisfied: numba==0.57.0 in /usr/local/lib/python3.11/dist-packages (from TTS==0.15.2) (0.57.0)\n",
            "Requirement already satisfied: Babel<3.0.0,>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS==0.15.2) (2.17.0)\n",
            "Requirement already satisfied: dateparser~=1.1.0 in /usr/local/lib/python3.11/dist-packages (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS==0.15.2) (1.1.8)\n",
            "Requirement already satisfied: gruut-ipa<1.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS==0.15.2) (0.13.0)\n",
            "Requirement already satisfied: gruut-lang-en~=2.0.0 in /usr/local/lib/python3.11/dist-packages (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS==0.15.2) (2.0.1)\n",
            "Requirement already satisfied: jsonlines~=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS==0.15.2) (1.2.0)\n",
            "Requirement already satisfied: num2words<1.0.0,>=0.5.10 in /usr/local/lib/python3.11/dist-packages (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS==0.15.2) (0.5.14)\n",
            "Requirement already satisfied: python-crfsuite~=0.9.7 in /usr/local/lib/python3.11/dist-packages (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS==0.15.2) (0.9.11)\n",
            "Requirement already satisfied: gruut-lang-de~=2.0.0 in /usr/local/lib/python3.11/dist-packages (from gruut[de,es,fr]==2.2.3->TTS==0.15.2) (2.0.1)\n",
            "Requirement already satisfied: gruut-lang-es~=2.0.0 in /usr/local/lib/python3.11/dist-packages (from gruut[de,es,fr]==2.2.3->TTS==0.15.2) (2.0.1)\n",
            "Requirement already satisfied: gruut-lang-fr~=2.0.0 in /usr/local/lib/python3.11/dist-packages (from gruut[de,es,fr]==2.2.3->TTS==0.15.2) (2.0.2)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa==0.10.0.*->TTS==0.15.2) (3.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from librosa==0.10.0.*->TTS==0.15.2) (1.6.1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.11/dist-packages (from librosa==0.10.0.*->TTS==0.15.2) (1.5.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa==0.10.0.*->TTS==0.15.2) (4.4.2)\n",
            "Requirement already satisfied: pooch<1.7,>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa==0.10.0.*->TTS==0.15.2) (1.6.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa==0.10.0.*->TTS==0.15.2) (0.5.0.post1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa==0.10.0.*->TTS==0.15.2) (4.13.2)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa==0.10.0.*->TTS==0.15.2) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa==0.10.0.*->TTS==0.15.2) (1.1.0)\n",
            "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba==0.57.0->TTS==0.15.2) (0.40.1)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn==0.5.1->TTS==0.15.2) (0.5.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile->TTS==0.15.2) (1.17.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->TTS==0.15.2) (3.18.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->TTS==0.15.2) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->TTS==0.15.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->TTS==0.15.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->TTS==0.15.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->TTS==0.15.2) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->TTS==0.15.2) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->TTS==0.15.2) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->TTS==0.15.2) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->TTS==0.15.2) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->TTS==0.15.2) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->TTS==0.15.2) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->TTS==0.15.2) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->TTS==0.15.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->TTS==0.15.2) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->TTS==0.15.2) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->TTS==0.15.2) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.7->TTS==0.15.2) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->TTS==0.15.2) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->TTS==0.15.2) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->TTS==0.15.2) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->TTS==0.15.2) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->TTS==0.15.2) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->TTS==0.15.2) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->TTS==0.15.2) (1.20.0)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask->TTS==0.15.2) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask->TTS==0.15.2) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask->TTS==0.15.2) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask->TTS==0.15.2) (1.9.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from k-diffusion->TTS==0.15.2) (1.6.0)\n",
            "Requirement already satisfied: clean-fid in /usr/local/lib/python3.11/dist-packages (from k-diffusion->TTS==0.15.2) (0.1.35)\n",
            "Requirement already satisfied: clip-anytorch in /usr/local/lib/python3.11/dist-packages (from k-diffusion->TTS==0.15.2) (2.6.0)\n",
            "Requirement already satisfied: dctorch in /usr/local/lib/python3.11/dist-packages (from k-diffusion->TTS==0.15.2) (0.1.2)\n",
            "Requirement already satisfied: jsonmerge in /usr/local/lib/python3.11/dist-packages (from k-diffusion->TTS==0.15.2) (1.9.2)\n",
            "Requirement already satisfied: kornia in /usr/local/lib/python3.11/dist-packages (from k-diffusion->TTS==0.15.2) (0.8.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from k-diffusion->TTS==0.15.2) (11.2.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from k-diffusion->TTS==0.15.2) (0.5.3)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from k-diffusion->TTS==0.15.2) (0.24.0)\n",
            "Requirement already satisfied: torchdiffeq in /usr/local/lib/python3.11/dist-packages (from k-diffusion->TTS==0.15.2) (0.2.5)\n",
            "Requirement already satisfied: torchsde in /usr/local/lib/python3.11/dist-packages (from k-diffusion->TTS==0.15.2) (0.2.6)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from k-diffusion->TTS==0.15.2) (0.21.0+cu124)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (from k-diffusion->TTS==0.15.2) (0.19.11)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->TTS==0.15.2) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->TTS==0.15.2) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->TTS==0.15.2) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->TTS==0.15.2) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->TTS==0.15.2) (3.2.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->TTS==0.15.2) (2024.11.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from trainer->TTS==0.15.2) (5.9.5)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from trainer->TTS==0.15.2) (2.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers->TTS==0.15.2) (0.31.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers->TTS==0.15.2) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->TTS==0.15.2) (0.21.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile->TTS==0.15.2) (2.22)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.11/dist-packages (from dateparser~=1.1.0->gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS==0.15.2) (5.3.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers->TTS==0.15.2) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.7->TTS==0.15.2) (3.0.2)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.11/dist-packages (from num2words<1.0.0,>=0.5.10->gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS==0.15.2) (0.6.2)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from pooch<1.7,>=1.0->librosa==0.10.0.*->TTS==0.15.2) (1.4.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->TTS==0.15.2) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->TTS==0.15.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->TTS==0.15.2) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->TTS==0.15.2) (2025.4.26)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->librosa==0.10.0.*->TTS==0.15.2) (3.6.0)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip-anytorch->k-diffusion->TTS==0.15.2) (6.3.1)\n",
            "Requirement already satisfied: jsonschema>2.4.0 in /usr/local/lib/python3.11/dist-packages (from jsonmerge->k-diffusion->TTS==0.15.2) (4.23.0)\n",
            "Requirement already satisfied: kornia_rs>=0.1.9 in /usr/local/lib/python3.11/dist-packages (from kornia->k-diffusion->TTS==0.15.2) (0.1.9)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->k-diffusion->TTS==0.15.2) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->k-diffusion->TTS==0.15.2) (2025.3.30)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard->trainer->TTS==0.15.2) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard->trainer->TTS==0.15.2) (1.71.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard->trainer->TTS==0.15.2) (3.8)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard->trainer->TTS==0.15.2) (5.29.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->trainer->TTS==0.15.2) (75.2.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->trainer->TTS==0.15.2) (0.7.2)\n",
            "Requirement already satisfied: trampoline>=0.1.2 in /usr/local/lib/python3.11/dist-packages (from torchsde->k-diffusion->TTS==0.15.2) (0.1.2)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb->k-diffusion->TTS==0.15.2) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->k-diffusion->TTS==0.15.2) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb->k-diffusion->TTS==0.15.2) (4.3.8)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb->k-diffusion->TTS==0.15.2) (2.11.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->k-diffusion->TTS==0.15.2) (2.27.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb->k-diffusion->TTS==0.15.2) (1.3.6)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->k-diffusion->TTS==0.15.2) (4.0.12)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>2.4.0->jsonmerge->k-diffusion->TTS==0.15.2) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>2.4.0->jsonmerge->k-diffusion->TTS==0.15.2) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>2.4.0->jsonmerge->k-diffusion->TTS==0.15.2) (0.24.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb->k-diffusion->TTS==0.15.2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb->k-diffusion->TTS==0.15.2) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb->k-diffusion->TTS==0.15.2) (0.4.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip-anytorch->k-diffusion->TTS==0.15.2) (0.2.13)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->k-diffusion->TTS==0.15.2) (5.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y espeak-ng"
      ],
      "metadata": {
        "id": "nymZzYIfrL7N",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a24357c9-01dc-4814-e584-a4ca4970bd89"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "espeak-ng is already the newest version (1.50+dfsg-10ubuntu0.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from TTS.api import TTS\n",
        "import shutil\n",
        "\n",
        "fake_audio_folder = \"/content/vctk_samples/VCTK-Corpus/VCTK-Corpus/fake_wav48\"\n",
        "# פונקציה למחיקת כל קבצי האודיו שנוצרו\n",
        "def clear_fake_audio_folder(fake_audio_folder):\n",
        "    if os.path.exists(fake_audio_folder):\n",
        "        shutil.rmtree(fake_audio_folder)  # מוחק את כל התיקייה כולל הקבצים שבה\n",
        "        os.makedirs(fake_audio_folder, exist_ok=True)  # יוצר מחדש את התיקייה הריקה\n",
        "        print(f\" כל הקבצים בתיקייה '{fake_audio_folder}' נמחקו!\")\n",
        "\n",
        "clear_fake_audio_folder(fake_audio_folder)\n",
        "quit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfJgcbcSqmrh",
        "outputId": "a1cb7a23-2d62-425e-d485-313bb5e157d5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " כל הקבצים בתיקייה '/content/vctk_samples/VCTK-Corpus/VCTK-Corpus/fake_wav48' נמחקו!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from TTS.api import TTS\n",
        "import shutil\n",
        "\n",
        "# Define input and output directories\n",
        "text_folder = \"/content/vctk_samples/VCTK-Corpus/VCTK-Corpus/txt\"  # Source text files\n",
        "fake_audio_folder = \"/content/vctk_samples/VCTK-Corpus/VCTK-Corpus/fake_wav48\"  # Destination for generated speech\n",
        "os.makedirs(fake_audio_folder, exist_ok=True)\n",
        "\n",
        "# Load the TTS model (pre-trained model)\n",
        "tts = TTS(model_name=\"tts_models/en/vctk/vits\", progress_bar=True)\n",
        "\n",
        "# Define a dictionary for speaker characteristics\n",
        "# The key is the speaker ID (e.g., \"225\"), and the value is a tuple with gender, age, and accent\n",
        "speaker_info = {\n",
        "    \"225\": (\"F\", \"22\", \"Southern England\"),\n",
        "    \"226\": (\"M\", \"22\", \"Surrey\"),\n",
        "    \"227\": (\"M\", \"38\", \"Cumbria\"),\n",
        "    \"228\": (\"F\", \"22\", \"Southern England\"),\n",
        "}\n",
        "\n",
        "# Iterate through each subfolder in the text folder\n",
        "for subdir in os.listdir(text_folder):\n",
        "    subdir_path = os.path.join(text_folder, subdir)\n",
        "\n",
        "    # Ensure it's a directory\n",
        "    if not os.path.isdir(subdir_path):\n",
        "        continue\n",
        "\n",
        "    # Create a corresponding subfolder in fake_audio_folder\n",
        "    fake_subdir = os.path.join(fake_audio_folder, subdir)\n",
        "    os.makedirs(fake_subdir, exist_ok=True)\n",
        "\n",
        "    # Process each text file in the subfolder\n",
        "    for filename in os.listdir(subdir_path):\n",
        "        if filename.endswith(\".txt\"):  # Ensure it's a text file\n",
        "            text_file_path = os.path.join(subdir_path, filename)\n",
        "\n",
        "            # Read text content\n",
        "            with open(text_file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "                text = file.read().strip()\n",
        "\n",
        "            # Extract speaker ID from filename (assuming filenames are like '225_01.txt')\n",
        "            speaker_id = filename.split('_')[0]  # Extract the ID (e.g., \"225\")\n",
        "            if speaker_id in speaker_info:\n",
        "                gender, age, accent = speaker_info[speaker_id]\n",
        "            else:\n",
        "                gender, age, accent = \"M\", \"30\", \"Neutral\"  # Default values if not found\n",
        "\n",
        "            # Here you can modify the model parameters based on gender, age, or accent\n",
        "            # For example, you can pass a specific model for each speaker or change the prosody.\n",
        "\n",
        "            # Generate fake audio file path\n",
        "            output_path = os.path.join(fake_subdir, filename.replace(\".txt\", \".wav\"))\n",
        "\n",
        "            # Generate speech and save to file\n",
        "            tts.tts_to_file(text=text, speaker=f\"{speaker_id}\", file_path=output_path)\n",
        "\n",
        "            # You can log the gender, age, and accent for verification\n",
        "            # print(f\"Generated: {output_path} (Speaker ID: {speaker_id}, Gender: {gender}, Age: {age}, Accent: {accent})\")\n",
        "\n",
        "# print(f\"Fake audio saved in {fake_audio_folder}\")"
      ],
      "metadata": {
        "id": "1ZDmhBIjEHQW",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f557b85-d7ea-4b14-e958-7f4fc2d60a95"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " > Processing time: 5.001868724822998\n",
            " > Real-time factor: 1.3460652873260484\n",
            " > Text splitted to sentences.\n",
            "['We believe the world has too many refugees already.']\n",
            " > Processing time: 5.026058673858643\n",
            " > Real-time factor: 1.4524087041122755\n",
            " > Text splitted to sentences.\n",
            "['Six spoons of fresh snow peas, five thick slabs of blue cheese, and maybe a snack for her brother Bob.']\n",
            " > Processing time: 10.948988676071167\n",
            " > Real-time factor: 1.4507331044332832\n",
            " > Text splitted to sentences.\n",
            "['No other vehicle was involved in the crash.']\n",
            " > Processing time: 3.8437747955322266\n",
            " > Real-time factor: 1.2829261661644102\n",
            " > Text splitted to sentences.\n",
            "[\"I didn't feel under any pressure.\"]\n",
            " > Processing time: 2.2081100940704346\n",
            " > Real-time factor: 0.9366117954419261\n",
            " > Text splitted to sentences.\n",
            "['Both sides have been hit by injury.']\n",
            " > Processing time: 2.415067672729492\n",
            " > Real-time factor: 0.9452613281682282\n",
            " > Text splitted to sentences.\n",
            "['Neither side can win this war.']\n",
            " > Processing time: 2.3545351028442383\n",
            " > Real-time factor: 0.9794276150338714\n",
            " > Text splitted to sentences.\n",
            "['Well, they were the unlucky ones.']\n",
            " > Processing time: 4.511049032211304\n",
            " > Real-time factor: 1.5726763085039723\n",
            " > Text splitted to sentences.\n",
            "['It is the latter which has prompted the warning.']\n",
            " > Processing time: 3.354114055633545\n",
            " > Real-time factor: 1.1646229359838698\n",
            " > Text splitted to sentences.\n",
            "['I said he was set to retire.']\n",
            " > Processing time: 2.1304967403411865\n",
            " > Real-time factor: 0.9604484201121025\n",
            " > Text splitted to sentences.\n",
            "['Why change it ?']\n",
            " > Processing time: 1.4079887866973877\n",
            " > Real-time factor: 0.8848082748141074\n",
            " > Text splitted to sentences.\n",
            "['Nothing has been put into action.']\n",
            " > Processing time: 2.267322063446045\n",
            " > Real-time factor: 0.9523478264817375\n",
            " > Text splitted to sentences.\n",
            "['Sadly, the revival could not be sustained.']\n",
            " > Processing time: 4.81392240524292\n",
            " > Real-time factor: 1.5240493486619342\n",
            " > Text splitted to sentences.\n",
            "['The squad is too small.']\n",
            " > Processing time: 1.9075157642364502\n",
            " > Real-time factor: 0.9604658979131743\n",
            " > Text splitted to sentences.\n",
            "['Treatment is not an issue with these people.']\n",
            " > Processing time: 3.2891528606414795\n",
            " > Real-time factor: 1.1850624277311212\n",
            " > Text splitted to sentences.\n",
            "['The confidence is low, but it is a difficult thing to understand.']\n",
            " > Processing time: 5.5199134349823\n",
            " > Real-time factor: 1.302367865534152\n",
            " > Text splitted to sentences.\n",
            "['I can hardly believe it.']\n",
            " > Processing time: 3.0466959476470947\n",
            " > Real-time factor: 1.5163336413330273\n",
            " > Text splitted to sentences.\n",
            "[\"That's as it should be.\"]\n",
            " > Processing time: 1.828434705734253\n",
            " > Real-time factor: 0.9100077930083126\n",
            " > Text splitted to sentences.\n",
            "['They know no other way.']\n",
            " > Processing time: 1.5983188152313232\n",
            " > Real-time factor: 0.8994214443612362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# הנתיב לקובץ שברצונך להוריד למחשב שלך\n",
        "file_path = \"/content/vctk_samples/VCTK-Corpus/VCTK-Corpus/fake_wav48\"\n",
        "\n",
        "# הורדת הקובץ\n",
        "files.download(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "ATqG7ZXl74D8",
        "outputId": "d84c2d3a-a692-4366-8f83-bff2d1edc26e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_fa39bfc1-005f-467e-bf0f-e99a167ae3f7\", \"fake_wav48\", 4096)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rotem"
      ],
      "metadata": {
        "id": "SDlMCAzdKCP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "\n",
        "def compute_lfcc(waveform, sr,\n",
        "                 n_fft=512,\n",
        "                 n_filter=80,\n",
        "                 n_coeff=80,\n",
        "                 hop_length=160,\n",
        "                 win_length=400):\n",
        "    # 1) power spectrogram\n",
        "    S = librosa.stft(waveform,\n",
        "                     n_fft=n_fft,\n",
        "                     hop_length=hop_length,\n",
        "                     win_length=win_length,\n",
        "                     window='hann')\n",
        "    S_mag = np.abs(S)**2\n",
        "\n",
        "    # 2) keep first n_filter bins\n",
        "    if S_mag.shape[0] < n_filter:\n",
        "        S_mag = np.pad(S_mag,\n",
        "                       ((0, n_filter - S_mag.shape[0]), (0, 0)),\n",
        "                       mode='constant')\n",
        "    S_lin = S_mag[:n_filter, :]\n",
        "\n",
        "    # 3) log\n",
        "    S_log = np.log10(S_lin + 1e-8)\n",
        "\n",
        "    # 4) DCT → LFCC\n",
        "    mfcc = librosa.feature.mfcc(sr=sr,\n",
        "                                S=S_log,\n",
        "                                n_mfcc=n_coeff,\n",
        "                                dct_type=2,\n",
        "                                norm='ortho')\n",
        "    return mfcc  # (n_coeff, time_frames)"
      ],
      "metadata": {
        "id": "ksAQALHYOZ6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torchaudio\n",
        "import librosa\n",
        "\n",
        "class VCTKDeepfakeDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 data_dir,\n",
        "                 speakers=None,\n",
        "                 mode=\"train\",\n",
        "                 train_ratio=0.7,\n",
        "                 val_ratio=0.15,\n",
        "                 segment_length=4.0,\n",
        "                 sr=16000):\n",
        "        self.sr = sr\n",
        "        self.segment_length = segment_length\n",
        "\n",
        "        # point _directly_ at vctk_samples, which has subfolders p225–p228\n",
        "        wav_root = data_dir\n",
        "\n",
        "        # choose which speakers to use\n",
        "        if speakers is None:\n",
        "            speakers = [d for d in os.listdir(wav_root)\n",
        "                        if d.startswith(\"p\")]\n",
        "        all_files = []\n",
        "        for spk in speakers:\n",
        "            spk_dir = os.path.join(wav_root, spk)\n",
        "            for fn in os.listdir(spk_dir):\n",
        "                if fn.endswith(\".wav\"):\n",
        "                    all_files.append(os.path.join(spk_dir, fn))\n",
        "        all_files.sort()\n",
        "        if not all_files:\n",
        "            raise RuntimeError(f\"No .wav files found in {wav_root} for {speakers!r}\")\n",
        "\n",
        "        # split train / val / test\n",
        "        n = len(all_files)\n",
        "        n_train = int(n * train_ratio)\n",
        "        n_val   = int(n * val_ratio)\n",
        "        if mode == \"train\":\n",
        "            self.real_files = all_files[:n_train]\n",
        "        elif mode == \"val\":\n",
        "            self.real_files = all_files[n_train:n_train+n_val]\n",
        "        elif mode == \"test\":\n",
        "            self.real_files = all_files[n_train+n_val:]\n",
        "        else:\n",
        "            raise ValueError(\"mode must be train/val/test\")\n",
        "        if not self.real_files:\n",
        "            raise RuntimeError(f\"After split, {mode!r} has 0 files (total was {n})\")\n",
        "\n",
        "        # we'll mirror real→fake one-to-one\n",
        "        self.n = len(self.real_files)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n * 2\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # pick real or fake\n",
        "        if idx < self.n:\n",
        "            path, label = self.real_files[idx], 0\n",
        "        else:\n",
        "            path, label = self.real_files[idx - self.n], 1\n",
        "\n",
        "        # load + fix length\n",
        "        wav, _ = librosa.load(path, sr=self.sr)\n",
        "        tgt = int(self.segment_length * self.sr)\n",
        "        if len(wav) > tgt:\n",
        "            wav = wav[:tgt]\n",
        "        else:\n",
        "            wav = np.pad(wav, (0, tgt-len(wav)), mode=\"constant\")\n",
        "\n",
        "        # **fake generation**: simple down/up-sample artifact\n",
        "        if label == 1:\n",
        "            t = torch.from_numpy(wav).float()\n",
        "            t = torchaudio.transforms.Resample(self.sr, 4000)(t)\n",
        "            t = torchaudio.transforms.Resample(4000, self.sr)(t)\n",
        "            wav = t.numpy()\n",
        "\n",
        "        # features → LFCC\n",
        "        lfcc = compute_lfcc(wav, sr=self.sr)\n",
        "        # per-coeff normalization\n",
        "        m, s = lfcc.mean(axis=1, keepdims=True), lfcc.std(axis=1, keepdims=True)+1e-9\n",
        "        lfcc = (lfcc - m)/s\n",
        "\n",
        "        # to tensor (1×80×T)\n",
        "        feat = torch.from_numpy(lfcc).float().unsqueeze(0)\n",
        "        return feat, torch.tensor(label, dtype=torch.long)"
      ],
      "metadata": {
        "id": "usqaAFgnOanY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "data_dir = \"/content/vctk_samples\"   # ← this folder contains p225/, p226/, p227/, p228/\n",
        "speakers = [\"p225\",\"p226\",\"p227\",\"p228\"]\n",
        "\n",
        "train_ds = VCTKDeepfakeDataset(data_dir, speakers, mode=\"train\")\n",
        "val_ds   = VCTKDeepfakeDataset(data_dir, speakers, mode=\"val\")\n",
        "test_ds  = VCTKDeepfakeDataset(data_dir, speakers, mode=\"test\")\n",
        "\n",
        "print(len(train_ds), len(val_ds), len(test_ds))\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True,  drop_last=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=16, shuffle=False)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEm1xxNYOb_1",
        "outputId": "4c1ca5a3-d333-4e74-cd48-22f8c1ab5d5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1864 398 402\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FMSAttention(nn.Module):\n",
        "    \"\"\"Feature Map Scaling (FMS) attention block.\n",
        "    Scales each channel's feature map by a learned factor in [0,1].\"\"\"\n",
        "    def __init__(self, channels):\n",
        "        super(FMSAttention, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)  # global average pooling to (B, C, 1, 1)\n",
        "        # Learn a scale for each channel (uses sigmoid activation for 0-1 output)\n",
        "        self.fc = nn.Linear(channels, channels)\n",
        "        # Initialize fc weights/bias (optional): start near identity scaling\n",
        "        nn.init.xavier_uniform_(self.fc.weight, gain=1.0)\n",
        "        nn.init.constant_(self.fc.bias, 0.0)\n",
        "    def forward(self, x):\n",
        "        # x shape: (B, C, F, T)\n",
        "        B, C, Freq, Time = x.size()\n",
        "        # Global average pool over freq & time\n",
        "        y = self.avg_pool(x).view(B, C)             # shape (B, C)\n",
        "        # Linear layer + sigmoid to get scale factors\n",
        "        scales = torch.sigmoid(self.fc(y))          # shape (B, C), each in [0,1]\n",
        "        # Reshape scale vector for broadcasting and scale the input\n",
        "        scales = scales.view(B, C, 1, 1)            # (B, C, 1, 1)\n",
        "        return x * scales                           # multiply each channel by its scale\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"Pre-activation residual block with two 3x3 conv layers.\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, first_block=False):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.first_block = first_block\n",
        "        # Convolution layers: 3x3 kernels, padding=1 to preserve dimensions\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn1  = nn.BatchNorm2d(in_channels)  # will be used only if not first_block\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2  = nn.BatchNorm2d(out_channels)\n",
        "        # Identity skip convolution if channel dimensions differ\n",
        "        if in_channels != out_channels:\n",
        "            self.skip_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
        "        else:\n",
        "            self.skip_conv = None\n",
        "        # Initialize conv weights (Kaiming He initialization for LeakyReLU)\n",
        "        nn.init.kaiming_normal_(self.conv1.weight, a=0.3, nonlinearity='leaky_relu')\n",
        "        nn.init.kaiming_normal_(self.conv2.weight, a=0.3, nonlinearity='leaky_relu')\n",
        "        if self.skip_conv is not None:\n",
        "            nn.init.kaiming_normal_(self.skip_conv.weight, a=0.3, nonlinearity='leaky_relu')\n",
        "    def forward(self, x):\n",
        "        # x: (B, in_channels, F, T)\n",
        "        # Pre-activation: apply BN+LeakyReLU on input for conv1 (skip if first block)\n",
        "        if not self.first_block:\n",
        "            out = self.bn1(x)\n",
        "            out = F.leaky_relu(out, negative_slope=0.3, inplace=True)\n",
        "        else:\n",
        "            out = x  # first block: skip initial BN and activation (already done in preprocessing)\n",
        "        # First conv\n",
        "        out = self.conv1(out)                      # -> (B, out_channels, F, T)\n",
        "        # BN + activation for second conv\n",
        "        out = self.bn2(out)\n",
        "        out = F.leaky_relu(out, negative_slope=0.3, inplace=True)\n",
        "        # Second conv\n",
        "        out = self.conv2(out)                      # -> (B, out_channels, F, T)\n",
        "        # Skip connection\n",
        "        residual = x\n",
        "        if self.skip_conv is not None:\n",
        "            residual = self.skip_conv(residual)    # align channels if needed\n",
        "        # Add identity and conv paths\n",
        "        out = out + residual                       # element-wise sum\n",
        "        return out\n",
        "\n",
        "class SpecRNet(nn.Module):\n",
        "    def __init__(self, input_channels=1, input_freq_bins=80):\n",
        "        super(SpecRNet, self).__init__()\n",
        "        # Preliminary normalization (BatchNorm + SELU activation)\n",
        "        self.pre_bn = nn.BatchNorm2d(input_channels)\n",
        "        self.pre_act = nn.SELU(inplace=True)\n",
        "        # Residual blocks\n",
        "        self.res1 = ResidualBlock(in_channels=1,  out_channels=20, first_block=True)\n",
        "        self.res2 = ResidualBlock(in_channels=20, out_channels=64, first_block=False)\n",
        "        self.res3 = ResidualBlock(in_channels=64, out_channels=64, first_block=False)\n",
        "        # FMS attention blocks after each residual\n",
        "        self.fms1 = FMSAttention(channels=20)\n",
        "        self.fms2 = FMSAttention(channels=64)\n",
        "        self.fms3 = FMSAttention(channels=64)\n",
        "        # Pooling layers: after each resblock, do maxpool -> FMS -> maxpool.\n",
        "        # We'll use two pooling ops per block. Kernel sizes chosen to match paper's output shapes.\n",
        "        # Block1: input freq=80 -> output freq=20 (factor 4), input time -> time/4. Use two 2x2 pools.\n",
        "        self.pool1a = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2), ceil_mode=True)\n",
        "        self.pool1b = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2), ceil_mode=True)\n",
        "        # Block2: input freq=20 -> output freq=5 (factor 4), time -> time/4. Use two 2x2 pools.\n",
        "        self.pool2a = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2), ceil_mode=True)\n",
        "        self.pool2b = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2), ceil_mode=True)\n",
        "        # Block3: input freq=5 -> output freq=1, time -> time/4. Use 2x2 then 3x2 pooling.\n",
        "        self.pool3a = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2), ceil_mode=True)\n",
        "        self.pool3b = nn.MaxPool2d(kernel_size=(3,2), stride=(3,2), ceil_mode=True)\n",
        "        # Pre-recurrent normalization (BatchNorm + SELU for final 64×1×T map)\n",
        "        self.post_bn = nn.BatchNorm2d(64)\n",
        "        self.post_act = nn.SELU(inplace=True)\n",
        "        # Recurrent layers: 2-layer Bidirectional GRU (hidden size 64 per direction)\n",
        "        self.gru = nn.GRU(input_size=64, hidden_size=64, num_layers=2, batch_first=True, bidirectional=True)\n",
        "        # Classification head: two fully-connected layers (128 -> 128 -> 1)\n",
        "        self.fc1 = nn.Linear(128, 128)\n",
        "        self.fc2 = nn.Linear(128, 1)\n",
        "        nn.init.xavier_uniform_(self.fc1.weight); nn.init.constant_(self.fc1.bias, 0.0)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight); nn.init.constant_(self.fc2.bias, 0.0)\n",
        "    def forward(self, x):\n",
        "        # x shape: (B, 1, 80, N) where N is number of frames (variable per sample or padded)\n",
        "        # Preliminary normalization\n",
        "        x = self.pre_bn(x)\n",
        "        x = self.pre_act(x)                        # -> (B, 1, 80, N)\n",
        "        # Residual block 1\n",
        "        x = self.res1(x)                           # -> (B, 20, 80, N)\n",
        "        x = self.pool1a(x)                         # first pooling (downsample)\n",
        "        x = self.fms1(x)                           # FMS attention scaling\n",
        "        x = self.pool1b(x)                         # second pooling\n",
        "        # Residual block 2\n",
        "        x = self.res2(x)                           # -> (B, 64, 20, N/4)\n",
        "        x = self.pool2a(x)\n",
        "        x = self.fms2(x)\n",
        "        x = self.pool2b(x)\n",
        "        # Residual block 3\n",
        "        x = self.res3(x)                           # -> (B, 64, 5, N/16)\n",
        "        x = self.pool3a(x)\n",
        "        x = self.fms3(x)\n",
        "        x = self.pool3b(x)                         # -> (B, 64, 1, N/64)\n",
        "        # Pre-recurrent BN + activation\n",
        "        x = self.post_bn(x)\n",
        "        x = self.post_act(x)                       # shape remains (B, 64, 1, T')\n",
        "        # Prepare for RNN: flatten freq dimension (which is 1) and swap to (B, T', features)\n",
        "        # x is (B, 64, 1, T') -> (B, 64, T')\n",
        "        x = x.squeeze(2)                           # -> (B, 64, T')\n",
        "        x = x.permute(0, 2, 1)                     # -> (B, T', 64) sequence of 64-dim vectors\n",
        "        # Bi-GRU processing\n",
        "        gru_out, gru_h = self.gru(x)               # gru_out: (B, T', 128), gru_h: (4, B, 64) for 2 layers * 2 directions\n",
        "        # Take last hidden state of the last GRU layer (concatenate forward & backward)\n",
        "        # gru_h[-2] = last layer forward hidden, gru_h[-1] = last layer backward hidden\n",
        "        h_forward = gru_h[-2]                      # (B, 64)\n",
        "        h_backward = gru_h[-1]                     # (B, 64)\n",
        "        h_final = torch.cat([h_forward, h_backward], dim=1)  # (B, 128)\n",
        "        # Fully connected layers for classification\n",
        "        x = F.relu(self.fc1(h_final))              # (B, 128)\n",
        "        logit = self.fc2(x).squeeze(1)             # (B,) - raw score\n",
        "        prob = torch.sigmoid(logit)                # probability in [0,1]\n",
        "        return prob  # or return logit if using BCEWithLogitsLoss during training"
      ],
      "metadata": {
        "id": "X5_biDFDO_DU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "# Initialize model, loss, optimizer\n",
        "model = SpecRNet(input_channels=1, input_freq_bins=80)\n",
        "model = model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "criterion = nn.BCELoss()  # use BCE loss since model outputs probability\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "num_epochs = 10\n",
        "best_val_loss = float('inf')\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for features, labels in train_loader:\n",
        "        features = features.to(model.device) if hasattr(model, 'device') else features.to(model.fc1.weight.device)\n",
        "        labels = labels.to(features.device).float()  # BCELoss expects float labels for probabilities\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)            # forward pass -> outputs shape (B,) probability\n",
        "        loss = criterion(outputs, labels)    # compute binary cross-entropy loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * features.size(0)\n",
        "    avg_train_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for features, labels in val_loader:\n",
        "            features = features.to(model.fc1.weight.device)\n",
        "            labels = labels.to(features.device).float()\n",
        "            outputs = model(features)\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * features.size(0)\n",
        "            # Compute accuracy\n",
        "            preds = (outputs >= 0.5).long()      # threshold at 0.5\n",
        "            correct += (preds == labels.long()).sum().item()\n",
        "            total += features.size(0)\n",
        "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
        "    val_accuracy = correct / total * 100.0\n",
        "    print(f\"Epoch {epoch}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}, Val Acc = {val_accuracy:.2f}%\")\n",
        "    # Save best model\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), \"best_specrnet.pth\")\n",
        "        print(\"  (Best model saved)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXX0SdFcO3DF",
        "outputId": "ddd2d4f1-3849-4bb6-b88f-51fe9c6d6c2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss = 0.0927, Val Loss = 0.0134, Val Acc = 99.25%\n",
            "  (Best model saved)\n",
            "Epoch 2: Train Loss = 0.0373, Val Loss = 0.0790, Val Acc = 97.24%\n",
            "Epoch 3: Train Loss = 0.0343, Val Loss = 0.0004, Val Acc = 100.00%\n",
            "  (Best model saved)\n",
            "Epoch 4: Train Loss = 0.0060, Val Loss = 1.3479, Val Acc = 74.62%\n",
            "Epoch 5: Train Loss = 0.0032, Val Loss = 0.0146, Val Acc = 99.50%\n",
            "Epoch 6: Train Loss = 0.0057, Val Loss = 0.0002, Val Acc = 100.00%\n",
            "  (Best model saved)\n",
            "Epoch 7: Train Loss = 0.0080, Val Loss = 0.4645, Val Acc = 82.66%\n",
            "Epoch 8: Train Loss = 0.0018, Val Loss = 0.0020, Val Acc = 99.75%\n",
            "Epoch 9: Train Loss = 0.0069, Val Loss = 0.0250, Val Acc = 99.50%\n",
            "Epoch 10: Train Loss = 0.0021, Val Loss = 0.0204, Val Acc = 99.75%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on test set\n",
        "test_dataset = VCTKDeepfakeDataset(data_dir, speakers, mode='test')\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "model.eval()\n",
        "model.load_state_dict(torch.load(\"best_specrnet.pth\", map_location=features.device))\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for features, labels in test_loader:\n",
        "        features = features.to(model.fc1.weight.device)\n",
        "        outputs = model(features)            # probabilities\n",
        "        all_preds.extend(outputs.cpu().numpy().tolist())\n",
        "        all_labels.extend(labels.numpy().tolist())\n",
        "\n",
        "# Compute accuracy\n",
        "pred_labels = [1 if p >= 0.5 else 0 for p in all_preds]\n",
        "accuracy = sum(1 for pl, tl in zip(pred_labels, all_labels) if pl == tl) / len(all_labels)\n",
        "print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n",
        "\n",
        "# Compute confusion matrix\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
        "cm = confusion_matrix(all_labels, pred_labels, labels=[0,1])\n",
        "print(\"Confusion Matrix [[TN, FP],[FN, TP]]:\\n\", cm)\n",
        "# Compute ROC AUC\n",
        "auc = roc_auc_score(all_labels, all_preds)\n",
        "print(f\"ROC AUC: {auc:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4cC2Uf5O3yE",
        "outputId": "ebf60ac9-c28f-4794-d0f3-6fdb6b0e1184"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 100.00%\n",
            "Confusion Matrix [[TN, FP],[FN, TP]]:\n",
            " [[201   0]\n",
            " [  0 201]]\n",
            "ROC AUC: 1.000\n"
          ]
        }
      ]
    }
  ]
}