# -*- coding: utf-8 -*-
"""FinalProjectCS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ulNLlI2BeK7tX9LwYFk_tTOJKJrT4nuP

# extract_data
"""

# import shutil
# import os

# # ×”× ×ª×™×‘ ×œ×ª×™×§×™×™×” ×©× ×•×¦×¨×” ×‘×”×¨×¦×” ×”×§×•×“××ª
# destination_folder = "/content/vctk_full"

# # ×‘×“×™×§×” ×× ×”×ª×™×§×™×™×” ×§×™×™××ª, ×•××– ××—×™×§×”
# if os.path.exists(destination_folder):
#     shutil.rmtree(destination_folder)
#     print(f" ×”×ª×™×§×™×™×” '{destination_folder}' × ××—×§×” ×‘×”×¦×œ×—×”.")
# else:
#     print(f"â„¹ ×”×ª×™×§×™×™×” '{destination_folder}' ×œ× ×§×™×™××ª, ××™×Ÿ ××” ×œ××—×•×§.")

import zipfile
import os
import shutil
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# List contents to verify paths (optional)
root_path = '/content/drive/My Drive/'
print("Contents of 'My Drive':", os.listdir(root_path))

subfolder_path = '/content/drive/My Drive/Colab Notebooks/'
print("Contents of 'Colab Notebooks':", os.listdir(subfolder_path))

# Define paths
zip_file = "/content/drive/My Drive/Colab Notebooks/archive.zip"  # Path to your ZIP file
destination_folder = "/content/vctk_samples"  # Where to extract selected data
wanted_speakers = ["p225", "p226", "p227", "p228"]  # Select specific speakers

# Check if the ZIP file exists
if os.path.isfile(zip_file):
    print(" ZIP file found:", zip_file)
else:
    raise FileNotFoundError(f" ZIP file not found: {zip_file}")

# Create destination folder if it doesn't exist
os.makedirs(destination_folder, exist_ok=True)

# Selectively extract only desired speaker folders from the ZIP
with zipfile.ZipFile(zip_file, 'r') as zip_ref:
    extracted_files = 0
    for file in zip_ref.namelist():
        if any(f"VCTK-Corpus/wav48/{spk}/" in file or f"VCTK-Corpus/txt/{spk}/" in file for spk in wanted_speakers):
            # Ensure directory structure is preserved
            target_path = os.path.join(destination_folder, file)
            os.makedirs(os.path.dirname(target_path), exist_ok=True)
            with zip_ref.open(file) as source, open(target_path, 'wb') as target:
                shutil.copyfileobj(source, target)
            extracted_files += 1

print(f"\n Extraction complete: {extracted_files} files were extracted.")
print(f" Extracted data is available in: {destination_folder}")

"""# Installations

"""

!pip install coqui-tts

!apt-get install -y espeak-ng

"""Deleting fake_wav48 if it exists

# generate_fake_data
"""

# import os
# from TTS.api import TTS
# import shutil

# fake_audio_folder = "/content/vctk_samples/VCTK-Corpus/VCTK-Corpus/fake_wav48"
# # ×¤×•× ×§×¦×™×” ×œ××—×™×§×ª ×›×œ ×§×‘×¦×™ ×”××•×“×™×• ×©× ×•×¦×¨×•
# def clear_fake_audio_folder(fake_audio_folder):
#     if os.path.exists(fake_audio_folder):
#         shutil.rmtree(fake_audio_folder)  # ××•×—×§ ××ª ×›×œ ×”×ª×™×§×™×™×” ×›×•×œ×œ ×”×§×‘×¦×™× ×©×‘×”
#         os.makedirs(fake_audio_folder, exist_ok=True)  # ×™×•×¦×¨ ××—×“×© ××ª ×”×ª×™×§×™×™×” ×”×¨×™×§×”
#         print(f" ×›×œ ×”×§×‘×¦×™× ×‘×ª×™×§×™×™×” '{fake_audio_folder}' × ××—×§×•!")

# clear_fake_audio_folder(fake_audio_folder)
# quit()

"""Creating Fake Audio (No Need to run now)"""

import os
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import torch

from TTS.api import TTS

# =========================
# CONFIG
# =========================
# 1) Input text files (one .txt per utterance)
TEXT_ROOT = Path("/content/vctk_samples/VCTK-Corpus/VCTK-Corpus/txt")

# 2) Real audio folder (matching real WAVs you want to clone the voice from)
#    The script will try to find a matching WAV by the text filename stem inside the corresponding subfolder.
#    Example: If text is ".../p228/p228_065.txt", it will try "/.../real_audio_folder/p228/p228_065.wav"
REAL_AUDIO_ROOT = Path("/content/vctk_samples/VCTK-Corpus/VCTK-Corpus/wav48")  # <-- CHANGE if needed

# 3) Output folder for fake audio
OUT_ROOT = Path("/content/vctk_samples/VCTK-Corpus/VCTK-Corpus/fake_wav48_xtts")
OUT_ROOT.mkdir(parents=True, exist_ok=True)

# 4) Speaker metadata (for bookkeeping only; XTTS uses speaker_wav for actual voice)
SPEAKER_INFO = {
    "p225": ("F", "22", "Southern England"),
    "p226": ("M", "22", "Surrey"),
    "p227": ("M", "38", "Cumbria"),
    "p228": ("F", "22", "Southern England"),
}

# 5) Language to synthesize in (VCTK is English)
LANGUAGE = "en"

# 6) Concurrency settings:
#    On GPU, keep MAX_WORKERS=1 (XTTS is heavy and not thread-safe on CUDA).
#    On CPU, you can increase to 4 (or more if your machine can handle it).
GPU_AVAILABLE = torch.cuda.is_available()
MAX_WORKERS = 1 if GPU_AVAILABLE else 4

# =========================
# MODEL LOADING
# =========================
# Use XTTS v2 for voice cloning with a reference WAV.
# (We also lazy-load a VCTK-VITS fallback if a real WAV is missing.)
print("Loading XTTS v2 model...")
tts_xtts = TTS(model_name="tts_models/multilingual/multi-dataset/xtts_v2", progress_bar=True)
device = "cuda" if GPU_AVAILABLE else "cpu"
tts_xtts.to(device)
print(f"XTTS is running on: {device}")

# Fallback multi-speaker model (only used when no real WAV is found)
print("Loading VCTK-VITS fallback model...")
tts_vctk = TTS(model_name="tts_models/en/vctk/vits", progress_bar=False)
tts_vctk.to(device)

# Mutex for model calls if you insist on >1 threads on CPU (XTTS is heavy; serialize calls by default on GPU).
synth_lock = threading.Lock() if MAX_WORKERS > 1 else None

# =========================
# HELPERS
# =========================
def to_vctk_id(s: str) -> str:
    """Convert '228' -> 'p228' for VCTK-style speaker IDs."""
    s = s.strip()
    return s if s.startswith("p") else f"p{s}"

def find_matching_real_wav(real_root: Path, subdir: str, txt_filename: str) -> Path | None:
    """
    Try to find the real WAV that matches the text file.
    Strategy:
      1) exact same stem under REAL_AUDIO_ROOT/subdir:  <stem>.wav
      2) any .wav in subdir that contains the stem (fallback)
      3) final fallback: None
    """
    stem = Path(txt_filename).stem  # e.g., 'p228_065' or '228_065'
    # Common VCTK stems look like 'p228_065'. If it's numeric-only, normalize:
    parts = stem.split("_")
    if parts and not parts[0].startswith("p"):
        parts[0] = "p" + parts[0]
    norm_stem = "_".join(parts)

    cand1 = real_root / subdir / f"{norm_stem}.wav"
    if cand1.exists():
        return cand1

    # Try exactly the original stem (if it already had 'p')
    cand2 = real_root / subdir / f"{stem}.wav"
    if cand2.exists():
        return cand2

    # Fallback: search within subdir for anything containing norm_stem or the raw stem
    subdir_path = real_root / subdir
    if subdir_path.is_dir():
        for fn in os.listdir(subdir_path):
            if not fn.lower().endswith(".wav"):
                continue
            if norm_stem in fn or stem in fn:
                return subdir_path / fn

    return None

def synth_xtts(text: str, speaker_wav: Path, out_path: Path, language: str = "en"):
    """
    Synthesize with XTTS v2 using a reference speaker WAV. This is the key for high voice similarity.
    """
    # Serialize heavy GPU calls if needed
    if synth_lock:
        with synth_lock:
            tts_xtts.tts_to_file(text=text, file_path=str(out_path), speaker_wav=str(speaker_wav), language=language)
    else:
        tts_xtts.tts_to_file(text=text, file_path=str(out_path), speaker_wav=str(speaker_wav), language=language)

def synth_vctk(text: str, speaker_id: str, out_path: Path):
    """
    Fallback synthesis with VCTK-VITS multi-speaker model (uses 'p###' speakers).
    """
    if synth_lock:
        with synth_lock:
            tts_vctk.tts_to_file(text=text, speaker=speaker_id, file_path=str(out_path))
    else:
        tts_vctk.tts_to_file(text=text, speaker=speaker_id, file_path=str(out_path))

def process_one(text_path: Path, out_subdir: Path):
    """
    Process a single text file:
      - Read text
      - Resolve speaker_id from filename (for metadata/fallback)
      - Find matching real WAV
      - Prefer XTTS cloning; fallback to VCTK-VITS speaker if real WAV missing
    """
    text = text_path.read_text(encoding="utf-8").strip()
    if not text:
        return f"[SKIP] Empty text: {text_path.name}"

    # Resolve speaker id from the filename (e.g., 'p228' from 'p228_065.txt')
    raw_id = text_path.stem.split("_")[0]          # 'p228' or '228'
    speaker_id = to_vctk_id(raw_id)                # 'p228'

    # Metadata (for filename only)
    gender, age, accent = SPEAKER_INFO.get(speaker_id, ("F", "22", "Southern England"))

    # Try to find the matching real wav in REAL_AUDIO_ROOT/<subdir>/
    subdir = text_path.parent.name
    real_wav = find_matching_real_wav(REAL_AUDIO_ROOT, subdir, text_path.name)

    # Build output path (include metadata in filename)
    out_name = f"{text_path.stem}__{speaker_id}__{gender}_{age}_{accent}.wav"
    out_path = out_subdir / out_name

    # Prefer XTTS cloning if real wav exists; otherwise fallback to VCTK-VITS speaker
    if real_wav and real_wav.exists():
        msg = f"[XTTS] {text_path.name} -> clone from {real_wav.name} -> {out_name}"
        synth_xtts(text=text, speaker_wav=real_wav, out_path=out_path, language=LANGUAGE)
        return msg
    else:
        msg = f"[FALLBACK VCTK] {text_path.name} -> speaker={speaker_id} -> {out_name}"
        synth_vctk(text=text, speaker_id=speaker_id, out_path=out_path)
        return msg

# =========================
# BUILD JOBS
# =========================
jobs = []
for subdir in os.listdir(TEXT_ROOT):
    subdir_path = TEXT_ROOT / subdir
    if not subdir_path.is_dir():
        continue

    out_subdir = OUT_ROOT / subdir
    out_subdir.mkdir(parents=True, exist_ok=True)

    for fn in os.listdir(subdir_path):
        if not fn.lower().endswith(".txt"):
            continue
        jobs.append((subdir_path / fn, out_subdir))

print(f"Found {len(jobs)} text files.")

# =========================
# RUN
# =========================
if not jobs:
    print("No jobs found. Check TEXT_ROOT.")
else:
    print(f"Starting synthesis with MAX_WORKERS={MAX_WORKERS} (device={device})")
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
        futs = [ex.submit(process_one, text_path, out_dir) for (text_path, out_dir) in jobs]
        for fut in as_completed(futs):
            try:
                info = fut.result()
                print(info)
            except Exception as e:
                print("[ERROR]", repr(e))

print("Done. Fake audio saved under:", OUT_ROOT)

"""Saving the fake audio"""

# import shutil
# from google.colab import files

# folder_path = "/content/vctk_samples/VCTK-Corpus/VCTK-Corpus/fake_wav48_xtts"
# zip_path = "/content/fake_audio.zip"
# shutil.make_archive(base_name=zip_path.replace(".zip", ""), format='zip', root_dir=folder_path)

# files.download(zip_path)

"""# Extracting the fake audio"""

import zipfile
import os
import shutil
from google.colab import drive

# Define paths
zip_file = "/content/drive/My Drive/Colab Notebooks/fake_audio.zip"  # Path to your ZIP file
destination_folder = "/content/vctk_samples/VCTK-Corpus/VCTK-Corpus/fake_audio"  # Where to extract selected data

# Example: define the speakers you want to extract
wanted_speakers = ["p225", "p226","p227","p228"]  # change this list as needed

# Check if the ZIP file exists
if os.path.isfile(zip_file):
    print(" ZIP file found:", zip_file)
else:
    raise FileNotFoundError(f" ZIP file not found: {zip_file}")

# Create destination folder if it doesn't exist
os.makedirs(destination_folder, exist_ok=True)

# Selectively extract only desired speaker folders from the ZIP
with zipfile.ZipFile(zip_file, 'r') as zip_ref:
    extracted_files = 0
    for file in zip_ref.namelist():
        if any(f"{spk}/" in file for spk in wanted_speakers):
            target_path = os.path.join(destination_folder, file)

            # If this entry is a directory â†’ skip it
            if file.endswith('/'):
                os.makedirs(target_path, exist_ok=True)
                continue

            # Ensure directory structure is preserved
            os.makedirs(os.path.dirname(target_path), exist_ok=True)

            # Copy file content
            with zip_ref.open(file) as source, open(target_path, 'wb') as target:
                shutil.copyfileobj(source, target)
            extracted_files += 1

print(f"Extracted {extracted_files} files for speakers: {wanted_speakers}")

"""# Importing libraries"""

!nvidia-smi -L || echo "No GPU"

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# set -euo pipefail
# 
# # --- Clone CLAD fresh (idempotent: remove existing dir if present) ---
# # remove previous clone to ensure a clean edit of requirements
# rm -rf CLAD
# git clone https://github.com/CLAD23/CLAD.git
# 
# # --- Normalize requirements for Python 3.12 (single source of truth) ---
# cp CLAD/requirements.txt CLAD/requirements.bak
# 
# # 1) Remove torch lines (Torch is installed manually for the correct CUDA wheel)
# sed -i '/^torch==/d; /^torchvision==/d; /^torchaudio==/d' CLAD/requirements.txt
# 
# # 2) Core pins for Py3.12 + Numba 0.60.0 (compatible with llvmlite 0.43.0 and NumPy 1.26.4)
# #    These ensure no NumPy 2.x is pulled by accident.
# if grep -q '^numpy' CLAD/requirements.txt; then
#   sed -i 's/^numpy==.*/numpy==1.26.4/' CLAD/requirements.txt
# else
#   sed -i '1i numpy==1.26.4' CLAD/requirements.txt
# fi
# 
# if grep -q '^numba' CLAD/requirements.txt; then
#   sed -i 's/^numba==.*/numba==0.60.0/' CLAD/requirements.txt
# else
#   sed -i '1i numba==0.60.0' CLAD/requirements.txt
# fi
# 
# if grep -q '^llvmlite' CLAD/requirements.txt; then
#   sed -i 's/^llvmlite==.*/llvmlite==0.43.0/' CLAD/requirements.txt
# else
#   sed -i '1i llvmlite==0.43.0' CLAD/requirements.txt
# fi
# 
# # 3) Stable Matplotlib on Py3.12
# if grep -q '^matplotlib' CLAD/requirements.txt; then
#   sed -i 's/^matplotlib==.*/matplotlib==3.8.4/' CLAD/requirements.txt
# else
#   sed -i '1i matplotlib==3.8.4' CLAD/requirements.txt
# fi
# 
# # 4) Librosa must satisfy coqui-tts (>=0.11.0); keep it permissive to avoid conflicts
# if grep -q '^librosa' CLAD/requirements.txt; then
#   sed -i 's/^librosa.*/librosa>=0.11.0/' CLAD/requirements.txt
# else
#   sed -i '1i librosa>=0.11.0' CLAD/requirements.txt
# fi
# 
# # 5) Pin OpenCV to builds compatible with NumPy 1.26.x (avoid NumPy 2.x constraint)
# #    Only modify if opencv lines exist (do not add if the project doesn't use it).
# grep -q '^opencv-python' CLAD/requirements.txt && sed -i 's/^opencv-python==.*/opencv-python==4.9.0.80/' CLAD/requirements.txt || true
# grep -q '^opencv-contrib-python' CLAD/requirements.txt && sed -i 's/^opencv-contrib-python==.*/opencv-contrib-python==4.9.0.80/' CLAD/requirements.txt || true
# 
# # 6) Pin spaCy/Thinc to versions that work with NumPy 1.x (only if present)
# grep -q '^thinc' CLAD/requirements.txt && sed -i 's/^thinc==.*/thinc==8.2.2/' CLAD/requirements.txt || true
# grep -q '^spacy' CLAD/requirements.txt && sed -i 's/^spacy==.*/spacy==3.7.4/' CLAD/requirements.txt || true
# 
# echo "===== Updated CLAD/requirements.txt ====="
# sed -n '1,250p' CLAD/requirements.txt
# 
# # --- Upgrade pip to avoid resolver quirks ---
# python -m pip install -U pip
# 
# # --- Install PyTorch 2.3.1 CUDA 12.1 (use CPU wheels by removing the index line if no GPU) ---
# python -m pip install --index-url https://download.pytorch.org/whl/cu121 \
#   torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1
# 
# # --- Install remaining CLAD dependencies from the single normalized requirements file ---
# python -m pip install -r CLAD/requirements.txt
# 
# # --- System library for soundfile/librosa WAV I/O (safe to install always) ---
# apt-get update -y
# apt-get install -y libsndfile1
# 
# echo "===== DONE: Environment pinned for Python 3.12 ====="
# python -V
# python - <<'PY'
# import sys, numpy, numba, llvmlite, matplotlib
# import importlib
# print("Python:", sys.version.split()[0])
# print("NumPy:", numpy.__version__)
# print("Numba:", numba.__version__)
# print("llvmlite:", llvmlite.__version__)
# print("Matplotlib:", matplotlib.__version__)
# for m in ("torch","torchvision","torchaudio","librosa"):
#     try:
#         mod = importlib.import_module(m)
#         print(f"{m}:", getattr(mod,"__version__", "unknown"))
#     except Exception as e:
#         print(f"{m}: NOT INSTALLED ({e})")
# PY
#

#4
#Creating a differnet folder called my_audio so we won't destroy the data created

import os, pathlib, shutil

# Destination base folders
REAL_DST = pathlib.Path("/content/my_audio/real")
TXT_DST  = pathlib.Path("/content/my_audio/txt")
FAKE_DST = pathlib.Path("/content/my_audio/fake")

REAL_DST.mkdir(parents=True, exist_ok=True)
TXT_DST.mkdir(parents=True, exist_ok=True)
FAKE_DST.mkdir(parents=True, exist_ok=True)

# Source folders
FAKE_SRC = pathlib.Path("/content/vctk_samples/VCTK-Corpus/VCTK-Corpus/fake_audio")
TXT_SRC  = pathlib.Path("/content/vctk_samples/VCTK-Corpus/VCTK-Corpus/txt")
REAL_SRC = pathlib.Path("/content/vctk_samples/VCTK-Corpus/VCTK-Corpus/wav48")

# Copy FAKE wavs while keeping speaker folders
for folder in FAKE_SRC.glob("p*"):
    speaker_dst = FAKE_DST / folder.name
    speaker_dst.mkdir(parents=True, exist_ok=True)
    for wav in folder.glob("*.wav"):
        shutil.copy(wav, speaker_dst / wav.name)

# Copy REAL wavs while keeping speaker folders
for folder in REAL_SRC.glob("p*"):
    speaker_dst = REAL_DST / folder.name
    speaker_dst.mkdir(parents=True, exist_ok=True)
    for wav in folder.glob("*.wav"):
        shutil.copy(wav, speaker_dst / wav.name)

# Copy TXT transcripts (flat, no subfolders in original)
for folder in TXT_SRC.glob("p*"):
    speaker_dst = TXT_DST / folder.name
    speaker_dst.mkdir(parents=True, exist_ok=True)
    for txt in folder.glob("*.txt"):
        shutil.copy(txt, speaker_dst / txt.name)


print("âœ… All files copied with speaker folder structure preserved!")
print("  Real :", REAL_DST)
print("  Fake :", FAKE_DST)
print("  Text :", TXT_DST)

#7
# Speaker-disjoint splitter with:
# - guaranteed non-empty val
# - easy "switch speakers" controls
# - deletes old OUT folder before writing
#
# Works for both 4 speakers (auto 2/1/1) and 5+ speakers (targets 3/1/1 by speakers).

import os, shutil, random, glob, csv, itertools
from pathlib import Path
from collections import defaultdict

# ---------- CONFIG ----------
ROOT = Path("/content/my_audio")           # your current data root (real/fake/{speaker}/*.wav)
OUT  = Path("/content/my_audio_split")     # split will be (re)created here
AUDIO_EXTS = {".wav", ".flac", ".mp3", ".m4a", ".aac", ".ogg"}  # add if needed
SEED = 42
USE_SYMLINKS = True                        # False = copy files instead of symlink
REQUIRE_BOTH_CLASSES = True                # speakers must exist under BOTH real/ and fake/
# Desired speaker counts (train/val/test)
DESIRED_311 = (3, 1, 1)                    # prefer 3/1/1 when you have â‰¥5 speakers
FALLBACK_211 = (2, 1, 1)                   # for 4 speakers, this is the safe split
# >>> Force specific speakers into splits (edit these to "switch")
TRAIN_FORCE = set()                        # e.g., {"p226","p227","p228"}
VAL_FORCE   = set()                        # e.g., {"p225"}
TEST_FORCE  = set()                        # e.g., {"p229"}
# --------------------------------------

random.seed(SEED)

def list_speakers(root, cls):
    base = root/cls
    if not base.exists(): return []
    return sorted([d.name for d in base.iterdir() if d.is_dir() and d.name != "txt"])

def list_audio(dirpath):
    return sorted([p for p in dirpath.rglob("*")
                   if p.is_file() and p.suffix.lower() in AUDIO_EXTS])

# 1) Find eligible speakers (present and non-empty under both classes if required)
real_spk = set(list_speakers(ROOT, "real"))
fake_spk = set(list_speakers(ROOT, "fake"))
if REQUIRE_BOTH_CLASSES:
    eligible = sorted(real_spk & fake_spk)
else:
    eligible = sorted(real_spk | fake_spk)

def nonempty_both(s):
    if not REQUIRE_BOTH_CLASSES:  # just need at least one side non-empty
        return (len(list_audio(ROOT/"real"/s)) + len(list_audio(ROOT/"fake"/s))) > 0
    return len(list_audio(ROOT/"real"/s)) > 0 and len(list_audio(ROOT/"fake"/s)) > 0

eligible = [s for s in eligible if nonempty_both(s)]
n_spk = len(eligible)
if n_spk < 2:
    raise RuntimeError(f"Need â‰¥2 eligible speakers, found {n_spk}: {eligible}")

# 2) Choose target split sizes by number of speakers
if n_spk >= sum(DESIRED_311):
    N_TRAIN, N_VAL, N_TEST = DESIRED_311   # 3/1/1
else:
    # With 4 speakers, 2/1/1 is the right shape to keep val+test non-empty
    N_TRAIN, N_VAL, N_TEST = FALLBACK_211  # 2/1/1

# 3) Validate FORCE sets and fill remaining slots
forced = TRAIN_FORCE | VAL_FORCE | TEST_FORCE
if forced:
    missing = forced - set(eligible)
    if missing:
        raise RuntimeError(f"Forced speakers not found/eligible: {sorted(missing)}")
    overlap = (TRAIN_FORCE & VAL_FORCE) | (TRAIN_FORCE & TEST_FORCE) | (VAL_FORCE & TEST_FORCE)
    if overlap:
        raise RuntimeError(f"Forced sets overlap: {sorted(overlap)}")

# file counts (use 'real' side as proxy for per-speaker volume)
spk_counts = {s: len(list_audio(ROOT/"real"/s)) for s in eligible}
total_files = sum(spk_counts.values())

def pick_k_closest(candidates, k, target_share):
    """Pick k speakers whose file-count sum is closest to target_share (in files)."""
    if k <= 0: return set()
    if len(candidates) <= k: return set(candidates)
    best, gap = None, float("inf")
    for combo in itertools.combinations(candidates, k):
        share = sum(spk_counts[s] for s in combo)
        g = abs(share - target_share)
        if g < gap:
            gap, best = g, set(combo)
    return best

# Start with forced
train_set, val_set, test_set = set(TRAIN_FORCE), set(VAL_FORCE), set(TEST_FORCE)
remaining = [s for s in eligible if s not in (train_set | val_set | test_set)]

need_train = max(0, N_TRAIN - len(train_set))
need_val   = max(0, N_VAL   - len(val_set))
need_test  = max(0, N_TEST  - len(test_set))

# Target train file share (rough guideline): ~60% if 3/1/1, ~50% if 2/1/1
target_train_share = 0.60*total_files if (N_TRAIN, N_VAL, N_TEST) == DESIRED_311 else 0.50*total_files

# Fill TRAIN first to hit the share as best as possible
if need_train > 0:
    add = pick_k_closest(remaining, need_train, target_train_share - sum(spk_counts[s] for s in train_set))
    train_set |= add
    remaining = [s for s in remaining if s not in add]

# Fill VAL with lighter speakers (to keep val/test similar size)
if need_val > 0:
    remaining.sort(key=lambda s: spk_counts[s])  # lightest first
    add = set(remaining[:need_val])
    val_set |= add
    remaining = remaining[need_val:]

# Fill TEST with the rest needed
if need_test > 0:
    add = set(remaining[:need_test])
    test_set |= add
    remaining = remaining[need_test:]

# Final sanity: exact sizes, disjointness
if not (len(train_set) == N_TRAIN and len(val_set) == N_VAL and len(test_set) == N_TEST):
    raise RuntimeError(f"Final sizes must be {N_TRAIN}/{N_VAL}/{N_TEST}, got {len(train_set)}/{len(val_set)}/{len(test_set)}")
if not (train_set.isdisjoint(val_set) and train_set.isdisjoint(test_set) and val_set.isdisjoint(test_set)):
    raise RuntimeError("Splits are not disjoint by speakers.")

print("Eligible speakers:", eligible)
print("Chosen split (by speakers):")
print("  train:", sorted(train_set))
print("  val  :", sorted(val_set))
print("  test :", sorted(test_set))
print("By-file shares (train/val/test):",
      round(sum(spk_counts[s] for s in train_set)/total_files, 3),
      round(sum(spk_counts[s] for s in val_set)/total_files, 3),
      round(sum(spk_counts[s] for s in test_set)/total_files, 3))

# 4) DELETE OLD OUT (so the previous no-val split is removed), then rebuild
if OUT.exists():
    shutil.rmtree(OUT)
for split in ["train", "val", "test"]:
    for cls in ["real", "fake"]:
        (OUT/split/cls).mkdir(parents=True, exist_ok=True)

def which_split(speaker):
    if speaker in train_set: return "train"
    if speaker in val_set:   return "val"
    return "test"

# 5) Materialize split (symlink/copy) + manifest
rows, counts = [], defaultdict(int)
for cls in ["real", "fake"]:
    base = ROOT/cls
    for sp in (train_set | val_set | test_set):
        src_dir = base/sp
        dst_dir = OUT/which_split(sp)/cls/sp
        dst_dir.mkdir(parents=True, exist_ok=True)
        for src in list_audio(src_dir):
            dst = dst_dir/src.name
            if dst.exists():
                try: dst.unlink()
                except: pass
            if USE_SYMLINKS:
                try:
                    os.symlink(src.resolve(), dst)
                except FileExistsError:
                    pass
            else:
                shutil.copy2(src, dst)
            rows.append({
                "split": which_split(sp),
                "speaker": sp,
                "label": 0 if cls=="real" else 1,
                "src_path": str(src.resolve()),
                "dst_path": str(dst.resolve())
            })
            counts[(which_split(sp), cls)] += 1

manifest_csv = OUT/"manifest.csv"
with open(manifest_csv, "w", newline="") as f:
    writer = csv.DictWriter(f, fieldnames=["split","speaker","label","src_path","dst_path"])
    writer.writeheader(); writer.writerows(rows)

print("\nManifest:", manifest_csv)
print("Counts per split/class:")
for split in ["train","val","test"]:
    for cls in ["real","fake"]:
        print(f"  {split:5s} {cls:4s}: {counts[(split, cls)]}")

# Extra safety: show speaker overlap (should be empty)
print("\nOverlap checks (should be empty):")
print("  train âˆ© val :", train_set & val_set)
print("  train âˆ© test:", train_set & test_set)
print("  val   âˆ© test:", val_set & test_set)

#6
# Choose ONE of the two options below:

USE_LIBROSA = True   # set to False to use scipy.io.wavfile only

if USE_LIBROSA:
    # Librosa path: convenient resample-to-16k + mono in one call
    !pip -q install librosa soundfile
else:
    # Scipy path: no extra system libs; we will do a small numpy resample
    !pip -q install scipy

# Commented out IPython magic to ensure Python compatibility.
# Fix NumPy/SciPy ABI mismatch for Python 3.12
# %pip uninstall -y numpy scipy
# %pip cache purge

# Install a matching pair (works well on Colab Py3.12)
# %pip install --no-cache-dir --force-reinstall "numpy==2.1.3" "scipy==1.14.1" "soundfile>=0.12.0"

import IPython; print("ğŸ”„ Restarting kernelâ€¦"); IPython.Application.instance().kernel.do_shutdown(True)

# Preprocess audio with soundfile+scipy: 16kHz mono + pad/trim to 64600
from pathlib import Path
import soundfile as sf
import numpy as np
from scipy.signal import resample_poly
import math

TARGET_SR  = 16000
TARGET_LEN = 64600  # ~4s @16kHz

def resample_to(x, sr, target_sr):
    if sr == target_sr:
        return x
    g   = math.gcd(sr, target_sr)
    up  = target_sr // g
    down= sr // g
    return resample_poly(x, up, down).astype(np.float32)

def preprocess_wav_np(in_path: Path):
    x, sr = sf.read(str(in_path), dtype="float32")  # x: [T] or [T, C]
    if x.ndim > 1:  # to mono
        x = x.mean(axis=1)
    x = resample_to(x, sr, TARGET_SR)
    T = x.shape[0]
    if T < TARGET_LEN:
        x = np.pad(x, (0, TARGET_LEN - T))
    else:
        x = x[:TARGET_LEN]
    return x

for split in ["train", "val", "test"]:
    split_dir = Path("/content/my_audio_split") / split
    for cls in ["real", "fake"]:
        for wav_path in (split_dir / cls).rglob("*.wav"):
            x = preprocess_wav_np(wav_path)
            sf.write(str(wav_path), x, TARGET_SR)

print("âœ… All audio preprocessed to 16kHz (soundfile+scipy) and padded/clipped to 64600 samples")

# Commented out IPython magic to ensure Python compatibility.
# 1) × ×¤×˜×¨×™× ××¡×¤×¨×™×™×” ×©××›×¨×™×—×” numba (×œ× ×¦×¨×™×š ×›×©××©×ª××©×™× ×‘-torchaudio)
# %pip uninstall -y librosa numba resampy llvmlite soundfile

# 2) ××™×™×©×¨×™× ×’×¨×¡××•×ª ×©×ª×•×××•×ª ×œ-Python 3.12 ×•×œ-coqui/tsfresh
# %pip install -q --no-cache-dir "numpy==2.1.2" "scipy==1.14.1"

# 3) ××ª×—×•×œ ××œ× ×©×œ ×”×§×¨× ×œ ×›×“×™ ×©×”-ABI ×™×™×˜×¢×Ÿ ××—×“×© ××•×œ ×”×’×¨×¡××•×ª ×”×—×“×©×•×ª
import IPython; print("ğŸ”„ Restarting kernel..."); IPython.Application.instance().kernel.do_shutdown(True)

# === DIAG A: ×‘×“×™×§×ª ×¡×‘×™×‘×” ×‘×¡×™×¡×™×ª ===
import numpy as np, scipy, torch, torchaudio, platform
print("python:", platform.python_version())
print("numpy :", np.__version__)
print("scipy :", scipy.__version__)
print("torch :", torch.__version__)
print("torchaudio:", torchaudio.__version__)

# ××‘×—×Ÿ ABI ×œ-numpy.random (×× ×–×” × ×•×¤×œ -> ABI ×©×‘×•×¨)
from numpy.random import RandomState
_ = RandomState(0)
print("âœ… numpy.random ABI OK")

# Commented out IPython magic to ensure Python compatibility.
# Clean & reinstall ABI-compatible wheels for Py 3.12
# %pip uninstall -y numpy numpy-base
# %pip cache purge
# %pip install -q --no-cache-dir --force-reinstall "numpy==2.1.3" "scipy==1.14.1"

# Restart kernel (mandatory)
import IPython; print("ğŸ”„ Restarting kernel..."); IPython.Application.instance().kernel.do_shutdown(True)

# 8
import torch
from pathlib import Path
import os
os.chdir('/content/CLAD')

from Model import MoCo_v2, RawNetEncoderBaseline

# Step 1: Define the RawNet encoder configuration
d_args = {
    "in_channels": 1,
    "first_conv": 251,
    "filts": [
        128,  # output channels for sinc conv
        [128, 128],  # block0 and block1
        [128, 256],  # block2
        [256, 256]   # block3-5
    ],
    "nb_fc_node": 1024,
    "gru_node": 1024,
    "nb_gru_layer": 3,
    "nb_classes": 2
}

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Step 2: Create both encoders
encoder_q = RawNetEncoderBaseline(d_args, device)
encoder_k = RawNetEncoderBaseline(d_args, device)

# Step 3: Create the MoCo_v2 model
model = MoCo_v2(
    encoder_q=encoder_q,
    encoder_k=encoder_k,
    queue_feature_dim=1024,  # matches encoder output
    mlp=True,
    return_q=True
)

# Step 4: Load pretrained weights
ckpt_path = Path("pretrained_models/CLAD_150_10_2310.pth.tar")
if not ckpt_path.exists():
    raise FileNotFoundError(f"Checkpoint not found: {ckpt_path}")

ckpt = torch.load(ckpt_path, map_location='cpu')
state_dict = ckpt.get("state_dict", ckpt)

missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
print("Missing keys:", missing_keys)
print("Unexpected keys:", unexpected_keys)

model.to(device)
model.eval()

print("CLAD model loaded and ready.")

SR = 16000
CLIP_SECONDS = 4.0
N_SAMPLES = int(SR * CLIP_SECONDS)  # = 64000

with torch.no_grad():
    dummy = torch.randn(2, N_SAMPLES, device=device)  # (B,T)
    try:
        f = encoder_q(dummy)              # try (B,T)
    except Exception:
        f = encoder_q(dummy.unsqueeze(1)) # fallback (B,1,T)

    # Pool any extra time/freq dims so we have (B,D)
    if f.dim() == 3:
        f = f.mean(dim=2)                 # (B,D)
    elif f.dim() > 3:
        f = f.mean(dim=tuple(range(2, f.dim())))  # reduce to (B,D)

    print("Encoder output shape:", tuple(f.shape))
    D = f.shape[1]
    print("Probed feature dim D =", D)

# ------------------------------------------------------------
# Build a minimal Dataset and train_loader for pretraining.
# It expects a train split with two folders: .../train/real and .../train/fake
# It returns (waveform[T], label) where label: 1=real, 0=fake.
# ------------------------------------------------------------
import os
from pathlib import Path
import torch
from torch.utils.data import Dataset, DataLoader
import torchaudio

SR = 16000
N_SAMPLES = 64600
BATCH_SIZE = 24
NUM_WORKERS = 2
REAL_CLASS_ID = 1  # keep consistent with your pretraining cell

# Try common train-split locations; pick the first that exists
_CANDIDATE_TRAIN_DIRS = [
    "/content/my_audio_split/train",
    "/content/my_audio/train",
    "/content/my_audio_splits/train",
]
TRAIN_DIR = None
for _p in _CANDIDATE_TRAIN_DIRS:
    if Path(_p).exists():
        TRAIN_DIR = _p
        break
if TRAIN_DIR is None:
    raise FileNotFoundError(
        "Could not find a train split. Expected one of: " + ", ".join(_CANDIDATE_TRAIN_DIRS)
    )

CLASS_TO_ID = {"fake": 0, "real": 1}  # adjust only if your naming differs

def _pad_trim(x: torch.Tensor, target_len: int) -> torch.Tensor:
    # x: [T]
    T = x.size(0)
    if T == target_len:
        return x
    if T < target_len:
        pad = target_len - T
        return torch.nn.functional.pad(x, (0, pad))
    return x[:target_len]

class AudioFolderDataset(Dataset):
    """Walks train_dir/{real,fake}/**/*.wav and yields (waveform[T], label)."""
    def __init__(self, train_dir: str, sr: int = SR, n_samples: int = N_SAMPLES):
        self.train_dir = Path(train_dir)
        self.sr = sr
        self.n_samples = n_samples
        self.items = []  # list of (path, label)
        for cls in CLASS_TO_ID.keys():
            cls_dir = self.train_dir / cls
            if not cls_dir.exists():
                continue
            for p in cls_dir.rglob("*.wav"):
                self.items.append((p, CLASS_TO_ID[cls]))
        if len(self.items) == 0:
            raise RuntimeError(f"No .wav files found under {self.train_dir}/(real|fake)")

    def __len__(self):
        return len(self.items)

    def __getitem__(self, idx: int):
        path, label = self.items[idx]
        # load audio as mono at SR
        wav, sr = torchaudio.load(str(path))  # [C,T]
        if wav.size(0) > 1:
            wav = wav.mean(dim=0, keepdim=True)  # to mono
        if sr != self.sr:
            wav = torchaudio.functional.resample(wav, sr, self.sr)
        wav = wav.squeeze(0)  # [T]
        wav = _pad_trim(wav, self.n_samples)   # [N_SAMPLES]
        wav = wav.clamp(-1.0, 1.0).float()
        return wav, int(label)

# Build dataset and loader only if missing
if 'train_loader' not in globals():
    train_ds = AudioFolderDataset(TRAIN_DIR, sr=SR, n_samples=N_SAMPLES)
    train_loader = DataLoader(
        train_ds,
        batch_size=BATCH_SIZE,
        shuffle=True,
        num_workers=NUM_WORKERS,
        pin_memory=True,
        drop_last=True,      # helpful for contrastive batches
    )
    print(f"âœ… train_loader is ready from: {TRAIN_DIR}")
    print(f"   samples: {len(train_ds)} | batch_size: {BATCH_SIZE}")
else:
    print("â„¹ï¸ train_loader already defined; skipping rebuild.")

# ================================================================
# CLAD-style pretraining cell (MoCo + Length Loss) â€” self-contained
# ------------------------------------------------
# What this cell does (short):
# 1) Builds a momentum contrast pretraining loop around your existing encoders:
#    - encoder_q: updated by optimizer
#    - encoder_k: updated by momentum (no gradients)
# 2) Generates two augmented "views" per waveform using audio augmentations.
# 3) Computes InfoNCE contrastive loss against a memory queue of negatives.
# 4) Adds the CLAD "Length Loss" term to shape embedding norms w.r.t. real/fake labels.
# 5) Trains for EPOCHS, saves a checkpoint, and keeps dimensions consistent (D=1024).
#
# Assumptions / Integration:
# - Variables already defined earlier: `encoder_q`, `encoder_k`, `device`, `train_loader`.
# - Input shape is either [B,T] or [B,1,T]. This cell handles both.
# - Output features have dim D=1024 (matches your earlier forward check).
# - Labels in `train_loader` are integers; set REAL_CLASS_ID below to match your dataset.
#
# If you later want to switch to end-to-end supervised training, you can skip this cell.
# ================================================================

import math
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim.lr_scheduler import CosineAnnealingLR

# ----------------------------
# Hyperparameters (match CLAD paper defaults where applicable)
# ----------------------------
SR                = 16000
N_SAMPLES         = 64600          # keep consistent with your preprocessing
D                 = 1024           # encoder output dim (verified by your forward)
QUEUE_SIZE        = 6144           # memory queue length
TAU               = 0.07           # temperature for InfoNCE
MOMENTUM          = 0.999          # key encoder EMA
LR                = 5e-4           # Adam learning rate for encoder_q
WEIGHT_DECAY      = 1e-4
EPOCHS            = 150            # adjust if you want shorter runs first
REAL_CLASS_ID     = 1              # change if your dataset encodes REAL differently

# Length Loss (CLAD)
ALPHA             = 2.0            # weight for length loss
MARGIN            = 4.0
W_REAL            = 9.0            # stronger pull for real embeddings

# Checkpoints
CKPT_DIR          = "/content/checkpoints"
CKPT_PATH         = f"{CKPT_DIR}/clad_pretrain_moco_lenloss.pth"

# ----------------------------
# Utility: simple audio augmentations (torch-only; lightweight)
# ----------------------------
class AudioAug:
    """Returns two independently augmented views for each waveform in a batch."""
    def __init__(self, sr=SR, n_samples=N_SAMPLES):
        self.sr = sr
        self.n_samples = n_samples

    def random_gain(self, x, min_db=-6.0, max_db=6.0):
        # x: [B,T]
        g_db = (torch.rand(x.size(0), device=x.device) * (max_db - min_db) + min_db)
        g = 10.0 ** (g_db / 20.0)
        return x * g.unsqueeze(1)

    def random_time_shift(self, x, max_shift=int(0.05 * SR)):
        # circular shift in time
        if max_shift <= 0:
            return x
        B, T = x.shape
        shift = torch.randint(low=-max_shift, high=max_shift + 1, size=(B,), device=x.device)
        idx = (torch.arange(T, device=x.device).unsqueeze(0) - shift.unsqueeze(1)) % T
        return torch.gather(x, dim=1, index=idx)

    def random_fade(self, x, p=0.5):
        # linear fade-in/out with random length
        if torch.rand(1).item() > p:
            return x
        B, T = x.shape
        fade_len = torch.randint(int(0.01 * T), int(0.1 * T) + 1, (B,), device=x.device)
        out = x.clone()
        for b in range(B):
            if torch.rand(1).item() < 0.5:
                # fade-in
                L = fade_len[b].item()
                ramp = torch.linspace(0, 1, L, device=x.device)
                out[b, :L] = out[b, :L] * ramp
            else:
                # fade-out
                L = fade_len[b].item()
                ramp = torch.linspace(1, 0, L, device=x.device)
                out[b, -L:] = out[b, -L:] * ramp
        return out

    def random_white_noise(self, x, snr_db_min=10.0, snr_db_max=30.0, p=0.8):
        # additive white noise at random SNR
        if torch.rand(1).item() > p:
            return x
        B, T = x.shape
        power = (x**2).mean(dim=1, keepdim=True).clamp_min(1e-9)
        snr_db = (torch.rand(B, 1, device=x.device) * (snr_db_max - snr_db_min) + snr_db_min)
        snr = 10.0 ** (snr_db / 10.0)
        noise_power = (power / snr).clamp_min(1e-12)
        noise = torch.randn_like(x) * torch.sqrt(noise_power)
        return (x + noise).clamp_(-1.0, 1.0)

    def _one_view(self, x):
        # compose a lightweight chain; extend as needed
        x = self.random_gain(x)
        x = self.random_time_shift(x)
        x = self.random_fade(x, p=0.5)
        x = self.random_white_noise(x, p=0.8)
        return x

    def __call__(self, x):
        # return two independent augmented views
        v1 = self._one_view(x)
        v2 = self._one_view(x.clone())
        return v1, v2

AUG = AudioAug()

# ----------------------------
# Memory queue for negatives (MoCo style)
# ----------------------------
class MemoryQueue(nn.Module):
    def __init__(self, dim=D, K=QUEUE_SIZE, device="cpu"):
        super().__init__()
        self.K = K
        self.register_buffer("queue", F.normalize(torch.randn(K, dim, device=device), dim=1))
        self.register_buffer("ptr", torch.zeros(1, dtype=torch.long, device=device))

    @torch.no_grad()
    def enqueue(self, keys):  # keys: [B,D], already normalized
        B = keys.size(0)
        K = self.K
        ptr = int(self.ptr.item())
        if ptr + B <= K:
            self.queue[ptr:ptr+B] = keys
            ptr = (ptr + B) % K
        else:
            n1 = K - ptr
            self.queue[ptr:] = keys[:n1]
            self.queue[:B-n1] = keys[n1:]
            ptr = (B - n1) % K
        self.ptr[0] = ptr

    def get(self):
        return self.queue  # [K,D]

QUEUE = MemoryQueue(dim=D, K=QUEUE_SIZE, device=device).to(device)

# ----------------------------
# Optimizer + scheduler for encoder_q only (encoder_k is momentum-updated)
# ----------------------------
opt = torch.optim.Adam(encoder_q.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)
sched = CosineAnnealingLR(opt, T_max=EPOCHS, eta_min=LR * 0.1)

# Ensure key encoder starts as a copy of query encoder; no gradients for encoder_k
encoder_q.to(device).train()
encoder_k.to(device).eval()
for p in encoder_k.parameters():
    p.requires_grad_(False)
with torch.no_grad():
    for p_k, p_q in zip(encoder_k.parameters(), encoder_q.parameters()):
        p_k.data.copy_(p_q.data)

# ----------------------------
# Helper: forward encoder and reduce features to [B,D]
# ----------------------------
def encode_reduce(encoder, x):
    # x can be [B,T] or [B,1,T] â€” convert to [B,T]
    if x.dim() == 3 and x.size(1) == 1:
        x = x[:, 0, :]
    # forward
    z = encoder(x)  # expected [B,D] or [B,D,L,...]
    if z.dim() == 3:           # [B,D,L] -> mean over L
        z = z.mean(dim=2)
    elif z.dim() > 3:          # [B,D,H,W,...] -> mean over extras
        z = z.mean(dim=tuple(range(2, z.dim())))
    return z  # [B,D]

# ----------------------------
# Training loop
# ----------------------------
scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())

for epoch in range(EPOCHS):
    encoder_q.train()
    total_loss, total_contrast, total_len = 0.0, 0.0, 0.0
    n_batches = 0

    for wavs, labels in train_loader:
        # Move to device and ensure float + proper shape
        x = wavs.to(device).float()      # [B, T] or [B, 1, T]
        y = labels.to(device).long()     # [B]

        # Make sure each sample has length N_SAMPLES (guard; your preprocess already does this)
        if x.dim() == 3 and x.size(-1) != N_SAMPLES:
            T = x.size(-1)
            if T < N_SAMPLES:
                x = F.pad(x, (0, N_SAMPLES - T))
            else:
                x = x[..., :N_SAMPLES]
        elif x.dim() == 2 and x.size(-1) != N_SAMPLES:
            T = x.size(-1)
            if T < N_SAMPLES:
                x = F.pad(x, (0, N_SAMPLES - T))
            else:
                x = x[:, :N_SAMPLES]

        # Create two augmented views from [B,T]
        xb = x[:, 0, :] if (x.dim() == 3 and x.size(1) == 1) else (x if x.dim() == 2 else x.squeeze(1))
        v1, v2 = AUG(xb)  # both are [B,T]

        opt.zero_grad(set_to_none=True)
        with torch.autocast(device_type="cuda", dtype=torch.float16, enabled=torch.cuda.is_available()):
            # Query and Key embeddings
            q = encode_reduce(encoder_q, v1)     # [B,D]
            with torch.no_grad():
                k = encode_reduce(encoder_k, v2) # [B,D]

            # Normalize for InfoNCE
            q = F.normalize(q, dim=1)
            k = F.normalize(k, dim=1)

            # Positive logits: qÂ·k+; Negative logits: qÂ·Q (queue)
            l_pos = torch.einsum('bd,bd->b', q, k).unsqueeze(1) / TAU           # [B,1]
            q_neg = QUEUE.get()                                                 # [K,D]
            l_neg = (q @ q_neg.T) / TAU                                         # [B,K]
            logits = torch.cat([l_pos, l_neg], dim=1)                           # [B,1+K]
            targets = torch.zeros(q.size(0), dtype=torch.long, device=device)   # positives at index 0
            L_contrast = F.cross_entropy(logits, targets)

            # Length Loss: encourage larger ||z|| for real, and below margin for fake
            # Use non-normalized features for length (recompute from encoder_q output without L2 norm)
            z_nonorm = encode_reduce(encoder_q, v1.detach())
            z_norms  = torch.norm(z_nonorm, p=2, dim=1)       # [B]
            y_real   = (y == REAL_CLASS_ID).float()           # [B] in {0,1}
            L_len_real = y_real * W_REAL * z_norms
            L_len_fake = (1.0 - y_real) * torch.clamp(MARGIN - z_norms, min=0.0)
            L_len = (L_len_real + L_len_fake).mean()

            loss = L_contrast + ALPHA * L_len

        scaler.scale(loss).backward()
        scaler.step(opt)
        scaler.update()

        # Momentum update for encoder_k (EMA)
        with torch.no_grad():
            for p_k, p_q in zip(encoder_k.parameters(), encoder_q.parameters()):
                p_k.data.mul_(MOMENTUM).add_(p_q.data, alpha=1.0 - MOMENTUM)

        # Update the memory queue with current keys
        with torch.no_grad():
            QUEUE.enqueue(k)

        total_loss     += loss.item()
        total_contrast += L_contrast.item()
        total_len      += L_len.item()
        n_batches      += 1

    sched.step()
    avg_loss = total_loss / max(n_batches, 1)
    avg_cl   = total_contrast / max(n_batches, 1)
    avg_ll   = total_len / max(n_batches, 1)
    print(f"[Pretrain][Epoch {epoch+1:03d}/{EPOCHS}] "
          f"loss={avg_loss:.4f} (InfoNCE={avg_cl:.4f} + {ALPHA}*Len={avg_ll:.4f}), "
          f"tau={TAU}, m={MOMENTUM}, lr={sched.get_last_lr()[0]:.2e}")

# ----------------------------
# Save encoder_q (and optionally encoder_k) after pretraining
# ----------------------------
os.makedirs(CKPT_DIR, exist_ok=True)
torch.save(
    {
        "encoder_q": encoder_q.state_dict(),
        "encoder_k": encoder_k.state_dict(),
        "dim": D,
        "queue_size": QUEUE_SIZE,
        "tau": TAU,
        "momentum": MOMENTUM,
        "alpha": ALPHA,
        "margin": MARGIN,
        "w_real": W_REAL,
        "sr": SR,
        "n_samples": N_SAMPLES,
        "real_class_id": REAL_CLASS_ID,
        "epochs": EPOCHS,
    },
    CKPT_PATH
)
print(f"âœ… CLAD-style pretraining finished. Checkpoint saved to: {CKPT_PATH}")

# ================== Single-Cell: Flexible checkpoint loading (no other code changes) ==================
import torch
from collections import OrderedDict
from pathlib import Path

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ---- choose ONE of the modes below (no other code changes needed) ----
# "none"                -> ×œ× ×˜×•×¢×Ÿ ×›×œ×•× (××™××•×Ÿ ×××¤×¡)
# "moco_full"           -> ×× ×¡×” ×œ×˜×¢×•×Ÿ state_dict ××œ× ×©×œ ×”××•×“×œ ×©×œ×š (MoCo_v2 + RawNet) ×©× ×©××¨ ×‘×¢×‘×¨ ×××š
# "rawnet_encoder_only" -> ×× ×¡×” ×œ×˜×¢×•×Ÿ ×¨×§ ××ª ×”××§×•×“×“ encoder_q ×-ckpt ×—×™×¦×•× ×™ ×©×“×•××” ×œ-RawNet (×œ× MoCo),
#                          ×¢× ××™×¤×•×™ ×©××•×ª ×¤×©×•×˜. ×× ×œ× ×™×ª××™× â€” ××“×œ×’.
CKPT_MODE = "none"              # â† ×©× ×” ×œ-"moco_full" ××• "rawnet_encoder_only" ×× ×™×© ×œ×š ×§×•×‘×¥ ××ª××™×
CKPT_PATH = "checkpoints/my_moco_rawnet_epXX.pth"  # ×œ×©×™××•×© ×¢× "moco_full" (state_dict ×©×©××¨×ª ××”××•×“×œ ×”×–×”)
RAWNET_CKPT_PATH = "checkpoints/rawnet_encoder_only.pth"  # ×œ×©×™××•×© ×¢× "rawnet_encoder_only" (×× ×™×©)

MIN_MATCH_RATIO = 0.80  # ×¤×—×•×ª ××–×” = ××“×œ×’×™× ××•×˜×•××˜×™×ª

def _strip_prefix(d, prefix="module."):
    return { (k[len(prefix):] if k.startswith(prefix) else k): v for k,v in d.items() }

def _load_state(path):
    obj = torch.load(path, map_location="cpu")
    return obj.get("state_dict", obj)

def _try_load_full_moco(model, path):
    """Expect a state_dict ×©× ×©××¨ ××”××•×œ ××•×“×œ ×©×œ×š (××•×ª×• MoCo_v2 + RawNet + projection heads)."""
    state = _load_state(path)
    state = _strip_prefix(state, "module.")
    msd = model.state_dict()
    new_sd = OrderedDict(msd)
    matched = skipped = 0
    for k, v in state.items():
        if k in new_sd and new_sd[k].shape == v.shape:
            new_sd[k] = v; matched += 1
        else:
            skipped += 1
    model.load_state_dict(new_sd)
    ratio = matched / (matched + skipped) if (matched + skipped) else 0.0
    print(f"[CKPT full] matched={matched}, skipped={skipped}, ratio={ratio:.2%}")
    return ratio

def _try_load_rawnet_encoder_only(model, path):
    """
    Load only encoder_q.* from a rawnet-like checkpoint (NOT MoCo).
    ××™×¤×•×™ ×‘×¡×™×¡×™: 'encoder.' -> 'encoder_q.'; ×× ×©××•×ª ××—×¨×™× â€“ ×™×ª××™× ×¨×§ ××” ×©×‘×××ª ×–×”×” ×‘×¦×•×¨×”.
    """
    state = _load_state(path)
    state = _strip_prefix(state, "module.")
    mapped = OrderedDict()
    for k, v in state.items():
        nk = k
        if nk.startswith("encoder."):                # × ×¤×•×¥ ×‘×¦'×§×¤×•×™× ×˜×™× ×©×œ ××§×•×“×“ ×‘×œ×‘×“
            nk = "encoder_q." + nk[len("encoder."):]
        elif nk.startswith("rawnet.") or nk.startswith("model.encoder."):
            # ×”×•×¡×¤×ª ×›×œ×œ×™× ×œ×¤×™ ×”×¦×•×¨×š; × ×©××™×¨ ×©××¨× ×™ ×›×‘×¨×™×¨×ª ××—×“×œ
            nk = "encoder_q." + nk.split(".", 1)[-1]
        mapped[nk] = v

    msd = model.state_dict()
    new_sd = OrderedDict(msd)
    matched = skipped = 0
    for k, v in mapped.items():
        if k.startswith("encoder_q.") and (k in new_sd) and (new_sd[k].shape == v.shape):
            new_sd[k] = v; matched += 1
        else:
            skipped += 1

    model.load_state_dict(new_sd)
    ratio = matched / (matched + skipped) if (matched + skipped) else 0.0
    print(f"[CKPT rawnet-encoder] matched={matched}, skipped={skipped}, ratio={ratio:.2%}")
    return ratio

def _copy_q_to_k(model):
    sd = model.state_dict()
    updates = {}
    for k, v in sd.items():
        if k.startswith("encoder_q."):
            twin = "encoder_k." + k[len("encoder_q."):]
            if twin in sd and sd[twin].shape == v.shape:
                updates[twin] = v.clone()
    sd.update(updates)
    model.load_state_dict(sd)
    print(f"Synced encoder_q â†’ encoder_k for {len(updates)} tensors.")

# ---------------- run chosen mode ----------------
loaded = False
if CKPT_MODE == "moco_full" and Path(CKPT_PATH).exists():
    ratio = _try_load_full_moco(model, CKPT_PATH)
    if ratio >= MIN_MATCH_RATIO:
        _copy_q_to_k(model)
        loaded = True
    else:
        print("[FALLBACK] Low match ratio â€” skipping pretrained load.")

elif CKPT_MODE == "rawnet_encoder_only" and Path(RAWNET_CKPT_PATH).exists():
    ratio = _try_load_rawnet_encoder_only(model, RAWNET_CKPT_PATH)
    if ratio >= MIN_MATCH_RATIO:
        _copy_q_to_k(model)   # × ×•×— ×œ×”×ª×—×™×œ q ×•×§ ×‘××•×ª×• ××¦×‘
        loaded = True
    else:
        print("[FALLBACK] Low match ratio â€” skipping encoder_q preload.")

else:
    if CKPT_MODE != "none":
        print(f"[INFO] CKPT_MODE={CKPT_MODE} but file not found. Skipping.")

# ---- Advice: if no pretrained was loaded, do NOT freeze the encoder ----
if not loaded:
    for p in encoder_q.parameters():
        p.requires_grad = True
    print("No compatible checkpoint loaded â†’ training from scratch (encoder unfrozen).")

# ---- Quick smoke test (keeps your shapes consistent) ----
model.eval(); encoder_q.eval()
with torch.no_grad():
    dummy = torch.randn(2, 64600, device=device)  # [B,T] ×›××• ×‘×¤×¨×”-×¤×¨×•×¡×¡ ×©×œ×š
    try:
        f = encoder_q(dummy)
        if f.dim()==3:  f = f.mean(2)
        elif f.dim()>3: f = f.mean(dim=tuple(range(2,f.dim())))
        print("Forward OK. Encoder feature shape:", tuple(f.shape))
    except Exception as e:
        print("Forward FAILED:", repr(e))
# =====================================================================================================

# ===== QUICK FORWARD SMOKE TEST =====
model.eval()
encoder_q.eval()
with torch.no_grad():
    dummy = torch.randn(2, 64600, device=device)  # [B,T] ×œ×¤×™ ×”-preprocess ×©×œ×š
    f = encoder_q(dummy)                          # RawNet expects [B, T]
    if f.dim() == 3:
        f = f.mean(dim=2)
    elif f.dim() > 3:
        f = f.mean(dim=tuple(range(2, f.dim())))
    print("Encoder feature shape:", tuple(f.shape))

# This cell performs a one-batch overfitting sanity check.
# We FREEZE the pretrained encoder (`encoder_q`), extract fixed features `f` from a single batch,
# optionally normalize them, and then train a small probe head (`probe`) with cross-entropy to drive
# the batch accuracy to ~100%. If accuracy does not approach 1.0, gradients or features may be faulty.

import torch
import torch.nn as nn

# -------------------------------
# 1) Get one batch and move to device
# -------------------------------
wavs, labels = next(iter(train_loader))     # uses existing train_loader
x = wavs.to(device).float()                 # (B, 1, T) or similar; keep dtype consistent
y = labels.to(device).long()                # class indices as long for CE

# -------------------------------
# 2) Freeze encoder_q and set to eval to fix BN/Dropout behavior
# -------------------------------
encoder_q.eval()
for p in encoder_q.parameters():
    p.requires_grad_(False)

# -------------------------------
# 3) Extract features once (no gradient through encoder)
#    Handle variable tensor ranks robustly and reduce temporal/extra dims by mean where needed
# -------------------------------
with torch.no_grad():
    f = encoder_q(x)                        # expected shape (B, D) or (B, D, L, ...)
    if f.dim() == 3:
        # e.g., (B, D, L) -> mean over temporal axis
        f = f.mean(dim=2)
    elif f.dim() > 3:
        # e.g., (B, D, H, W, ...) -> mean over all non-(B,D) axes
        reduce_axes = tuple(range(2, f.dim()))
        f = f.mean(dim=reduce_axes)
# ensure features are detached (safety; already in no_grad)
f = f.detach()

# -------------------------------
# 4) Optional feature normalization (per-dimension z-score)
#    Clamp std to avoid division by zero on tiny batches
# -------------------------------
f = (f - f.mean(dim=0, keepdim=True)) / (f.std(dim=0, keepdim=True).clamp_min(1e-5))

# -------------------------------
# 5) Define probe head, optimizer, and loss (keep variable names consistent)
# -------------------------------
probe = nn.Sequential(
    nn.Linear(f.shape[1], 128),
    nn.ReLU(),
    nn.Linear(128, 2)                       # binary: real vs fake
).to(device)

opt  = torch.optim.AdamW(probe.parameters(), lr=1e-2, weight_decay=0.0)
crit = nn.CrossEntropyLoss()

# -------------------------------
# 6) Train the probe on the single batch for several steps
# -------------------------------
torch.manual_seed(0)                        # reproducibility for this check

for step in range(1000):                    # increase to 2000 if needed
    logits = probe(f)                       # (B, 2); no no_grad here to allow backprop into probe
    loss = crit(logits, y)

    opt.zero_grad()
    loss.backward()
    opt.step()

    if (step + 1) % 200 == 0:
        with torch.no_grad():
            acc = (logits.argmax(dim=1) == y).float().mean().item()
        print(f"step {step+1}: loss {loss.item():.4f}, acc {acc:.3f}")

# -------------------------------
# 7) Final accuracy report on the same batch
# -------------------------------
with torch.no_grad():
    final_acc = (logits.argmax(dim=1) == y).float().mean().item()
print("Final overfit-batch acc:", final_acc)

# Notes:
# - This cell preserves existing variable names: wavs, labels, x, y, f, probe, opt, crit, logits.
# - encoder_q remains frozen and in eval() throughout; only `probe` is trained here.

#1 =========================================
# Fine-tuning: linear classifier on top of FROZEN encoder_q
# (RawNet encoder_q expects input shape [B, T], not [B,1,T]!)
# =========================================
from pathlib import Path
import torch, torch.nn as nn, torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import torchaudio
import torch.nn.functional as F
import os

# Change directory to CLAD
os.chdir('/content/CLAD')

from Model import MoCo_v2, RawNetEncoderBaseline

# Step 1: Define the RawNet encoder configuration
d_args = {
    "in_channels": 1,
    "first_conv": 251,
    "filts": [
        128,  # output channels for sinc conv
        [128, 128],  # block0 and block1
        [128, 256],  # block2
        [256, 256]   # block3-5
    ],
    "nb_fc_node": 1024,
    "gru_node": 1024,
    "nb_gru_layer": 3,
    "nb_classes": 2
}

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Step 2: Create both encoders
encoder_q = RawNetEncoderBaseline(d_args, device)
encoder_k = RawNetEncoderBaseline(d_args, device)

# Step 3: Create the MoCo_v2 model
model = MoCo_v2(
    encoder_q=encoder_q,
    encoder_k=encoder_k,
    queue_feature_dim=1024,  # matches encoder output
    mlp=True,
    return_q=True
)

# Step 4: Load pretrained weights
ckpt_path = Path("pretrained_models/CLAD_150_10_2310.pth.tar")
if not ckpt_path.exists():
    raise FileNotFoundError(f"Checkpoint not found: {ckpt_path}")

ckpt = torch.load(ckpt_path, map_location='cpu')
state_dict = ckpt.get("state_dict", ckpt)

missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
print("Missing keys:", missing_keys)
print("Unexpected keys:", unexpected_keys)

model.to(device)
model.eval()

print("CLAD model loaded and ready.")


# -------- Config --------
TARGET_SR  = 16000
TARGET_LEN = 64600
LABELS     = {"real": 0, "fake": 1}
BATCH_SIZE = 16
EPOCHS     = 8
LR         = 1e-3

# -------- Dataset: returns [T] (mono 16k, fixed length) --------
class AudioDataset(Dataset):
    def __init__(self, root_dir: str):
        self.samples = []
        root = Path(root_dir)
        for cls in ("real", "fake"):
            base = root / cls
            if not base.exists():
                raise FileNotFoundError(f"Missing directory: {base}")
            for p in base.rglob("*.wav"):
                if p.is_file():
                    self.samples.append((p, LABELS[cls]))
        if len(self.samples) == 0:
            raise ValueError(f"No .wav files found under {root_dir}. Ensure split+preprocess ran correctly.")
        self._resamplers = {}

    def _fix(self, path: Path) -> torch.Tensor:
        wav, sr = torchaudio.load(str(path))  # [C,T]
        wav = wav.float()
        if wav.shape[0] > 1:
            wav = wav.mean(dim=0, keepdim=True)       # [1,T]
        if sr != TARGET_SR:
            if sr not in self._resamplers:
                self._resamplers[sr] = torchaudio.transforms.Resample(orig_freq=sr, new_freq=TARGET_SR)
            wav = self._resamplers[sr](wav)           # [1,T']
        T = wav.shape[-1]
        if T < TARGET_LEN:
            wav = F.pad(wav, (0, TARGET_LEN - T))
        else:
            wav = wav[..., :TARGET_LEN]
        return wav.squeeze(0)                         # -> [T]

    def __len__(self): return len(self.samples)
    def __getitem__(self, idx):
        path, label = self.samples[idx]
        return self._fix(path), label                 # ([T], label)

# -------- DataLoaders --------
train_ds = AudioDataset("/content/my_audio_split/train")
val_ds   = AudioDataset("/content/my_audio_split/val")
train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  drop_last=False)
val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, drop_last=False)

# -------- Freeze encoder --------
encoder_q.to(device).eval()
for p in encoder_q.parameters():
    p.requires_grad = False
encoder_q.eval().to(device)

# -------- Infer feature dimension D (NOTE: pass [B,T] to encoder_q) --------
with torch.no_grad():
    wavs0, _ = next(iter(train_loader))     # wavs0: [B, T]
    x0 = wavs0.to(device)                   # [B, T]  <-- no unsqueeze
    feat0 = encoder_q(x0)                   # RawNet expects [B, T]
    # If encoder returns >2D (e.g., [B, C, T']), reduce over time dims:
    if feat0.dim() == 3:
        feat0 = feat0.mean(dim=2)           # [B, C]
    elif feat0.dim() > 3:
        feat0 = feat0.mean(dim=tuple(range(2, feat0.dim())))
    D = feat0.shape[1]

# -------- Classifier, loss, optimizer --------
classifier = nn.Sequential(
    nn.Linear(D, 64),  # use your probed feature dim here
    nn.ReLU(),
    nn.Dropout(0.2),
    nn.Linear(64, 2)
).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(classifier.parameters(), lr=2e-3, weight_decay=1e-4)

# -------- Train + Validate --------
for epoch in range(1, EPOCHS + 1):
    # ---- Train ----
    classifier.train()
    tr_loss = tr_correct = tr_total = 0.0

    for wavs, labels in train_loader:
        x = wavs.to(device).float()          # [B, T]
        y = labels.to(device).long()         # 0=real, 1=fake

        # encoder forward (NO grad here)
        with torch.no_grad():
            f = encoder_q(x)                 # (B, D) or (B, D, T, ...)
            if f.dim() == 3:
                f = f.mean(dim=2)            # pool time -> (B, D)
            elif f.dim() > 3:
                f = f.mean(dim=tuple(range(2, f.dim())))  # reduce extra dims

        # head forward WITH grad
        logits = classifier(f)               # (B, 2)
        loss = criterion(logits, y)

        optimizer.zero_grad(set_to_none=True)
        loss.backward()
        optimizer.step()

        tr_loss   += loss.item() * x.size(0)
        tr_correct += (logits.argmax(dim=1) == y).sum().item()
        tr_total  += x.size(0)

    # ---- Val ----
    classifier.eval()
    va_loss, va_correct, va_total = 0.0, 0, 0
    with torch.no_grad():
      for wavs, labels in val_loader:
          x = wavs.to(device).float()
          y = labels.to(device).long()
          f = encoder_q(x)
          if f.dim() == 3:
              f = f.mean(dim=2)
          elif f.dim() > 3:
              f = f.mean(dim=tuple(range(2, f.dim())))
          logits = classifier(f)
          va_loss   += criterion(logits, y).item() * x.size(0)
          va_correct += (logits.argmax(dim=1) == y).sum().item()
          va_total  += x.size(0)

    print(f"Epoch {epoch}/{EPOCHS} | "
          f"Train Loss: {tr_loss/tr_total:.4f}, Acc: {tr_correct/tr_total:.3f} | "
          f"Val Loss: {va_loss/va_total:.4f}, Acc: {va_correct/va_total:.3f}")

#2 =========================================
# Evaluate model on validation/test sets
# =========================================
from sklearn.metrics import classification_report
import torch
from pathlib import Path
from torch.utils.data import DataLoader

def evaluate(loader, split_name="val", model=None, classifier=None, device=None):
    if model is None or classifier is None or device is None:
        raise ValueError("model, classifier, and device must be provided to evaluate function.")

    # Ensure classifier is on the correct device
    classifier.to(device)
    classifier.eval()

    all_preds, all_labels = [], []
    with torch.no_grad():
        for wavs, labels in loader:
            # Ensure data is on the correct device
            wavs, labels = wavs.to(device), labels.to(device)

            # Use the encoder from the loaded model
            feats = model.encoder_q(wavs)
            # Pool any extra time/freq dims so we have (B,D)
            if feats.dim() == 3:
                feats = feats.mean(dim=2)
            elif feats.dim() > 3:
                feats = feats.mean(dim=tuple(range(2, feats.dim())))

            preds = classifier(feats)
            all_preds.extend(torch.argmax(preds, dim=1).cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    print(f"=== {split_name.upper()} RESULTS ===")
    print(classification_report(all_labels, all_preds, target_names=["real","fake"]))

# Run evaluation
# Pass the necessary objects to the evaluate function
evaluate(val_loader, "val", model=model, classifier=classifier, device=device)

# Re-create test_loader as it might not be defined in the current runtime
test_ds = AudioDataset("/content/my_audio_split/test")
test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)
evaluate(test_loader, "test", model=model, classifier=classifier, device=device)

#×‘××§×•× 12
# ===== Unified predict_file: encoder_q + classifier (replaces any older predict_file) =====
import numpy as np
import torch.nn.functional as F
import torchaudio

TARGET_SR  = 16000
TARGET_LEN = 64600

def _load_and_fix(path: Path):
    wav, sr = torchaudio.load(str(path))   # [C,T]
    wav = wav.float()
    if wav.shape[0] > 1:
        wav = wav.mean(dim=0, keepdim=True)
    if sr != TARGET_SR:
        res = torchaudio.transforms.Resample(orig_freq=sr, new_freq=TARGET_SR)
        wav = res(wav)                      # [1,T']
    T = wav.shape[-1]
    if T < TARGET_LEN:
        wav = F.pad(wav, (0, TARGET_LEN - T))
    else:
        wav = wav[..., :TARGET_LEN]
    return wav.squeeze(0)                   # -> [T]

def predict_file(path: Path, encoder, classifier, device):
    """Return (label, fake_conf, real_conf, embedding) using encoder_q + classifier."""
    wav = _load_and_fix(path)               # [T]
    x = wav.unsqueeze(0).to(device)         # [1,T]
    encoder.eval(); classifier.eval()
    with torch.no_grad():
        feats = encoder(x)
        if feats.dim() == 3:
            feats = feats.mean(dim=2)
        elif feats.dim() > 3:
            feats = feats.mean(dim=tuple(range(2, feats.dim())))
        logits = classifier(feats)          # [1,2]
        probs = F.softmax(logits, dim=1)[0].detach().cpu().numpy()
    label = "fake" if int(np.argmax(probs)) == 1 else "real"
    return label, float(probs[1]), float(probs[0]), feats.squeeze(0).cpu().numpy()

# # 12
# import torch
# import torchaudio
# import numpy as np
# import torch.nn.functional as F
# from pathlib import Path

# # Set device (important!)
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# # Function to load audio as 16k mono
# def load_audio_16k_mono(path: Path):
#     """Load an audio file as mono 16kHz."""
#     waveform, sr = torchaudio.load(str(path))
#     if waveform.shape[0] > 1:
#         waveform = torch.mean(waveform, dim=0, keepdim=True)
#     waveform = waveform.squeeze().numpy()
#     if sr != 16000:
#         resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)
#         waveform = resampler(torch.tensor(waveform).unsqueeze(0)).squeeze().numpy()
#     return waveform, 16000

# # Function to run prediction
# def predict_file(path: Path):
#     """Run CLAD on one WAV file. Returns (pred_label, fake_conf, real_conf)."""
#     y, sr = load_audio_16k_mono(path)
#     x = torch.from_numpy(y).float().unsqueeze(0).unsqueeze(1).to(device)  # [B=1, C=1, T]
#     with torch.no_grad():
#         logits = model(x)  # expected shape [B, 2] for [real, fake]
#         probs  = F.softmax(logits, dim=1)[0].detach().cpu().numpy()
#     label = "fake" if int(np.argmax(probs)) == 1 else "real"
#     fake_conf = float(probs[1])
#     real_conf = float(probs[0])
#     return label, fake_conf, real_conf

# #13
# # This cell scans your external real/fake folders, runs CLAD, and writes a CSV with results.
# # It never writes inside the CLAD repo.
# import torch
# from pathlib import Path
# import os
# # os.chdir('/content/CLAD') # No need to change directory here for prediction

# import glob
# import pandas as pd
# import torch.nn.functional as F

# # Assuming 'model', 'classifier', and 'device' are defined in previous cells
# # (Specifically, 'model' from loading CLAD and 'classifier' from fine-tuning)
# if 'model' not in globals() or 'classifier' not in globals() or 'device' not in globals():
#      raise RuntimeError("CLAD model, classifier, or device not found. Please run the preceding cells first.")

# # Ensure model and classifier are on the correct device and in eval mode
# model.to(device).eval()
# classifier.to(device).eval()

# # Function to load audio as 16k mono and preprocess
# def load_and_preprocess_audio(path: Path):
#     """Load an audio file, resample to 16kHz mono, pad/trim to TARGET_LEN."""
#     wav, sr = torchaudio.load(str(path))   # [C,T]
#     wav = wav.float()
#     if wav.shape[0] > 1:
#         wav = wav.mean(dim=0, keepdim=True)   # [1,T]
#     if sr != TARGET_SR:
#         res = torchaudio.transforms.Resample(orig_freq=sr, new_freq=TARGET_SR)
#         wav = res(wav)                         # [1,T']
#     T = wav.shape[-1]
#     if T < TARGET_LEN:
#         wav = F.pad(wav, (0, TARGET_LEN - T))
#     else:
#         wav = wav[..., :TARGET_LEN]
#     return wav.squeeze(0) # -> [T]


# # Function to run prediction
# def predict_file(path: Path, encoder, classifier, device):
#     """Run CLAD encoder + classifier on one WAV file. Returns (pred_label, fake_conf, real_conf)."""
#     try:
#         wav = load_and_preprocess_audio(path) # [T]
#         x = wav.unsqueeze(0).to(device)       # [B=1, T]

#         with torch.no_grad():
#             # Use only the encoder part of the MoCo model
#             feats = encoder(x) # RawNet expects [B, T]
#             # Pool any extra time/freq dims if necessary
#             if feats.dim() == 3:
#                 feats = feats.mean(dim=2)
#             elif feats.dim() > 3:
#                  feats = feats.mean(dim=tuple(range(2, feats.dim())))

#             # Pass features through the classifier
#             logits = classifier(feats)        # [B=1, 2]
#             probs  = F.softmax(logits, dim=1)[0].detach().cpu().numpy()

#         label = "fake" if int(np.argmax(probs)) == 1 else "real"
#         fake_conf = float(probs[1])
#         real_conf = float(probs[0])
#         return label, fake_conf, real_conf, feats.squeeze(0).cpu().numpy() # Also return embedding

#     except Exception as e:
#         print(f"Error processing file {path}: {e}")
#         return "error", 0.0, 0.0, None # Return None for embedding on error


# # Function to extract embedding (optional)
# def embed_file(path: Path, encoder, device):
#     """Extract embedding from CLAD encoder for one WAV file."""
#     try:
#         wav = load_and_preprocess_audio(path) # [T]
#         x = wav.unsqueeze(0).to(device)       # [B=1, T]
#         with torch.no_grad():
#             feats = encoder(x) # RawNet expects [B, T]
#             # Pool any extra time/freq dims if necessary
#             if feats.dim() == 3:
#                 feats = feats.mean(dim=2)
#             elif feats.dim() > 3:
#                  feats = feats.mean(dim=tuple(range(2, feats.dim())))
#         return feats.squeeze(0).cpu().numpy()
#     except Exception as e:
#         print(f"Error embedding file {path}: {e}")
#         return None


# REAL_DIR = Path("/content/my_audio/real")
# FAKE_DIR = Path("/content/my_audio/fake")
# OUT_DIR  = Path("/content/my_audio_results")
# OUT_DIR.mkdir(parents=True, exist_ok=True)

# real_files = sorted(REAL_DIR.rglob("*.wav"))
# fake_files = sorted(FAKE_DIR.rglob("*.wav"))

# print(f"Found {len(real_files)} real and {len(fake_files)} fake WAVs.")

# rows = []
# for f in real_files + fake_files:
#     p = Path(f)
#     # Pass encoder_q, classifier, and device to the prediction function
#     pred, fake_conf, real_conf, emb = predict_file(p, model.encoder_q, classifier, device)
#     rows.append({
#         "file": str(p),
#         "pred": pred,
#         "fake_conf": fake_conf,
#         "real_conf": real_conf,
#         "embedding_dim": (len(emb) if emb is not None else None),
#         "embedding": emb.tolist() if emb is not None else None # Convert numpy array to list for CSV
#     })

# df = pd.DataFrame(rows)
# csv_path = OUT_DIR / "clad_results.csv"
# df.to_csv(csv_path, index=False)
# print("Saved CSV:", csv_path)
# display(df.head())

#14
# This cell is optional. It visualizes embeddings in 2D if they were extracted.
# If embedding is None for all files (no encoder exposed), skip this cell.

import numpy as np
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

emb_rows = [r for r in rows if r["embedding"] is not None]
if len(emb_rows) >= 2:
    X = np.vstack([np.array(r["embedding"], dtype=np.float32) for r in emb_rows])
    y = np.array([r["pred"] for r in emb_rows])

    X2d = TSNE(n_components=2, random_state=0, perplexity=min(15, len(emb_rows)-1)).fit_transform(X)

    plt.figure(figsize=(6,5))
    for cls, marker in [("real", "o"), ("fake", "x")]:
        mask = (y == cls)
        plt.scatter(X2d[mask,0], X2d[mask,1], label=cls, marker=marker, alpha=0.85)
    plt.title("CLAD embeddings (t-SNE)")
    plt.legend()
    plt.tight_layout()
    plt.show()
else:
    print("No embeddings available (encoder not exposed or too few samples).")