{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "cell_execution_strategy": "setup"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# extract_data\n"
      ],
      "metadata": {
        "id": "CeCXF8rG9JdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import shutil\n",
        "# import os\n",
        "\n",
        "# # הנתיב לתיקייה שנוצרה בהרצה הקודמת\n",
        "# destination_folder = \"/content/vctk_full\"\n",
        "\n",
        "# # בדיקה אם התיקייה קיימת, ואז מחיקה\n",
        "# if os.path.exists(destination_folder):\n",
        "#     shutil.rmtree(destination_folder)\n",
        "#     print(f\" התיקייה '{destination_folder}' נמחקה בהצלחה.\")\n",
        "# else:\n",
        "#     print(f\"ℹ התיקייה '{destination_folder}' לא קיימת, אין מה למחוק.\")\n"
      ],
      "metadata": {
        "id": "qsB_qTqinrA3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# List contents to verify paths (optional)\n",
        "root_path = '/content/drive/My Drive/'\n",
        "print(\"Contents of 'My Drive':\", os.listdir(root_path))\n",
        "\n",
        "subfolder_path = '/content/drive/My Drive/Colab Notebooks/'\n",
        "print(\"Contents of 'Colab Notebooks':\", os.listdir(subfolder_path))\n",
        "\n",
        "# Define paths\n",
        "zip_file = \"/content/drive/My Drive/Colab Notebooks/archive.zip\"  # Path to your ZIP file\n",
        "destination_folder = \"/content/vctk_samples\"  # Where to extract selected data\n",
        "wanted_speakers = [\"p225\", \"p226\", \"p227\", \"p228\"]  # Select specific speakers\n",
        "\n",
        "# Check if the ZIP file exists\n",
        "if os.path.isfile(zip_file):\n",
        "    print(\" ZIP file found:\", zip_file)\n",
        "else:\n",
        "    raise FileNotFoundError(f\" ZIP file not found: {zip_file}\")\n",
        "\n",
        "# Create destination folder if it doesn't exist\n",
        "os.makedirs(destination_folder, exist_ok=True)\n",
        "\n",
        "# Selectively extract only desired speaker folders from the ZIP\n",
        "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "    extracted_files = 0\n",
        "    for file in zip_ref.namelist():\n",
        "        if any(f\"VCTK-Corpus/wav48/{spk}/\" in file or f\"VCTK-Corpus/txt/{spk}/\" in file for spk in wanted_speakers):\n",
        "            # Ensure directory structure is preserved\n",
        "            target_path = os.path.join(destination_folder, file)\n",
        "            os.makedirs(os.path.dirname(target_path), exist_ok=True)\n",
        "            with zip_ref.open(file) as source, open(target_path, 'wb') as target:\n",
        "                shutil.copyfileobj(source, target)\n",
        "            extracted_files += 1\n",
        "\n",
        "print(f\"\\n Extraction complete: {extracted_files} files were extracted.\")\n",
        "print(f\" Extracted data is available in: {destination_folder}\")\n"
      ],
      "metadata": {
        "id": "9YxlgeQE9ufN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abe54fc2-edfd-4744-8ae5-e0cb94de081a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Contents of 'My Drive': ['Colab Notebooks', 'Deep learning', 'ColabEnvLocks', 'ColabModels', 'Colab_Data']\n",
            "Contents of 'Colab Notebooks': ['FinalProjectCS.ipynb', 'archive.zip', 'fake_audio.zip', 'Copy of Welcome To Colab', 'Untitled0.ipynb', 'Untitled1.ipynb', 'FinalProjectCS_reset_setup.ipynb']\n",
            " ZIP file found: /content/drive/My Drive/Colab Notebooks/archive.zip\n",
            "\n",
            " Extraction complete: 2684 files were extracted.\n",
            " Extracted data is available in: /content/vctk_samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installations\n"
      ],
      "metadata": {
        "id": "zB_QfTQcEA-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install coqui-tts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XXaOF5YzsZ3X",
        "outputId": "5608609f-445c-4549-ef4b-3e7a1ade9774"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting coqui-tts\n",
            "  Downloading coqui_tts-0.27.2-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting anyascii>=0.3.0 (from coqui-tts)\n",
            "  Downloading anyascii-0.3.3-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting coqpit-config<0.3.0,>=0.2.0 (from coqui-tts)\n",
            "  Downloading coqpit_config-0.2.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting coqui-tts-trainer<0.4.0,>=0.3.0 (from coqui-tts)\n",
            "  Downloading coqui_tts_trainer-0.3.1-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: cython>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from coqui-tts) (3.0.12)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from coqui-tts) (0.8.1)\n",
            "Collecting encodec>=0.1.1 (from coqui-tts)\n",
            "  Downloading encodec-0.1.1.tar.gz (3.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: fsspec>=2023.6.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2023.6.0->coqui-tts) (2025.3.0)\n",
            "Collecting gruut>=2.4.0 (from gruut[de,es,fr]>=2.4.0->coqui-tts)\n",
            "  Downloading gruut-2.4.0.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: inflect>=5.6.0 in /usr/local/lib/python3.12/dist-packages (from coqui-tts) (7.5.0)\n",
            "Requirement already satisfied: librosa>=0.11.0 in /usr/local/lib/python3.12/dist-packages (from coqui-tts) (0.11.0)\n",
            "Requirement already satisfied: matplotlib>=3.8.4 in /usr/local/lib/python3.12/dist-packages (from coqui-tts) (3.10.0)\n",
            "Collecting monotonic-alignment-search>=0.1.0 (from coqui-tts)\n",
            "  Downloading monotonic_alignment_search-0.2.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting num2words>=0.5.14 (from coqui-tts)\n",
            "  Downloading num2words-0.5.14-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numba>=0.58.0 in /usr/local/lib/python3.12/dist-packages (from coqui-tts) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from coqui-tts) (2.0.2)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.12/dist-packages (from coqui-tts) (25.0)\n",
            "Collecting pysbd>=0.3.4 (from coqui-tts)\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: pyyaml>=6.0 in /usr/local/lib/python3.12/dist-packages (from coqui-tts) (6.0.3)\n",
            "Requirement already satisfied: scipy>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from coqui-tts) (1.16.2)\n",
            "Requirement already satisfied: soundfile>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from coqui-tts) (0.13.1)\n",
            "Requirement already satisfied: torch<2.9,>=2.1 in /usr/local/lib/python3.12/dist-packages (from coqui-tts) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchaudio<2.9,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from coqui-tts) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.12/dist-packages (from coqui-tts) (4.67.1)\n",
            "Collecting transformers<4.56,>=4.52.1 (from coqui-tts)\n",
            "  Downloading transformers-4.55.4-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.10 in /usr/local/lib/python3.12/dist-packages (from coqui-tts) (4.15.0)\n",
            "Requirement already satisfied: psutil>=5 in /usr/local/lib/python3.12/dist-packages (from coqui-tts-trainer<0.4.0,>=0.3.0->coqui-tts) (5.9.5)\n",
            "Requirement already satisfied: tensorboard>=2.17.0 in /usr/local/lib/python3.12/dist-packages (from coqui-tts-trainer<0.4.0,>=0.3.0->coqui-tts) (2.19.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2023.6.0->coqui-tts) (3.13.0)\n",
            "Requirement already satisfied: Babel<3.0.0,>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from gruut>=2.4.0->gruut[de,es,fr]>=2.4.0->coqui-tts) (2.17.0)\n",
            "Collecting dateparser~=1.1.1 (from gruut>=2.4.0->gruut[de,es,fr]>=2.4.0->coqui-tts)\n",
            "  Downloading dateparser-1.1.8-py2.py3-none-any.whl.metadata (27 kB)\n",
            "Collecting gruut-ipa<1.0,>=0.12.0 (from gruut>=2.4.0->gruut[de,es,fr]>=2.4.0->coqui-tts)\n",
            "  Downloading gruut-ipa-0.13.0.tar.gz (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gruut_lang_en~=2.0.0 (from gruut>=2.4.0->gruut[de,es,fr]>=2.4.0->coqui-tts)\n",
            "  Downloading gruut_lang_en-2.0.1.tar.gz (15.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m123.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jsonlines~=1.2.0 (from gruut>=2.4.0->gruut[de,es,fr]>=2.4.0->coqui-tts)\n",
            "  Downloading jsonlines-1.2.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: networkx>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from gruut>=2.4.0->gruut[de,es,fr]>=2.4.0->coqui-tts) (3.5)\n",
            "Collecting python-crfsuite~=0.9.7 (from gruut>=2.4.0->gruut[de,es,fr]>=2.4.0->coqui-tts)\n",
            "  Downloading python_crfsuite-0.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Collecting gruut_lang_de~=2.0.0 (from gruut[de,es,fr]>=2.4.0->coqui-tts)\n",
            "  Downloading gruut_lang_de-2.0.1.tar.gz (18.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gruut_lang_es~=2.0.0 (from gruut[de,es,fr]>=2.4.0->coqui-tts)\n",
            "  Downloading gruut_lang_es-2.0.1.tar.gz (31.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gruut_lang_fr~=2.0.0 (from gruut[de,es,fr]>=2.4.0->coqui-tts)\n",
            "  Downloading gruut_lang_fr-2.0.2.tar.gz (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m135.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more_itertools>=8.5.0 in /usr/local/lib/python3.12/dist-packages (from inflect>=5.6.0->coqui-tts) (10.8.0)\n",
            "Requirement already satisfied: typeguard>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from inflect>=5.6.0->coqui-tts) (4.4.4)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.11.0->coqui-tts) (3.0.1)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.11.0->coqui-tts) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.11.0->coqui-tts) (1.5.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.11.0->coqui-tts) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.11.0->coqui-tts) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.11.0->coqui-tts) (1.0.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.11.0->coqui-tts) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.11.0->coqui-tts) (1.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.4->coqui-tts) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.4->coqui-tts) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.4->coqui-tts) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.4->coqui-tts) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.4->coqui-tts) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.4->coqui-tts) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.4->coqui-tts) (2.9.0.post0)\n",
            "Collecting docopt>=0.6.2 (from num2words>=0.5.14->coqui-tts)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.58.0->coqui-tts) (0.43.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.0->coqui-tts) (2.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<2.9,>=2.1->coqui-tts) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<2.9,>=2.1->coqui-tts) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<2.9,>=2.1->coqui-tts) (1.13.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<2.9,>=2.1->coqui-tts) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<2.9,>=2.1->coqui-tts) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<2.9,>=2.1->coqui-tts) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<2.9,>=2.1->coqui-tts) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<2.9,>=2.1->coqui-tts) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<2.9,>=2.1->coqui-tts) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<2.9,>=2.1->coqui-tts) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<2.9,>=2.1->coqui-tts) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<2.9,>=2.1->coqui-tts) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<2.9,>=2.1->coqui-tts) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<2.9,>=2.1->coqui-tts) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<2.9,>=2.1->coqui-tts) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<2.9,>=2.1->coqui-tts) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<2.9,>=2.1->coqui-tts) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<2.9,>=2.1->coqui-tts) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<2.9,>=2.1->coqui-tts) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers<4.56,>=4.52.1->coqui-tts) (0.35.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<4.56,>=4.52.1->coqui-tts) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers<4.56,>=4.52.1->coqui-tts) (2.32.4)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers<4.56,>=4.52.1->coqui-tts)\n",
            "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<4.56,>=4.52.1->coqui-tts) (0.6.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2023.6.0->coqui-tts) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2023.6.0->coqui-tts) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2023.6.0->coqui-tts) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2023.6.0->coqui-tts) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2023.6.0->coqui-tts) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2023.6.0->coqui-tts) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2023.6.0->coqui-tts) (1.22.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.0->coqui-tts) (2.23)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.12/dist-packages (from dateparser~=1.1.1->gruut>=2.4.0->gruut[de,es,fr]>=2.4.0->coqui-tts) (2025.2)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.12/dist-packages (from dateparser~=1.1.1->gruut>=2.4.0->gruut[de,es,fr]>=2.4.0->coqui-tts) (5.3.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers<4.56,>=4.52.1->coqui-tts) (1.1.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from jsonlines~=1.2.0->gruut>=2.4.0->gruut[de,es,fr]>=2.4.0->coqui-tts) (1.17.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa>=0.11.0->coqui-tts) (4.5.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers<4.56,>=4.52.1->coqui-tts) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers<4.56,>=4.52.1->coqui-tts) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers<4.56,>=4.52.1->coqui-tts) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers<4.56,>=4.52.1->coqui-tts) (2025.10.5)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa>=0.11.0->coqui-tts) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<2.9,>=2.1->coqui-tts) (1.3.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.17.0->coqui-tts-trainer<0.4.0,>=0.3.0->coqui-tts) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.17.0->coqui-tts-trainer<0.4.0,>=0.3.0->coqui-tts) (1.75.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.17.0->coqui-tts-trainer<0.4.0,>=0.3.0->coqui-tts) (3.9)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.17.0->coqui-tts-trainer<0.4.0,>=0.3.0->coqui-tts) (5.29.5)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.17.0->coqui-tts-trainer<0.4.0,>=0.3.0->coqui-tts) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.17.0->coqui-tts-trainer<0.4.0,>=0.3.0->coqui-tts) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<2.9,>=2.1->coqui-tts) (3.0.3)\n",
            "Downloading coqui_tts-0.27.2-py3-none-any.whl (858 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m858.4/858.4 kB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anyascii-0.3.3-py3-none-any.whl (345 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coqpit_config-0.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading coqui_tts_trainer-0.3.1-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic_alignment_search-0.2.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (648 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m648.4/648.4 kB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading num2words-0.5.14-py3-none-any.whl (163 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.55.4-py3-none-any.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m146.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dateparser-1.1.8-py2.py3-none-any.whl (293 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.8/293.8 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonlines-1.2.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading python_crfsuite-0.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m120.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: encodec, gruut, docopt, gruut-ipa, gruut_lang_de, gruut_lang_en, gruut_lang_es, gruut_lang_fr\n",
            "  Building wheel for encodec (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for encodec: filename=encodec-0.1.1-py3-none-any.whl size=45759 sha256=c663c3de365564b6dcbb2efd29ba72b52f20ded7b4bcb230288aa143971c8225\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/eb/9f/e13610cc46ab39d3199fbabebd1c3e142d44b679526e0f228a\n",
            "  Building wheel for gruut (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut: filename=gruut-2.4.0-py3-none-any.whl size=86758 sha256=a238f7f3b356cefb14e4bd7315c30a19d580c7c698e4dbcf885bc9ea09f08338\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/93/9c/fbd0d8778ac586a48a20e73d2a64d882984d9501fd5e8daf24\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=f33814ec8a4a7dcfc4188eea2211f772a714d8d1b4c381a5fab43301b305744e\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/bf/a1/4cee4f7678c68c5875ca89eaccf460593539805c3906722228\n",
            "  Building wheel for gruut-ipa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut-ipa: filename=gruut_ipa-0.13.0-py3-none-any.whl size=104873 sha256=ec678f50e1cebabe41d0352f0404da022af043a339cc13561762cce68842a0ac\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/9d/d2/d6f6eb77784f063fcd497427fd93324cebf974247984bba85b\n",
            "  Building wheel for gruut_lang_de (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_de: filename=gruut_lang_de-2.0.1-py3-none-any.whl size=18498314 sha256=5f6cf76ad8e490495b41bce335d050160775e96dea5593180232ec16370a7c86\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/75/26/e627d52dac0253ad7d11e5b9f74d51d82e040d07432f53ad9b\n",
            "  Building wheel for gruut_lang_en (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_en: filename=gruut_lang_en-2.0.1-py3-none-any.whl size=15326858 sha256=2970afd8736dfde445f13ce1ec3dccb7c9e696ab72e91f26d5f98f6a1f09a45b\n",
            "  Stored in directory: /root/.cache/pip/wheels/b0/ca/a4/d1a6f20e47b857313689ca1f31684102ba67cecda2acae368d\n",
            "  Building wheel for gruut_lang_es (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_es: filename=gruut_lang_es-2.0.1-py3-none-any.whl size=32173927 sha256=f8efdb8c4c31672ee1d59703e36b97cc205a9292157564f3acbfff08e0d19ecf\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/19/79/0a65f77c4921ae0daa8d01e5b11502a909b55bd22fa188962d\n",
            "  Building wheel for gruut_lang_fr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_fr: filename=gruut_lang_fr-2.0.2-py3-none-any.whl size=10968767 sha256=1a8f5a0ef641b78da563bcb17180bba40e23e3cc4130b0caa985a4d4d91d60db\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/91/46/0ab326f9e46bc2cc2fe2f35b0e0e6f3b8284d78efd25192d96\n",
            "Successfully built encodec gruut docopt gruut-ipa gruut_lang_de gruut_lang_en gruut_lang_es gruut_lang_fr\n",
            "Installing collected packages: gruut_lang_fr, gruut_lang_es, gruut_lang_en, gruut_lang_de, docopt, python-crfsuite, pysbd, num2words, monotonic-alignment-search, jsonlines, gruut-ipa, coqpit-config, anyascii, dateparser, tokenizers, gruut, transformers, coqui-tts-trainer, encodec, coqui-tts\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.22.1\n",
            "    Uninstalling tokenizers-0.22.1:\n",
            "      Successfully uninstalled tokenizers-0.22.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.0\n",
            "    Uninstalling transformers-4.57.0:\n",
            "      Successfully uninstalled transformers-4.57.0\n",
            "Successfully installed anyascii-0.3.3 coqpit-config-0.2.1 coqui-tts-0.27.2 coqui-tts-trainer-0.3.1 dateparser-1.1.8 docopt-0.6.2 encodec-0.1.1 gruut-2.4.0 gruut-ipa-0.13.0 gruut_lang_de-2.0.1 gruut_lang_en-2.0.1 gruut_lang_es-2.0.1 gruut_lang_fr-2.0.2 jsonlines-1.2.0 monotonic-alignment-search-0.2.1 num2words-0.5.14 pysbd-0.3.4 python-crfsuite-0.9.11 tokenizers-0.21.4 transformers-4.55.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y espeak-ng"
      ],
      "metadata": {
        "id": "nymZzYIfrL7N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "81ecaf9f-4482-452f-af14-54e30a58456c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  espeak-ng-data libespeak-ng1 libpcaudio0 libsonic0\n",
            "The following NEW packages will be installed:\n",
            "  espeak-ng espeak-ng-data libespeak-ng1 libpcaudio0 libsonic0\n",
            "0 upgraded, 5 newly installed, 0 to remove and 38 not upgraded.\n",
            "Need to get 4,526 kB of archives.\n",
            "After this operation, 11.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libpcaudio0 amd64 1.1-6build2 [8,956 B]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsonic0 amd64 0.2.0-11build1 [10.3 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 espeak-ng-data amd64 1.50+dfsg-10ubuntu0.1 [3,956 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libespeak-ng1 amd64 1.50+dfsg-10ubuntu0.1 [207 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 espeak-ng amd64 1.50+dfsg-10ubuntu0.1 [343 kB]\n",
            "Fetched 4,526 kB in 2s (2,742 kB/s)\n",
            "Selecting previously unselected package libpcaudio0:amd64.\n",
            "(Reading database ... 126675 files and directories currently installed.)\n",
            "Preparing to unpack .../libpcaudio0_1.1-6build2_amd64.deb ...\n",
            "Unpacking libpcaudio0:amd64 (1.1-6build2) ...\n",
            "Selecting previously unselected package libsonic0:amd64.\n",
            "Preparing to unpack .../libsonic0_0.2.0-11build1_amd64.deb ...\n",
            "Unpacking libsonic0:amd64 (0.2.0-11build1) ...\n",
            "Selecting previously unselected package espeak-ng-data:amd64.\n",
            "Preparing to unpack .../espeak-ng-data_1.50+dfsg-10ubuntu0.1_amd64.deb ...\n",
            "Unpacking espeak-ng-data:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Selecting previously unselected package libespeak-ng1:amd64.\n",
            "Preparing to unpack .../libespeak-ng1_1.50+dfsg-10ubuntu0.1_amd64.deb ...\n",
            "Unpacking libespeak-ng1:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Selecting previously unselected package espeak-ng.\n",
            "Preparing to unpack .../espeak-ng_1.50+dfsg-10ubuntu0.1_amd64.deb ...\n",
            "Unpacking espeak-ng (1.50+dfsg-10ubuntu0.1) ...\n",
            "Setting up libpcaudio0:amd64 (1.1-6build2) ...\n",
            "Setting up libsonic0:amd64 (0.2.0-11build1) ...\n",
            "Setting up espeak-ng-data:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Setting up libespeak-ng1:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Setting up espeak-ng (1.50+dfsg-10ubuntu0.1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deleting fake_wav48 if it exists"
      ],
      "metadata": {
        "id": "a4EjZRVMERfm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# generate_fake_data"
      ],
      "metadata": {
        "id": "3x2fEEe0oHhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# from TTS.api import TTS\n",
        "# import shutil\n",
        "\n",
        "# fake_audio_folder = \"/content/vctk_samples/VCTK-Corpus/VCTK-Corpus/fake_wav48\"\n",
        "# # פונקציה למחיקת כל קבצי האודיו שנוצרו\n",
        "# def clear_fake_audio_folder(fake_audio_folder):\n",
        "#     if os.path.exists(fake_audio_folder):\n",
        "#         shutil.rmtree(fake_audio_folder)  # מוחק את כל התיקייה כולל הקבצים שבה\n",
        "#         os.makedirs(fake_audio_folder, exist_ok=True)  # יוצר מחדש את התיקייה הריקה\n",
        "#         print(f\" כל הקבצים בתיקייה '{fake_audio_folder}' נמחקו!\")\n",
        "\n",
        "# clear_fake_audio_folder(fake_audio_folder)\n",
        "# quit()"
      ],
      "metadata": {
        "id": "bfJgcbcSqmrh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb01a767-0a93-410b-de05-4115be55fe65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " כל הקבצים בתיקייה '/content/vctk_samples/VCTK-Corpus/VCTK-Corpus/fake_wav48' נמחקו!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating Fake Audio (No Need to run now)"
      ],
      "metadata": {
        "id": "YqWhklMLEWba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import threading\n",
        "import torch\n",
        "\n",
        "from TTS.api import TTS\n",
        "\n",
        "# =========================\n",
        "# CONFIG\n",
        "# =========================\n",
        "# 1) Input text files (one .txt per utterance)\n",
        "TEXT_ROOT = Path(\"/content/vctk_samples/VCTK-Corpus/VCTK-Corpus/txt\")\n",
        "\n",
        "# 2) Real audio folder (matching real WAVs you want to clone the voice from)\n",
        "#    The script will try to find a matching WAV by the text filename stem inside the corresponding subfolder.\n",
        "#    Example: If text is \".../p228/p228_065.txt\", it will try \"/.../real_audio_folder/p228/p228_065.wav\"\n",
        "REAL_AUDIO_ROOT = Path(\"/content/vctk_samples/VCTK-Corpus/VCTK-Corpus/wav48\")  # <-- CHANGE if needed\n",
        "\n",
        "# 3) Output folder for fake audio\n",
        "OUT_ROOT = Path(\"/content/vctk_samples/VCTK-Corpus/VCTK-Corpus/fake_wav48_xtts\")\n",
        "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 4) Speaker metadata (for bookkeeping only; XTTS uses speaker_wav for actual voice)\n",
        "SPEAKER_INFO = {\n",
        "    \"p225\": (\"F\", \"22\", \"Southern England\"),\n",
        "    \"p226\": (\"M\", \"22\", \"Surrey\"),\n",
        "    \"p227\": (\"M\", \"38\", \"Cumbria\"),\n",
        "    \"p228\": (\"F\", \"22\", \"Southern England\"),\n",
        "}\n",
        "\n",
        "# 5) Language to synthesize in (VCTK is English)\n",
        "LANGUAGE = \"en\"\n",
        "\n",
        "# 6) Concurrency settings:\n",
        "#    On GPU, keep MAX_WORKERS=1 (XTTS is heavy and not thread-safe on CUDA).\n",
        "#    On CPU, you can increase to 4 (or more if your machine can handle it).\n",
        "GPU_AVAILABLE = torch.cuda.is_available()\n",
        "MAX_WORKERS = 1 if GPU_AVAILABLE else 4\n",
        "\n",
        "# =========================\n",
        "# MODEL LOADING\n",
        "# =========================\n",
        "# Use XTTS v2 for voice cloning with a reference WAV.\n",
        "# (We also lazy-load a VCTK-VITS fallback if a real WAV is missing.)\n",
        "print(\"Loading XTTS v2 model...\")\n",
        "tts_xtts = TTS(model_name=\"tts_models/multilingual/multi-dataset/xtts_v2\", progress_bar=True)\n",
        "device = \"cuda\" if GPU_AVAILABLE else \"cpu\"\n",
        "tts_xtts.to(device)\n",
        "print(f\"XTTS is running on: {device}\")\n",
        "\n",
        "# Fallback multi-speaker model (only used when no real WAV is found)\n",
        "print(\"Loading VCTK-VITS fallback model...\")\n",
        "tts_vctk = TTS(model_name=\"tts_models/en/vctk/vits\", progress_bar=False)\n",
        "tts_vctk.to(device)\n",
        "\n",
        "# Mutex for model calls if you insist on >1 threads on CPU (XTTS is heavy; serialize calls by default on GPU).\n",
        "synth_lock = threading.Lock() if MAX_WORKERS > 1 else None\n",
        "\n",
        "# =========================\n",
        "# HELPERS\n",
        "# =========================\n",
        "def to_vctk_id(s: str) -> str:\n",
        "    \"\"\"Convert '228' -> 'p228' for VCTK-style speaker IDs.\"\"\"\n",
        "    s = s.strip()\n",
        "    return s if s.startswith(\"p\") else f\"p{s}\"\n",
        "\n",
        "def find_matching_real_wav(real_root: Path, subdir: str, txt_filename: str) -> Path | None:\n",
        "    \"\"\"\n",
        "    Try to find the real WAV that matches the text file.\n",
        "    Strategy:\n",
        "      1) exact same stem under REAL_AUDIO_ROOT/subdir:  <stem>.wav\n",
        "      2) any .wav in subdir that contains the stem (fallback)\n",
        "      3) final fallback: None\n",
        "    \"\"\"\n",
        "    stem = Path(txt_filename).stem  # e.g., 'p228_065' or '228_065'\n",
        "    # Common VCTK stems look like 'p228_065'. If it's numeric-only, normalize:\n",
        "    parts = stem.split(\"_\")\n",
        "    if parts and not parts[0].startswith(\"p\"):\n",
        "        parts[0] = \"p\" + parts[0]\n",
        "    norm_stem = \"_\".join(parts)\n",
        "\n",
        "    cand1 = real_root / subdir / f\"{norm_stem}.wav\"\n",
        "    if cand1.exists():\n",
        "        return cand1\n",
        "\n",
        "    # Try exactly the original stem (if it already had 'p')\n",
        "    cand2 = real_root / subdir / f\"{stem}.wav\"\n",
        "    if cand2.exists():\n",
        "        return cand2\n",
        "\n",
        "    # Fallback: search within subdir for anything containing norm_stem or the raw stem\n",
        "    subdir_path = real_root / subdir\n",
        "    if subdir_path.is_dir():\n",
        "        for fn in os.listdir(subdir_path):\n",
        "            if not fn.lower().endswith(\".wav\"):\n",
        "                continue\n",
        "            if norm_stem in fn or stem in fn:\n",
        "                return subdir_path / fn\n",
        "\n",
        "    return None\n",
        "\n",
        "def synth_xtts(text: str, speaker_wav: Path, out_path: Path, language: str = \"en\"):\n",
        "    \"\"\"\n",
        "    Synthesize with XTTS v2 using a reference speaker WAV. This is the key for high voice similarity.\n",
        "    \"\"\"\n",
        "    # Serialize heavy GPU calls if needed\n",
        "    if synth_lock:\n",
        "        with synth_lock:\n",
        "            tts_xtts.tts_to_file(text=text, file_path=str(out_path), speaker_wav=str(speaker_wav), language=language)\n",
        "    else:\n",
        "        tts_xtts.tts_to_file(text=text, file_path=str(out_path), speaker_wav=str(speaker_wav), language=language)\n",
        "\n",
        "def synth_vctk(text: str, speaker_id: str, out_path: Path):\n",
        "    \"\"\"\n",
        "    Fallback synthesis with VCTK-VITS multi-speaker model (uses 'p###' speakers).\n",
        "    \"\"\"\n",
        "    if synth_lock:\n",
        "        with synth_lock:\n",
        "            tts_vctk.tts_to_file(text=text, speaker=speaker_id, file_path=str(out_path))\n",
        "    else:\n",
        "        tts_vctk.tts_to_file(text=text, speaker=speaker_id, file_path=str(out_path))\n",
        "\n",
        "def process_one(text_path: Path, out_subdir: Path):\n",
        "    \"\"\"\n",
        "    Process a single text file:\n",
        "      - Read text\n",
        "      - Resolve speaker_id from filename (for metadata/fallback)\n",
        "      - Find matching real WAV\n",
        "      - Prefer XTTS cloning; fallback to VCTK-VITS speaker if real WAV missing\n",
        "    \"\"\"\n",
        "    text = text_path.read_text(encoding=\"utf-8\").strip()\n",
        "    if not text:\n",
        "        return f\"[SKIP] Empty text: {text_path.name}\"\n",
        "\n",
        "    # Resolve speaker id from the filename (e.g., 'p228' from 'p228_065.txt')\n",
        "    raw_id = text_path.stem.split(\"_\")[0]          # 'p228' or '228'\n",
        "    speaker_id = to_vctk_id(raw_id)                # 'p228'\n",
        "\n",
        "    # Metadata (for filename only)\n",
        "    gender, age, accent = SPEAKER_INFO.get(speaker_id, (\"F\", \"22\", \"Southern England\"))\n",
        "\n",
        "    # Try to find the matching real wav in REAL_AUDIO_ROOT/<subdir>/\n",
        "    subdir = text_path.parent.name\n",
        "    real_wav = find_matching_real_wav(REAL_AUDIO_ROOT, subdir, text_path.name)\n",
        "\n",
        "    # Build output path (include metadata in filename)\n",
        "    out_name = f\"{text_path.stem}__{speaker_id}__{gender}_{age}_{accent}.wav\"\n",
        "    out_path = out_subdir / out_name\n",
        "\n",
        "    # Prefer XTTS cloning if real wav exists; otherwise fallback to VCTK-VITS speaker\n",
        "    if real_wav and real_wav.exists():\n",
        "        msg = f\"[XTTS] {text_path.name} -> clone from {real_wav.name} -> {out_name}\"\n",
        "        synth_xtts(text=text, speaker_wav=real_wav, out_path=out_path, language=LANGUAGE)\n",
        "        return msg\n",
        "    else:\n",
        "        msg = f\"[FALLBACK VCTK] {text_path.name} -> speaker={speaker_id} -> {out_name}\"\n",
        "        synth_vctk(text=text, speaker_id=speaker_id, out_path=out_path)\n",
        "        return msg\n",
        "\n",
        "# =========================\n",
        "# BUILD JOBS\n",
        "# =========================\n",
        "jobs = []\n",
        "for subdir in os.listdir(TEXT_ROOT):\n",
        "    subdir_path = TEXT_ROOT / subdir\n",
        "    if not subdir_path.is_dir():\n",
        "        continue\n",
        "\n",
        "    out_subdir = OUT_ROOT / subdir\n",
        "    out_subdir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for fn in os.listdir(subdir_path):\n",
        "        if not fn.lower().endswith(\".txt\"):\n",
        "            continue\n",
        "        jobs.append((subdir_path / fn, out_subdir))\n",
        "\n",
        "print(f\"Found {len(jobs)} text files.\")\n",
        "\n",
        "# =========================\n",
        "# RUN\n",
        "# =========================\n",
        "if not jobs:\n",
        "    print(\"No jobs found. Check TEXT_ROOT.\")\n",
        "else:\n",
        "    print(f\"Starting synthesis with MAX_WORKERS={MAX_WORKERS} (device={device})\")\n",
        "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
        "        futs = [ex.submit(process_one, text_path, out_dir) for (text_path, out_dir) in jobs]\n",
        "        for fut in as_completed(futs):\n",
        "            try:\n",
        "                info = fut.result()\n",
        "                print(info)\n",
        "            except Exception as e:\n",
        "                print(\"[ERROR]\", repr(e))\n",
        "\n",
        "print(\"Done. Fake audio saved under:\", OUT_ROOT)\n"
      ],
      "metadata": {
        "id": "1ZDmhBIjEHQW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "outputId": "94423c1f-3567-49fc-cad9-28f0deddec87",
        "collapsed": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jieba/__init__.py:44: SyntaxWarning: invalid escape sequence '\\.'\n",
            "  re_han_default = re.compile(\"([\\u4E00-\\u9FD5a-zA-Z0-9+#&\\._%\\-]+)\", re.U)\n",
            "/usr/local/lib/python3.12/dist-packages/jieba/__init__.py:46: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  re_skip_default = re.compile(\"(\\r\\n|\\s)\", re.U)\n",
            "/usr/local/lib/python3.12/dist-packages/jieba/finalseg/__init__.py:78: SyntaxWarning: invalid escape sequence '\\.'\n",
            "  re_skip = re.compile(\"([a-zA-Z0-9]+(?:\\.\\d+)?%?)\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading XTTS v2 model...\n",
            " > You must confirm the following:\n",
            " | > \"I have purchased a commercial license from Coqui: licensing@coqui.ai\"\n",
            " | > \"Otherwise, I agree to the terms of the non-commercial CPML: https://coqui.ai/cpml\" - [y/n]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-369431608.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# (We also lazy-load a VCTK-VITS fallback if a real WAV is missing.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading XTTS v2 model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mtts_xtts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTTS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tts_models/multilingual/multi-dataset/xtts_v2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mGPU_AVAILABLE\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mtts_xtts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/TTS/api.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_name, model_path, config_path, vocoder_name, vocoder_path, vocoder_config_path, encoder_path, encoder_config_path, speakers_file_path, language_ids_file_path, progress_bar, gpu)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"tts_models\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_tts_model_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocoder_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m\"voice_conversion_models\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_vc_model_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocoder_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/TTS/api.py\u001b[0m in \u001b[0;36mload_tts_model_by_name\u001b[0;34m(self, model_name, vocoder_name, gpu)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         model_path, config_path, vocoder_path, vocoder_config_path, model_dir = self.download_model_by_name(\n\u001b[0m\u001b[1;32m    233\u001b[0m             \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocoder_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/TTS/api.py\u001b[0m in \u001b[0;36mdownload_model_by_name\u001b[0;34m(self, model_name, vocoder_name)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocoder_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     ) -> tuple[Path | None, Path | None, Path | None, Path | None, Path | None]:\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_item\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         if (\n\u001b[1;32m    168\u001b[0m             \u001b[0;34m\"fairseq\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/TTS/utils/manage.py\u001b[0m in \u001b[0;36mdownload_model\u001b[0;34m(self, model_name)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is already downloaded.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dir_and_download_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;31m# find downloaded files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/TTS/utils/manage.py\u001b[0m in \u001b[0;36mcreate_dir_and_download_model\u001b[0;34m(self, model_name, model_item, output_path)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;31m# handle TOS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtos_agreed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mask_tos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m                 \u001b[0moutput_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" [!] You must agree to the terms of service to use this model.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/TTS/utils/manage.py\u001b[0m in \u001b[0;36mask_tos\u001b[0;34m(model_full_path)\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' | > \"I have purchased a commercial license from Coqui: licensing@coqui.ai\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' | > \"Otherwise, I agree to the terms of the non-commercial CPML: https://coqui.ai/cpml\" - [y/n]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" | | > \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"y\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtos_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the fake audio"
      ],
      "metadata": {
        "id": "C1gzk4KwGIyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import shutil\n",
        "# from google.colab import files\n",
        "\n",
        "# folder_path = \"/content/vctk_samples/VCTK-Corpus/VCTK-Corpus/fake_wav48_xtts\"\n",
        "# zip_path = \"/content/fake_audio.zip\"\n",
        "# shutil.make_archive(base_name=zip_path.replace(\".zip\", \"\"), format='zip', root_dir=folder_path)\n",
        "\n",
        "# files.download(zip_path)\n"
      ],
      "metadata": {
        "id": "ATqG7ZXl74D8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "8730374a-8303-435c-cd10-ae9ceef57af2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8c6401dd-0cc1-4b42-8cfa-7a13e6288529\", \"fake_audio.zip\", 133268336)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting the fake audio"
      ],
      "metadata": {
        "id": "dcjKFlnIKvFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "# Define paths\n",
        "zip_file = \"/content/drive/My Drive/Colab Notebooks/fake_audio.zip\"  # Path to your ZIP file\n",
        "destination_folder = \"/content/vctk_samples/VCTK-Corpus/VCTK-Corpus/fake_audio\"  # Where to extract selected data\n",
        "\n",
        "# Example: define the speakers you want to extract\n",
        "wanted_speakers = [\"p225\", \"p226\",\"p227\",\"p228\"]  # change this list as needed\n",
        "\n",
        "# Check if the ZIP file exists\n",
        "if os.path.isfile(zip_file):\n",
        "    print(\" ZIP file found:\", zip_file)\n",
        "else:\n",
        "    raise FileNotFoundError(f\" ZIP file not found: {zip_file}\")\n",
        "\n",
        "# Create destination folder if it doesn't exist\n",
        "os.makedirs(destination_folder, exist_ok=True)\n",
        "\n",
        "# Selectively extract only desired speaker folders from the ZIP\n",
        "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "    extracted_files = 0\n",
        "    for file in zip_ref.namelist():\n",
        "        if any(f\"{spk}/\" in file for spk in wanted_speakers):\n",
        "            target_path = os.path.join(destination_folder, file)\n",
        "\n",
        "            # If this entry is a directory → skip it\n",
        "            if file.endswith('/'):\n",
        "                os.makedirs(target_path, exist_ok=True)\n",
        "                continue\n",
        "\n",
        "            # Ensure directory structure is preserved\n",
        "            os.makedirs(os.path.dirname(target_path), exist_ok=True)\n",
        "\n",
        "            # Copy file content\n",
        "            with zip_ref.open(file) as source, open(target_path, 'wb') as target:\n",
        "                shutil.copyfileobj(source, target)\n",
        "            extracted_files += 1\n",
        "\n",
        "print(f\"Extracted {extracted_files} files for speakers: {wanted_speakers}\")\n"
      ],
      "metadata": {
        "id": "Qzof7wJ8kHse",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bef3d30-a144-45c7-f91f-abd4d8af9636"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ZIP file found: /content/drive/My Drive/Colab Notebooks/fake_audio.zip\n",
            "Extracted 1342 files for speakers: ['p225', 'p226', 'p227', 'p228']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing libraries"
      ],
      "metadata": {
        "id": "HloMV1V7yZ7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L || echo \"No GPU\""
      ],
      "metadata": {
        "id": "FU7_7Wq5uCOM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cf5ec43-e298-4fd0-9f9c-12aedaa285dc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-454b888f-8ac1-bb18-9f83-eac3563e53c2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "\n",
        "# --- Clone CLAD fresh (idempotent: remove existing dir if present) ---\n",
        "# remove previous clone to ensure a clean edit of requirements\n",
        "rm -rf CLAD\n",
        "git clone https://github.com/CLAD23/CLAD.git\n",
        "\n",
        "# --- Normalize requirements for Python 3.12 (single source of truth) ---\n",
        "cp CLAD/requirements.txt CLAD/requirements.bak\n",
        "\n",
        "# 1) Remove torch lines (Torch is installed manually for the correct CUDA wheel)\n",
        "sed -i '/^torch==/d; /^torchvision==/d; /^torchaudio==/d' CLAD/requirements.txt\n",
        "\n",
        "# 2) Core pins for Py3.12 + Numba 0.60.0 (compatible with llvmlite 0.43.0 and NumPy 1.26.4)\n",
        "#    These ensure no NumPy 2.x is pulled by accident.\n",
        "if grep -q '^numpy' CLAD/requirements.txt; then\n",
        "  sed -i 's/^numpy==.*/numpy==1.26.4/' CLAD/requirements.txt\n",
        "else\n",
        "  sed -i '1i numpy==1.26.4' CLAD/requirements.txt\n",
        "fi\n",
        "\n",
        "if grep -q '^numba' CLAD/requirements.txt; then\n",
        "  sed -i 's/^numba==.*/numba==0.60.0/' CLAD/requirements.txt\n",
        "else\n",
        "  sed -i '1i numba==0.60.0' CLAD/requirements.txt\n",
        "fi\n",
        "\n",
        "if grep -q '^llvmlite' CLAD/requirements.txt; then\n",
        "  sed -i 's/^llvmlite==.*/llvmlite==0.43.0/' CLAD/requirements.txt\n",
        "else\n",
        "  sed -i '1i llvmlite==0.43.0' CLAD/requirements.txt\n",
        "fi\n",
        "\n",
        "# 3) Stable Matplotlib on Py3.12\n",
        "if grep -q '^matplotlib' CLAD/requirements.txt; then\n",
        "  sed -i 's/^matplotlib==.*/matplotlib==3.8.4/' CLAD/requirements.txt\n",
        "else\n",
        "  sed -i '1i matplotlib==3.8.4' CLAD/requirements.txt\n",
        "fi\n",
        "\n",
        "# 4) Librosa must satisfy coqui-tts (>=0.11.0); keep it permissive to avoid conflicts\n",
        "if grep -q '^librosa' CLAD/requirements.txt; then\n",
        "  sed -i 's/^librosa.*/librosa>=0.11.0/' CLAD/requirements.txt\n",
        "else\n",
        "  sed -i '1i librosa>=0.11.0' CLAD/requirements.txt\n",
        "fi\n",
        "\n",
        "# 5) Pin OpenCV to builds compatible with NumPy 1.26.x (avoid NumPy 2.x constraint)\n",
        "#    Only modify if opencv lines exist (do not add if the project doesn't use it).\n",
        "grep -q '^opencv-python' CLAD/requirements.txt && sed -i 's/^opencv-python==.*/opencv-python==4.9.0.80/' CLAD/requirements.txt || true\n",
        "grep -q '^opencv-contrib-python' CLAD/requirements.txt && sed -i 's/^opencv-contrib-python==.*/opencv-contrib-python==4.9.0.80/' CLAD/requirements.txt || true\n",
        "\n",
        "# 6) Pin spaCy/Thinc to versions that work with NumPy 1.x (only if present)\n",
        "grep -q '^thinc' CLAD/requirements.txt && sed -i 's/^thinc==.*/thinc==8.2.2/' CLAD/requirements.txt || true\n",
        "grep -q '^spacy' CLAD/requirements.txt && sed -i 's/^spacy==.*/spacy==3.7.4/' CLAD/requirements.txt || true\n",
        "\n",
        "echo \"===== Updated CLAD/requirements.txt =====\"\n",
        "sed -n '1,250p' CLAD/requirements.txt\n",
        "\n",
        "# --- Upgrade pip to avoid resolver quirks ---\n",
        "python -m pip install -U pip\n",
        "\n",
        "# --- Install PyTorch 2.3.1 CUDA 12.1 (use CPU wheels by removing the index line if no GPU) ---\n",
        "python -m pip install --index-url https://download.pytorch.org/whl/cu121 \\\n",
        "  torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1\n",
        "\n",
        "# --- Install remaining CLAD dependencies from the single normalized requirements file ---\n",
        "python -m pip install -r CLAD/requirements.txt\n",
        "\n",
        "# --- System library for soundfile/librosa WAV I/O (safe to install always) ---\n",
        "apt-get update -y\n",
        "apt-get install -y libsndfile1\n",
        "\n",
        "echo \"===== DONE: Environment pinned for Python 3.12 =====\"\n",
        "python -V\n",
        "python - <<'PY'\n",
        "import sys, numpy, numba, llvmlite, matplotlib\n",
        "import importlib\n",
        "print(\"Python:\", sys.version.split()[0])\n",
        "print(\"NumPy:\", numpy.__version__)\n",
        "print(\"Numba:\", numba.__version__)\n",
        "print(\"llvmlite:\", llvmlite.__version__)\n",
        "print(\"Matplotlib:\", matplotlib.__version__)\n",
        "for m in (\"torch\",\"torchvision\",\"torchaudio\",\"librosa\"):\n",
        "    try:\n",
        "        mod = importlib.import_module(m)\n",
        "        print(f\"{m}:\", getattr(mod,\"__version__\", \"unknown\"))\n",
        "    except Exception as e:\n",
        "        print(f\"{m}: NOT INSTALLED ({e})\")\n",
        "PY\n"
      ],
      "metadata": {
        "id": "YaBoZc46f0p_",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a425a03d-cd7c-4d0a-eb80-4692825fe039"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Updated CLAD/requirements.txt =====\n",
            "llvmlite==0.43.0\n",
            "numba==0.60.0\n",
            "librosa>=0.11.0\n",
            "matplotlib==3.8.4\n",
            "numpy==1.26.4\n",
            "primePy==1.3\n",
            "torchcontrib\n",
            "pytorch_model_summary\n",
            "torchinfoRequirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Downloading pip-25.2-py3-none-any.whl (1.8 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/1.8 MB 34.0 MB/s eta 0:00:00\n",
            "Installing collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.2\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting torch==2.3.1\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.3.1%2Bcu121-cp312-cp312-linux_x86_64.whl (780.9 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 780.9/780.9 MB 24.3 MB/s  0:00:18\n",
            "Collecting torchvision==0.18.1\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.18.1%2Bcu121-cp312-cp312-linux_x86_64.whl (7.0 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.0/7.0 MB 121.3 MB/s  0:00:00\n",
            "Collecting torchaudio==2.3.1\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.3.1%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.4/3.4 MB 110.3 MB/s  0:00:00\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 23.7/23.7 MB 174.5 MB/s  0:00:00\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 823.6/823.6 kB 44.4 MB/s  0:00:00\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.1/14.1 MB 143.1 MB/s  0:00:00\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 731.7/731.7 MB ?  0:00:46\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 410.6/410.6 MB 26.4 MB/s  0:00:10\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.6/121.6 MB 73.2 MB/s  0:00:01\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.5/56.5 MB 65.2 MB/s  0:00:00\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.2/124.2 MB 71.6 MB/s  0:00:01\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 196.0/196.0 MB 10.3 MB/s  0:00:19\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 176.2/176.2 MB 14.8 MB/s  0:00:11\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.18.1) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.18.1) (11.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.1) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.3.1) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.3.1) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.8.0+cu126\n",
            "    Uninstalling torch-2.8.0+cu126:\n",
            "      Successfully uninstalled torch-2.8.0+cu126\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.23.0+cu126\n",
            "    Uninstalling torchvision-0.23.0+cu126:\n",
            "      Successfully uninstalled torchvision-0.23.0+cu126\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.8.0+cu126\n",
            "    Uninstalling torchaudio-2.8.0+cu126:\n",
            "      Successfully uninstalled torchaudio-2.8.0+cu126\n",
            "\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.3.1+cu121 torchaudio-2.3.1+cu121 torchvision-0.18.1+cu121\n",
            "Requirement already satisfied: llvmlite==0.43.0 in /usr/local/lib/python3.12/dist-packages (from -r CLAD/requirements.txt (line 1)) (0.43.0)\n",
            "Requirement already satisfied: numba==0.60.0 in /usr/local/lib/python3.12/dist-packages (from -r CLAD/requirements.txt (line 2)) (0.60.0)\n",
            "Requirement already satisfied: librosa>=0.11.0 in /usr/local/lib/python3.12/dist-packages (from -r CLAD/requirements.txt (line 3)) (0.11.0)\n",
            "Collecting matplotlib==3.8.4 (from -r CLAD/requirements.txt (line 4))\n",
            "  Downloading matplotlib-3.8.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Collecting numpy==1.26.4 (from -r CLAD/requirements.txt (line 5))\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting primePy==1.3 (from -r CLAD/requirements.txt (line 6))\n",
            "  Downloading primePy-1.3-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting torchcontrib (from -r CLAD/requirements.txt (line 7))\n",
            "  Downloading torchcontrib-0.0.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting pytorch_model_summary (from -r CLAD/requirements.txt (line 8))\n",
            "  Downloading pytorch_model_summary-0.1.2-py3-none-any.whl.metadata (35 kB)\n",
            "Collecting torchinfo (from -r CLAD/requirements.txt (line 9))\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.4->-r CLAD/requirements.txt (line 4)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.4->-r CLAD/requirements.txt (line 4)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.4->-r CLAD/requirements.txt (line 4)) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.4->-r CLAD/requirements.txt (line 4)) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.4->-r CLAD/requirements.txt (line 4)) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.4->-r CLAD/requirements.txt (line 4)) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.4->-r CLAD/requirements.txt (line 4)) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.4->-r CLAD/requirements.txt (line 4)) (2.9.0.post0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.11.0->-r CLAD/requirements.txt (line 3)) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.11.0->-r CLAD/requirements.txt (line 3)) (1.16.2)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.11.0->-r CLAD/requirements.txt (line 3)) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.11.0->-r CLAD/requirements.txt (line 3)) (1.5.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.11.0->-r CLAD/requirements.txt (line 3)) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.11.0->-r CLAD/requirements.txt (line 3)) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.11.0->-r CLAD/requirements.txt (line 3)) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.11.0->-r CLAD/requirements.txt (line 3)) (1.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.11.0->-r CLAD/requirements.txt (line 3)) (4.15.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.11.0->-r CLAD/requirements.txt (line 3)) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.11.0->-r CLAD/requirements.txt (line 3)) (1.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from pytorch_model_summary->-r CLAD/requirements.txt (line 8)) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from pytorch_model_summary->-r CLAD/requirements.txt (line 8)) (2.3.1+cu121)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa>=0.11.0->-r CLAD/requirements.txt (line 3)) (4.5.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa>=0.11.0->-r CLAD/requirements.txt (line 3)) (2.32.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib==3.8.4->-r CLAD/requirements.txt (line 4)) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.11.0->-r CLAD/requirements.txt (line 3)) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.11.0->-r CLAD/requirements.txt (line 3)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.11.0->-r CLAD/requirements.txt (line 3)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.11.0->-r CLAD/requirements.txt (line 3)) (2025.10.5)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa>=0.11.0->-r CLAD/requirements.txt (line 3)) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->librosa>=0.11.0->-r CLAD/requirements.txt (line 3)) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa>=0.11.0->-r CLAD/requirements.txt (line 3)) (2.23)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->pytorch_model_summary->-r CLAD/requirements.txt (line 8)) (3.20.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch->pytorch_model_summary->-r CLAD/requirements.txt (line 8)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->pytorch_model_summary->-r CLAD/requirements.txt (line 8)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch_model_summary->-r CLAD/requirements.txt (line 8)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->pytorch_model_summary->-r CLAD/requirements.txt (line 8)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch_model_summary->-r CLAD/requirements.txt (line 8)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch_model_summary->-r CLAD/requirements.txt (line 8)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch_model_summary->-r CLAD/requirements.txt (line 8)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch_model_summary->-r CLAD/requirements.txt (line 8)) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch_model_summary->-r CLAD/requirements.txt (line 8)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch_model_summary->-r CLAD/requirements.txt (line 8)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch_model_summary->-r CLAD/requirements.txt (line 8)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch_model_summary->-r CLAD/requirements.txt (line 8)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch_model_summary->-r CLAD/requirements.txt (line 8)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch_model_summary->-r CLAD/requirements.txt (line 8)) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch_model_summary->-r CLAD/requirements.txt (line 8)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->pytorch_model_summary->-r CLAD/requirements.txt (line 8)) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->pytorch_model_summary->-r CLAD/requirements.txt (line 8)) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch->pytorch_model_summary->-r CLAD/requirements.txt (line 8)) (1.3.0)\n",
            "Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.0/18.0 MB 162.0 MB/s  0:00:00\n",
            "Downloading matplotlib-3.8.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.6/11.6 MB 138.7 MB/s  0:00:00\n",
            "Downloading primePy-1.3-py3-none-any.whl (4.0 kB)\n",
            "Downloading pytorch_model_summary-0.1.2-py3-none-any.whl (9.3 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Building wheels for collected packages: torchcontrib\n",
            "  Building wheel for torchcontrib (setup.py): started\n",
            "  Building wheel for torchcontrib (setup.py): finished with status 'done'\n",
            "  Created wheel for torchcontrib: filename=torchcontrib-0.0.2-py3-none-any.whl size=7516 sha256=bd1d8455049dfaf6761fbf76c41c06c29a0644178d12795921c8dcf8380dd07b\n",
            "  Stored in directory: /root/.cache/pip/wheels/e3/d1/1f/63f00ffea223db446943147a04ff035eb40d00cec3e87d63e5\n",
            "Successfully built torchcontrib\n",
            "Installing collected packages: torchcontrib, primePy, torchinfo, numpy, matplotlib, pytorch_model_summary\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.0\n",
            "    Uninstalling matplotlib-3.10.0:\n",
            "      Successfully uninstalled matplotlib-3.10.0\n",
            "\n",
            "Successfully installed matplotlib-3.8.4 numpy-1.26.4 primePy-1.3 pytorch_model_summary-0.1.2 torchcontrib-0.0.2 torchinfo-1.8.0\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:7 https://cli.github.com/packages stable/main amd64 Packages [344 B]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,368 kB]\n",
            "Get:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,429 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,816 kB]\n",
            "Hit:15 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [5,936 kB]\n",
            "Get:17 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [38.5 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,278 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,742 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,755 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,584 kB]\n",
            "Fetched 34.4 MB in 3s (11.5 MB/s)\n",
            "Reading package lists...\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "libsndfile1 is already the newest version (1.0.31-2ubuntu0.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n",
            "===== DONE: Environment pinned for Python 3.12 =====\n",
            "Python 3.12.12\n",
            "Python: 3.12.12\n",
            "NumPy: 1.26.4\n",
            "Numba: 0.60.0\n",
            "llvmlite: 0.43.0\n",
            "Matplotlib: 3.8.4\n",
            "torch: 2.3.1+cu121\n",
            "torchvision: 0.18.1+cu121\n",
            "torchaudio: 2.3.1+cu121\n",
            "librosa: 0.11.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cloning into 'CLAD'...\n",
            "  DEPRECATION: Building 'torchcontrib' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'torchcontrib'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4\n",
        "#Creating a differnet folder called my_audio so we won't destroy the data created\n",
        "\n",
        "import os, pathlib, shutil\n",
        "\n",
        "# Destination base folders\n",
        "REAL_DST = pathlib.Path(\"/content/my_audio/real\")\n",
        "TXT_DST  = pathlib.Path(\"/content/my_audio/txt\")\n",
        "FAKE_DST = pathlib.Path(\"/content/my_audio/fake\")\n",
        "\n",
        "REAL_DST.mkdir(parents=True, exist_ok=True)\n",
        "TXT_DST.mkdir(parents=True, exist_ok=True)\n",
        "FAKE_DST.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Source folders\n",
        "FAKE_SRC = pathlib.Path(\"/content/vctk_samples/VCTK-Corpus/VCTK-Corpus/fake_audio\")\n",
        "TXT_SRC  = pathlib.Path(\"/content/vctk_samples/VCTK-Corpus/VCTK-Corpus/txt\")\n",
        "REAL_SRC = pathlib.Path(\"/content/vctk_samples/VCTK-Corpus/VCTK-Corpus/wav48\")\n",
        "\n",
        "# Copy FAKE wavs while keeping speaker folders\n",
        "for folder in FAKE_SRC.glob(\"p*\"):\n",
        "    speaker_dst = FAKE_DST / folder.name\n",
        "    speaker_dst.mkdir(parents=True, exist_ok=True)\n",
        "    for wav in folder.glob(\"*.wav\"):\n",
        "        shutil.copy(wav, speaker_dst / wav.name)\n",
        "\n",
        "# Copy REAL wavs while keeping speaker folders\n",
        "for folder in REAL_SRC.glob(\"p*\"):\n",
        "    speaker_dst = REAL_DST / folder.name\n",
        "    speaker_dst.mkdir(parents=True, exist_ok=True)\n",
        "    for wav in folder.glob(\"*.wav\"):\n",
        "        shutil.copy(wav, speaker_dst / wav.name)\n",
        "\n",
        "# Copy TXT transcripts (flat, no subfolders in original)\n",
        "for folder in TXT_SRC.glob(\"p*\"):\n",
        "    speaker_dst = TXT_DST / folder.name\n",
        "    speaker_dst.mkdir(parents=True, exist_ok=True)\n",
        "    for txt in folder.glob(\"*.txt\"):\n",
        "        shutil.copy(txt, speaker_dst / txt.name)\n",
        "\n",
        "\n",
        "print(\"✅ All files copied with speaker folder structure preserved!\")\n",
        "print(\"  Real :\", REAL_DST)\n",
        "print(\"  Fake :\", FAKE_DST)\n",
        "print(\"  Text :\", TXT_DST)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGDTDGCmYbTT",
        "outputId": "0995b977-205d-42ed-e075-388488ee2471"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All files copied with speaker folder structure preserved!\n",
            "  Real : /content/my_audio/real\n",
            "  Fake : /content/my_audio/fake\n",
            "  Text : /content/my_audio/txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7\n",
        "# Speaker-disjoint splitter with:\n",
        "# - guaranteed non-empty val\n",
        "# - easy \"switch speakers\" controls\n",
        "# - deletes old OUT folder before writing\n",
        "#\n",
        "# Works for both 4 speakers (auto 2/1/1) and 5+ speakers (targets 3/1/1 by speakers).\n",
        "\n",
        "import os, shutil, random, glob, csv, itertools\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "ROOT = Path(\"/content/my_audio\")           # your current data root (real/fake/{speaker}/*.wav)\n",
        "OUT  = Path(\"/content/my_audio_split\")     # split will be (re)created here\n",
        "AUDIO_EXTS = {\".wav\", \".flac\", \".mp3\", \".m4a\", \".aac\", \".ogg\"}  # add if needed\n",
        "SEED = 42\n",
        "USE_SYMLINKS = True                        # False = copy files instead of symlink\n",
        "REQUIRE_BOTH_CLASSES = True                # speakers must exist under BOTH real/ and fake/\n",
        "# Desired speaker counts (train/val/test)\n",
        "DESIRED_311 = (3, 1, 1)                    # prefer 3/1/1 when you have ≥5 speakers\n",
        "FALLBACK_211 = (2, 1, 1)                   # for 4 speakers, this is the safe split\n",
        "# >>> Force specific speakers into splits (edit these to \"switch\")\n",
        "TRAIN_FORCE = set()                        # e.g., {\"p226\",\"p227\",\"p228\"}\n",
        "VAL_FORCE   = set()                        # e.g., {\"p225\"}\n",
        "TEST_FORCE  = set()                        # e.g., {\"p229\"}\n",
        "# --------------------------------------\n",
        "\n",
        "random.seed(SEED)\n",
        "\n",
        "def list_speakers(root, cls):\n",
        "    base = root/cls\n",
        "    if not base.exists(): return []\n",
        "    return sorted([d.name for d in base.iterdir() if d.is_dir() and d.name != \"txt\"])\n",
        "\n",
        "def list_audio(dirpath):\n",
        "    return sorted([p for p in dirpath.rglob(\"*\")\n",
        "                   if p.is_file() and p.suffix.lower() in AUDIO_EXTS])\n",
        "\n",
        "# 1) Find eligible speakers (present and non-empty under both classes if required)\n",
        "real_spk = set(list_speakers(ROOT, \"real\"))\n",
        "fake_spk = set(list_speakers(ROOT, \"fake\"))\n",
        "if REQUIRE_BOTH_CLASSES:\n",
        "    eligible = sorted(real_spk & fake_spk)\n",
        "else:\n",
        "    eligible = sorted(real_spk | fake_spk)\n",
        "\n",
        "def nonempty_both(s):\n",
        "    if not REQUIRE_BOTH_CLASSES:  # just need at least one side non-empty\n",
        "        return (len(list_audio(ROOT/\"real\"/s)) + len(list_audio(ROOT/\"fake\"/s))) > 0\n",
        "    return len(list_audio(ROOT/\"real\"/s)) > 0 and len(list_audio(ROOT/\"fake\"/s)) > 0\n",
        "\n",
        "eligible = [s for s in eligible if nonempty_both(s)]\n",
        "n_spk = len(eligible)\n",
        "if n_spk < 2:\n",
        "    raise RuntimeError(f\"Need ≥2 eligible speakers, found {n_spk}: {eligible}\")\n",
        "\n",
        "# 2) Choose target split sizes by number of speakers\n",
        "if n_spk >= sum(DESIRED_311):\n",
        "    N_TRAIN, N_VAL, N_TEST = DESIRED_311   # 3/1/1\n",
        "else:\n",
        "    # With 4 speakers, 2/1/1 is the right shape to keep val+test non-empty\n",
        "    N_TRAIN, N_VAL, N_TEST = FALLBACK_211  # 2/1/1\n",
        "\n",
        "# 3) Validate FORCE sets and fill remaining slots\n",
        "forced = TRAIN_FORCE | VAL_FORCE | TEST_FORCE\n",
        "if forced:\n",
        "    missing = forced - set(eligible)\n",
        "    if missing:\n",
        "        raise RuntimeError(f\"Forced speakers not found/eligible: {sorted(missing)}\")\n",
        "    overlap = (TRAIN_FORCE & VAL_FORCE) | (TRAIN_FORCE & TEST_FORCE) | (VAL_FORCE & TEST_FORCE)\n",
        "    if overlap:\n",
        "        raise RuntimeError(f\"Forced sets overlap: {sorted(overlap)}\")\n",
        "\n",
        "# file counts (use 'real' side as proxy for per-speaker volume)\n",
        "spk_counts = {s: len(list_audio(ROOT/\"real\"/s)) for s in eligible}\n",
        "total_files = sum(spk_counts.values())\n",
        "\n",
        "def pick_k_closest(candidates, k, target_share):\n",
        "    \"\"\"Pick k speakers whose file-count sum is closest to target_share (in files).\"\"\"\n",
        "    if k <= 0: return set()\n",
        "    if len(candidates) <= k: return set(candidates)\n",
        "    best, gap = None, float(\"inf\")\n",
        "    for combo in itertools.combinations(candidates, k):\n",
        "        share = sum(spk_counts[s] for s in combo)\n",
        "        g = abs(share - target_share)\n",
        "        if g < gap:\n",
        "            gap, best = g, set(combo)\n",
        "    return best\n",
        "\n",
        "# Start with forced\n",
        "train_set, val_set, test_set = set(TRAIN_FORCE), set(VAL_FORCE), set(TEST_FORCE)\n",
        "remaining = [s for s in eligible if s not in (train_set | val_set | test_set)]\n",
        "\n",
        "need_train = max(0, N_TRAIN - len(train_set))\n",
        "need_val   = max(0, N_VAL   - len(val_set))\n",
        "need_test  = max(0, N_TEST  - len(test_set))\n",
        "\n",
        "# Target train file share (rough guideline): ~60% if 3/1/1, ~50% if 2/1/1\n",
        "target_train_share = 0.60*total_files if (N_TRAIN, N_VAL, N_TEST) == DESIRED_311 else 0.50*total_files\n",
        "\n",
        "# Fill TRAIN first to hit the share as best as possible\n",
        "if need_train > 0:\n",
        "    add = pick_k_closest(remaining, need_train, target_train_share - sum(spk_counts[s] for s in train_set))\n",
        "    train_set |= add\n",
        "    remaining = [s for s in remaining if s not in add]\n",
        "\n",
        "# Fill VAL with lighter speakers (to keep val/test similar size)\n",
        "if need_val > 0:\n",
        "    remaining.sort(key=lambda s: spk_counts[s])  # lightest first\n",
        "    add = set(remaining[:need_val])\n",
        "    val_set |= add\n",
        "    remaining = remaining[need_val:]\n",
        "\n",
        "# Fill TEST with the rest needed\n",
        "if need_test > 0:\n",
        "    add = set(remaining[:need_test])\n",
        "    test_set |= add\n",
        "    remaining = remaining[need_test:]\n",
        "\n",
        "# Final sanity: exact sizes, disjointness\n",
        "if not (len(train_set) == N_TRAIN and len(val_set) == N_VAL and len(test_set) == N_TEST):\n",
        "    raise RuntimeError(f\"Final sizes must be {N_TRAIN}/{N_VAL}/{N_TEST}, got {len(train_set)}/{len(val_set)}/{len(test_set)}\")\n",
        "if not (train_set.isdisjoint(val_set) and train_set.isdisjoint(test_set) and val_set.isdisjoint(test_set)):\n",
        "    raise RuntimeError(\"Splits are not disjoint by speakers.\")\n",
        "\n",
        "print(\"Eligible speakers:\", eligible)\n",
        "print(\"Chosen split (by speakers):\")\n",
        "print(\"  train:\", sorted(train_set))\n",
        "print(\"  val  :\", sorted(val_set))\n",
        "print(\"  test :\", sorted(test_set))\n",
        "print(\"By-file shares (train/val/test):\",\n",
        "      round(sum(spk_counts[s] for s in train_set)/total_files, 3),\n",
        "      round(sum(spk_counts[s] for s in val_set)/total_files, 3),\n",
        "      round(sum(spk_counts[s] for s in test_set)/total_files, 3))\n",
        "\n",
        "# 4) DELETE OLD OUT (so the previous no-val split is removed), then rebuild\n",
        "if OUT.exists():\n",
        "    shutil.rmtree(OUT)\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "    for cls in [\"real\", \"fake\"]:\n",
        "        (OUT/split/cls).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def which_split(speaker):\n",
        "    if speaker in train_set: return \"train\"\n",
        "    if speaker in val_set:   return \"val\"\n",
        "    return \"test\"\n",
        "\n",
        "# 5) Materialize split (symlink/copy) + manifest\n",
        "rows, counts = [], defaultdict(int)\n",
        "for cls in [\"real\", \"fake\"]:\n",
        "    base = ROOT/cls\n",
        "    for sp in (train_set | val_set | test_set):\n",
        "        src_dir = base/sp\n",
        "        dst_dir = OUT/which_split(sp)/cls/sp\n",
        "        dst_dir.mkdir(parents=True, exist_ok=True)\n",
        "        for src in list_audio(src_dir):\n",
        "            dst = dst_dir/src.name\n",
        "            if dst.exists():\n",
        "                try: dst.unlink()\n",
        "                except: pass\n",
        "            if USE_SYMLINKS:\n",
        "                try:\n",
        "                    os.symlink(src.resolve(), dst)\n",
        "                except FileExistsError:\n",
        "                    pass\n",
        "            else:\n",
        "                shutil.copy2(src, dst)\n",
        "            rows.append({\n",
        "                \"split\": which_split(sp),\n",
        "                \"speaker\": sp,\n",
        "                \"label\": 0 if cls==\"real\" else 1,\n",
        "                \"src_path\": str(src.resolve()),\n",
        "                \"dst_path\": str(dst.resolve())\n",
        "            })\n",
        "            counts[(which_split(sp), cls)] += 1\n",
        "\n",
        "manifest_csv = OUT/\"manifest.csv\"\n",
        "with open(manifest_csv, \"w\", newline=\"\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=[\"split\",\"speaker\",\"label\",\"src_path\",\"dst_path\"])\n",
        "    writer.writeheader(); writer.writerows(rows)\n",
        "\n",
        "print(\"\\nManifest:\", manifest_csv)\n",
        "print(\"Counts per split/class:\")\n",
        "for split in [\"train\",\"val\",\"test\"]:\n",
        "    for cls in [\"real\",\"fake\"]:\n",
        "        print(f\"  {split:5s} {cls:4s}: {counts[(split, cls)]}\")\n",
        "\n",
        "# Extra safety: show speaker overlap (should be empty)\n",
        "print(\"\\nOverlap checks (should be empty):\")\n",
        "print(\"  train ∩ val :\", train_set & val_set)\n",
        "print(\"  train ∩ test:\", train_set & test_set)\n",
        "print(\"  val   ∩ test:\", val_set & test_set)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHwvFTkYY4Ba",
        "outputId": "a5e8f48b-e6fc-49f9-fe51-9032b4ca4db5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eligible speakers: ['p225', 'p226', 'p227', 'p228']\n",
            "Chosen split (by speakers):\n",
            "  train: ['p225', 'p227']\n",
            "  val  : ['p226']\n",
            "  test : ['p228']\n",
            "By-file shares (train/val/test): 0.462 0.265 0.273\n",
            "\n",
            "Manifest: /content/my_audio_split/manifest.csv\n",
            "Counts per split/class:\n",
            "  train real: 620\n",
            "  train fake: 620\n",
            "  val   real: 356\n",
            "  val   fake: 356\n",
            "  test  real: 366\n",
            "  test  fake: 366\n",
            "\n",
            "Overlap checks (should be empty):\n",
            "  train ∩ val : set()\n",
            "  train ∩ test: set()\n",
            "  val   ∩ test: set()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6\n",
        "# Choose ONE of the two options below:\n",
        "\n",
        "USE_LIBROSA = True   # set to False to use scipy.io.wavfile only\n",
        "\n",
        "if USE_LIBROSA:\n",
        "    # Librosa path: convenient resample-to-16k + mono in one call\n",
        "    !pip -q install librosa soundfile\n",
        "else:\n",
        "    # Scipy path: no extra system libs; we will do a small numpy resample\n",
        "    !pip -q install scipy"
      ],
      "metadata": {
        "id": "SQIRMktSh-TU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix NumPy/SciPy ABI mismatch for Python 3.12\n",
        "%pip uninstall -y numpy scipy\n",
        "%pip cache purge\n",
        "\n",
        "# Install a matching pair (works well on Colab Py3.12)\n",
        "%pip install --no-cache-dir --force-reinstall \"numpy==2.1.3\" \"scipy==1.14.1\" \"soundfile>=0.12.0\"\n",
        "\n",
        "import IPython; print(\"🔄 Restarting kernel…\"); IPython.Application.instance().kernel.do_shutdown(True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 876
        },
        "id": "F4kr3-HGBo80",
        "outputId": "d0107197-7c63-47a6-dd3e-4adee666b0d4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "Found existing installation: scipy 1.16.2\n",
            "Uninstalling scipy-1.16.2:\n",
            "  Successfully uninstalled scipy-1.16.2\n",
            "Files removed: 181 (2854.7 MB)\n",
            "Collecting numpy==2.1.3\n",
            "  Downloading numpy-2.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Collecting scipy==1.14.1\n",
            "  Downloading scipy-1.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Collecting soundfile>=0.12.0\n",
            "  Downloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (16 kB)\n",
            "Collecting cffi>=1.0 (from soundfile>=0.12.0)\n",
            "  Downloading cffi-2.0.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.6 kB)\n",
            "Collecting pycparser (from cffi>=1.0->soundfile>=0.12.0)\n",
            "  Downloading pycparser-2.23-py3-none-any.whl.metadata (993 bytes)\n",
            "Downloading numpy-2.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m252.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m281.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m276.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cffi-2.0.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (219 kB)\n",
            "Downloading pycparser-2.23-py3-none-any.whl (118 kB)\n",
            "Installing collected packages: pycparser, numpy, scipy, cffi, soundfile\n",
            "\u001b[2K  Attempting uninstall: pycparser\n",
            "\u001b[2K    Found existing installation: pycparser 2.23\n",
            "\u001b[2K    Uninstalling pycparser-2.23:\n",
            "\u001b[2K      Successfully uninstalled pycparser-2.23\n",
            "\u001b[2K  Attempting uninstall: cffi\n",
            "\u001b[2K    Found existing installation: cffi 2.0.0\n",
            "\u001b[2K    Uninstalling cffi-2.0.0:\n",
            "\u001b[2K      Successfully uninstalled cffi-2.0.0\n",
            "\u001b[2K  Attempting uninstall: soundfile\n",
            "\u001b[2K    Found existing installation: soundfile 0.13.1\n",
            "\u001b[2K    Uninstalling soundfile-0.13.1:\n",
            "\u001b[2K      Successfully uninstalled soundfile-0.13.1\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [soundfile]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.1.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cffi-2.0.0 numpy-2.1.3 pycparser-2.23 scipy-1.14.1 soundfile-0.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_cffi_backend",
                  "_soundfile",
                  "_soundfile_data",
                  "numpy",
                  "scipy",
                  "soundfile"
                ]
              },
              "id": "1975f70de4d74a8bb6af64a293ac68f5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔄 Restarting kernel…\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess audio with soundfile+scipy: 16kHz mono + pad/trim to 64600\n",
        "from pathlib import Path\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "from scipy.signal import resample_poly\n",
        "import math\n",
        "\n",
        "TARGET_SR  = 16000\n",
        "TARGET_LEN = 64600  # ~4s @16kHz\n",
        "\n",
        "def resample_to(x, sr, target_sr):\n",
        "    if sr == target_sr:\n",
        "        return x\n",
        "    g   = math.gcd(sr, target_sr)\n",
        "    up  = target_sr // g\n",
        "    down= sr // g\n",
        "    return resample_poly(x, up, down).astype(np.float32)\n",
        "\n",
        "def preprocess_wav_np(in_path: Path):\n",
        "    x, sr = sf.read(str(in_path), dtype=\"float32\")  # x: [T] or [T, C]\n",
        "    if x.ndim > 1:  # to mono\n",
        "        x = x.mean(axis=1)\n",
        "    x = resample_to(x, sr, TARGET_SR)\n",
        "    T = x.shape[0]\n",
        "    if T < TARGET_LEN:\n",
        "        x = np.pad(x, (0, TARGET_LEN - T))\n",
        "    else:\n",
        "        x = x[:TARGET_LEN]\n",
        "    return x\n",
        "\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "    split_dir = Path(\"/content/my_audio_split\") / split\n",
        "    for cls in [\"real\", \"fake\"]:\n",
        "        for wav_path in (split_dir / cls).rglob(\"*.wav\"):\n",
        "            x = preprocess_wav_np(wav_path)\n",
        "            sf.write(str(wav_path), x, TARGET_SR)\n",
        "\n",
        "print(\"✅ All audio preprocessed to 16kHz (soundfile+scipy) and padded/clipped to 64600 samples\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNTVqDPYiGZ3",
        "outputId": "42388aec-c41f-421b-e727-efd23c6fd4c3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All audio preprocessed to 16kHz (soundfile+scipy) and padded/clipped to 64600 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) נפטרים מספרייה שמכריחה numba (לא צריך כשמשתמשים ב-torchaudio)\n",
        "%pip uninstall -y librosa numba resampy llvmlite soundfile\n",
        "\n",
        "# 2) מיישרים גרסאות שתואמות ל-Python 3.12 ול-coqui/tsfresh\n",
        "%pip install -q --no-cache-dir \"numpy==2.1.2\" \"scipy==1.14.1\"\n",
        "\n",
        "# 3) אתחול מלא של הקרנל כדי שה-ABI ייטען מחדש מול הגרסאות החדשות\n",
        "import IPython; print(\"🔄 Restarting kernel...\"); IPython.Application.instance().kernel.do_shutdown(True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03xHpSPlvtys",
        "outputId": "73e44fcd-b910-425a-efc1-044677c1ccd6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: librosa 0.11.0\n",
            "Uninstalling librosa-0.11.0:\n",
            "  Successfully uninstalled librosa-0.11.0\n",
            "Found existing installation: numba 0.60.0\n",
            "Uninstalling numba-0.60.0:\n",
            "  Successfully uninstalled numba-0.60.0\n",
            "\u001b[33mWARNING: Skipping resampy as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: llvmlite 0.43.0\n",
            "Uninstalling llvmlite-0.43.0:\n",
            "  Successfully uninstalled llvmlite-0.43.0\n",
            "Found existing installation: soundfile 0.13.1\n",
            "Uninstalling soundfile-0.13.1:\n",
            "  Successfully uninstalled soundfile-0.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "coqui-tts 0.27.2 requires librosa>=0.11.0, which is not installed.\n",
            "coqui-tts 0.27.2 requires numba>=0.58.0, which is not installed.\n",
            "coqui-tts 0.27.2 requires soundfile>=0.12.0, which is not installed.\n",
            "coqui-tts-trainer 0.3.1 requires soundfile>=0.12.0, which is not installed.\n",
            "cuml-cu12 25.6.0 requires numba<0.62.0a0,>=0.59.1, which is not installed.\n",
            "cudf-cu12 25.6.0 requires numba<0.62.0a0,>=0.59.1, which is not installed.\n",
            "dask-cuda 25.6.0 requires numba<0.62.0a0,>=0.59.1, which is not installed.\n",
            "stumpy 1.13.0 requires numba>=0.57.1, which is not installed.\n",
            "umap-learn 0.5.9.post2 requires numba>=0.51.2, which is not installed.\n",
            "pynndescent 0.5.13 requires llvmlite>=0.30, which is not installed.\n",
            "pynndescent 0.5.13 requires numba>=0.51.2, which is not installed.\n",
            "distributed-ucxx-cu12 0.44.0 requires numba<0.62.0a0,>=0.59.1, which is not installed.\n",
            "shap 0.48.0 requires numba>=0.54, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m🔄 Restarting kernel...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === DIAG A: בדיקת סביבה בסיסית ===\n",
        "import numpy as np, scipy, torch, torchaudio, platform\n",
        "print(\"python:\", platform.python_version())\n",
        "print(\"numpy :\", np.__version__)\n",
        "print(\"scipy :\", scipy.__version__)\n",
        "print(\"torch :\", torch.__version__)\n",
        "print(\"torchaudio:\", torchaudio.__version__)\n",
        "\n",
        "# מבחן ABI ל-numpy.random (אם זה נופל -> ABI שבור)\n",
        "from numpy.random import RandomState\n",
        "_ = RandomState(0)\n",
        "print(\"✅ numpy.random ABI OK\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfspLOTwmK8g",
        "outputId": "d1718d17-1158-4d36-f4eb-09b976c60cec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python: 3.12.12\n",
            "numpy : 2.1.2\n",
            "scipy : 1.14.1\n",
            "torch : 2.3.1+cu121\n",
            "torchaudio: 2.3.1+cu121\n",
            "✅ numpy.random ABI OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean & reinstall ABI-compatible wheels for Py 3.12\n",
        "%pip uninstall -y numpy numpy-base\n",
        "%pip cache purge\n",
        "%pip install -q --no-cache-dir --force-reinstall \"numpy==2.1.3\" \"scipy==1.14.1\"\n",
        "\n",
        "# Restart kernel (mandatory)\n",
        "import IPython; print(\"🔄 Restarting kernel...\"); IPython.Application.instance().kernel.do_shutdown(True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abGTlBnqmcA2",
        "outputId": "6331a009-7153-4b7d-cf0c-f2b2cdab5d06"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 2.1.2\n",
            "Uninstalling numpy-2.1.2:\n",
            "  Successfully uninstalled numpy-2.1.2\n",
            "\u001b[33mWARNING: Skipping numpy-base as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: No matching packages\u001b[0m\u001b[33m\n",
            "\u001b[0mFiles removed: 0 (0 bytes)\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "coqui-tts 0.27.2 requires librosa>=0.11.0, which is not installed.\n",
            "coqui-tts 0.27.2 requires numba>=0.58.0, which is not installed.\n",
            "coqui-tts 0.27.2 requires soundfile>=0.12.0, which is not installed.\n",
            "coqui-tts-trainer 0.3.1 requires soundfile>=0.12.0, which is not installed.\n",
            "cuml-cu12 25.6.0 requires numba<0.62.0a0,>=0.59.1, which is not installed.\n",
            "cudf-cu12 25.6.0 requires numba<0.62.0a0,>=0.59.1, which is not installed.\n",
            "dask-cuda 25.6.0 requires numba<0.62.0a0,>=0.59.1, which is not installed.\n",
            "stumpy 1.13.0 requires numba>=0.57.1, which is not installed.\n",
            "umap-learn 0.5.9.post2 requires numba>=0.51.2, which is not installed.\n",
            "pynndescent 0.5.13 requires llvmlite>=0.30, which is not installed.\n",
            "pynndescent 0.5.13 requires numba>=0.51.2, which is not installed.\n",
            "distributed-ucxx-cu12 0.44.0 requires numba<0.62.0a0,>=0.59.1, which is not installed.\n",
            "shap 0.48.0 requires numba>=0.54, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m🔄 Restarting kernel...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8\n",
        "import torch\n",
        "from pathlib import Path\n",
        "import os\n",
        "os.chdir('/content/CLAD')\n",
        "\n",
        "from Model import MoCo_v2, RawNetEncoderBaseline\n",
        "\n",
        "# Step 1: Define the RawNet encoder configuration\n",
        "d_args = {\n",
        "    \"in_channels\": 1,\n",
        "    \"first_conv\": 251,\n",
        "    \"filts\": [\n",
        "        128,  # output channels for sinc conv\n",
        "        [128, 128],  # block0 and block1\n",
        "        [128, 256],  # block2\n",
        "        [256, 256]   # block3-5\n",
        "    ],\n",
        "    \"nb_fc_node\": 1024,\n",
        "    \"gru_node\": 1024,\n",
        "    \"nb_gru_layer\": 3,\n",
        "    \"nb_classes\": 2\n",
        "}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Step 2: Create both encoders\n",
        "encoder_q = RawNetEncoderBaseline(d_args, device)\n",
        "encoder_k = RawNetEncoderBaseline(d_args, device)\n",
        "\n",
        "# Step 3: Create the MoCo_v2 model\n",
        "model = MoCo_v2(\n",
        "    encoder_q=encoder_q,\n",
        "    encoder_k=encoder_k,\n",
        "    queue_feature_dim=1024,  # matches encoder output\n",
        "    mlp=True,\n",
        "    return_q=True\n",
        ")\n",
        "\n",
        "# Step 4: Load pretrained weights\n",
        "ckpt_path = Path(\"pretrained_models/CLAD_150_10_2310.pth.tar\")\n",
        "if not ckpt_path.exists():\n",
        "    raise FileNotFoundError(f\"Checkpoint not found: {ckpt_path}\")\n",
        "\n",
        "ckpt = torch.load(ckpt_path, map_location='cpu')\n",
        "state_dict = ckpt.get(\"state_dict\", ckpt)\n",
        "\n",
        "missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n",
        "print(\"Missing keys:\", missing_keys)\n",
        "print(\"Unexpected keys:\", unexpected_keys)\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"CLAD model loaded and ready.\")\n",
        "\n",
        "SR = 16000\n",
        "CLIP_SECONDS = 4.0\n",
        "N_SAMPLES = int(SR * CLIP_SECONDS)  # = 64000\n",
        "\n",
        "with torch.no_grad():\n",
        "    dummy = torch.randn(2, N_SAMPLES, device=device)  # (B,T)\n",
        "    try:\n",
        "        f = encoder_q(dummy)              # try (B,T)\n",
        "    except Exception:\n",
        "        f = encoder_q(dummy.unsqueeze(1)) # fallback (B,1,T)\n",
        "\n",
        "    # Pool any extra time/freq dims so we have (B,D)\n",
        "    if f.dim() == 3:\n",
        "        f = f.mean(dim=2)                 # (B,D)\n",
        "    elif f.dim() > 3:\n",
        "        f = f.mean(dim=tuple(range(2, f.dim())))  # reduce to (B,D)\n",
        "\n",
        "    print(\"Encoder output shape:\", tuple(f.shape))\n",
        "    D = f.shape[1]\n",
        "    print(\"Probed feature dim D =\", D)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TaUUxo-Ym-bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "776316d4-ff02-4411-baa4-b14be632375c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing keys: ['encoder_q.first_bn.weight', 'encoder_q.first_bn.bias', 'encoder_q.first_bn.running_mean', 'encoder_q.first_bn.running_var', 'encoder_q.block0.0.conv1.weight', 'encoder_q.block0.0.conv1.bias', 'encoder_q.block0.0.bn2.weight', 'encoder_q.block0.0.bn2.bias', 'encoder_q.block0.0.bn2.running_mean', 'encoder_q.block0.0.bn2.running_var', 'encoder_q.block0.0.conv2.weight', 'encoder_q.block0.0.conv2.bias', 'encoder_q.block1.0.bn1.weight', 'encoder_q.block1.0.bn1.bias', 'encoder_q.block1.0.bn1.running_mean', 'encoder_q.block1.0.bn1.running_var', 'encoder_q.block1.0.conv1.weight', 'encoder_q.block1.0.conv1.bias', 'encoder_q.block1.0.bn2.weight', 'encoder_q.block1.0.bn2.bias', 'encoder_q.block1.0.bn2.running_mean', 'encoder_q.block1.0.bn2.running_var', 'encoder_q.block1.0.conv2.weight', 'encoder_q.block1.0.conv2.bias', 'encoder_q.block2.0.bn1.weight', 'encoder_q.block2.0.bn1.bias', 'encoder_q.block2.0.bn1.running_mean', 'encoder_q.block2.0.bn1.running_var', 'encoder_q.block2.0.conv1.weight', 'encoder_q.block2.0.conv1.bias', 'encoder_q.block2.0.bn2.weight', 'encoder_q.block2.0.bn2.bias', 'encoder_q.block2.0.bn2.running_mean', 'encoder_q.block2.0.bn2.running_var', 'encoder_q.block2.0.conv2.weight', 'encoder_q.block2.0.conv2.bias', 'encoder_q.block2.0.conv_downsample.weight', 'encoder_q.block2.0.conv_downsample.bias', 'encoder_q.block3.0.bn1.weight', 'encoder_q.block3.0.bn1.bias', 'encoder_q.block3.0.bn1.running_mean', 'encoder_q.block3.0.bn1.running_var', 'encoder_q.block3.0.conv1.weight', 'encoder_q.block3.0.conv1.bias', 'encoder_q.block3.0.bn2.weight', 'encoder_q.block3.0.bn2.bias', 'encoder_q.block3.0.bn2.running_mean', 'encoder_q.block3.0.bn2.running_var', 'encoder_q.block3.0.conv2.weight', 'encoder_q.block3.0.conv2.bias', 'encoder_q.block4.0.bn1.weight', 'encoder_q.block4.0.bn1.bias', 'encoder_q.block4.0.bn1.running_mean', 'encoder_q.block4.0.bn1.running_var', 'encoder_q.block4.0.conv1.weight', 'encoder_q.block4.0.conv1.bias', 'encoder_q.block4.0.bn2.weight', 'encoder_q.block4.0.bn2.bias', 'encoder_q.block4.0.bn2.running_mean', 'encoder_q.block4.0.bn2.running_var', 'encoder_q.block4.0.conv2.weight', 'encoder_q.block4.0.conv2.bias', 'encoder_q.block5.0.bn1.weight', 'encoder_q.block5.0.bn1.bias', 'encoder_q.block5.0.bn1.running_mean', 'encoder_q.block5.0.bn1.running_var', 'encoder_q.block5.0.conv1.weight', 'encoder_q.block5.0.conv1.bias', 'encoder_q.block5.0.bn2.weight', 'encoder_q.block5.0.bn2.bias', 'encoder_q.block5.0.bn2.running_mean', 'encoder_q.block5.0.bn2.running_var', 'encoder_q.block5.0.conv2.weight', 'encoder_q.block5.0.conv2.bias', 'encoder_q.fc_attention0.0.weight', 'encoder_q.fc_attention0.0.bias', 'encoder_q.fc_attention1.0.weight', 'encoder_q.fc_attention1.0.bias', 'encoder_q.fc_attention2.0.weight', 'encoder_q.fc_attention2.0.bias', 'encoder_q.fc_attention3.0.weight', 'encoder_q.fc_attention3.0.bias', 'encoder_q.fc_attention4.0.weight', 'encoder_q.fc_attention4.0.bias', 'encoder_q.fc_attention5.0.weight', 'encoder_q.fc_attention5.0.bias', 'encoder_q.bn_before_gru.weight', 'encoder_q.bn_before_gru.bias', 'encoder_q.bn_before_gru.running_mean', 'encoder_q.bn_before_gru.running_var', 'encoder_q.gru.weight_ih_l0', 'encoder_q.gru.weight_hh_l0', 'encoder_q.gru.bias_ih_l0', 'encoder_q.gru.bias_hh_l0', 'encoder_q.gru.weight_ih_l1', 'encoder_q.gru.weight_hh_l1', 'encoder_q.gru.bias_ih_l1', 'encoder_q.gru.bias_hh_l1', 'encoder_q.gru.weight_ih_l2', 'encoder_q.gru.weight_hh_l2', 'encoder_q.gru.bias_ih_l2', 'encoder_q.gru.bias_hh_l2', 'encoder_q.fc1_gru.weight', 'encoder_q.fc1_gru.bias', 'encoder_q.fc2_gru.weight', 'encoder_q.fc2_gru.bias', 'encoder_k.first_bn.weight', 'encoder_k.first_bn.bias', 'encoder_k.first_bn.running_mean', 'encoder_k.first_bn.running_var', 'encoder_k.block0.0.conv1.weight', 'encoder_k.block0.0.conv1.bias', 'encoder_k.block0.0.bn2.weight', 'encoder_k.block0.0.bn2.bias', 'encoder_k.block0.0.bn2.running_mean', 'encoder_k.block0.0.bn2.running_var', 'encoder_k.block0.0.conv2.weight', 'encoder_k.block0.0.conv2.bias', 'encoder_k.block1.0.bn1.weight', 'encoder_k.block1.0.bn1.bias', 'encoder_k.block1.0.bn1.running_mean', 'encoder_k.block1.0.bn1.running_var', 'encoder_k.block1.0.conv1.weight', 'encoder_k.block1.0.conv1.bias', 'encoder_k.block1.0.bn2.weight', 'encoder_k.block1.0.bn2.bias', 'encoder_k.block1.0.bn2.running_mean', 'encoder_k.block1.0.bn2.running_var', 'encoder_k.block1.0.conv2.weight', 'encoder_k.block1.0.conv2.bias', 'encoder_k.block2.0.bn1.weight', 'encoder_k.block2.0.bn1.bias', 'encoder_k.block2.0.bn1.running_mean', 'encoder_k.block2.0.bn1.running_var', 'encoder_k.block2.0.conv1.weight', 'encoder_k.block2.0.conv1.bias', 'encoder_k.block2.0.bn2.weight', 'encoder_k.block2.0.bn2.bias', 'encoder_k.block2.0.bn2.running_mean', 'encoder_k.block2.0.bn2.running_var', 'encoder_k.block2.0.conv2.weight', 'encoder_k.block2.0.conv2.bias', 'encoder_k.block2.0.conv_downsample.weight', 'encoder_k.block2.0.conv_downsample.bias', 'encoder_k.block3.0.bn1.weight', 'encoder_k.block3.0.bn1.bias', 'encoder_k.block3.0.bn1.running_mean', 'encoder_k.block3.0.bn1.running_var', 'encoder_k.block3.0.conv1.weight', 'encoder_k.block3.0.conv1.bias', 'encoder_k.block3.0.bn2.weight', 'encoder_k.block3.0.bn2.bias', 'encoder_k.block3.0.bn2.running_mean', 'encoder_k.block3.0.bn2.running_var', 'encoder_k.block3.0.conv2.weight', 'encoder_k.block3.0.conv2.bias', 'encoder_k.block4.0.bn1.weight', 'encoder_k.block4.0.bn1.bias', 'encoder_k.block4.0.bn1.running_mean', 'encoder_k.block4.0.bn1.running_var', 'encoder_k.block4.0.conv1.weight', 'encoder_k.block4.0.conv1.bias', 'encoder_k.block4.0.bn2.weight', 'encoder_k.block4.0.bn2.bias', 'encoder_k.block4.0.bn2.running_mean', 'encoder_k.block4.0.bn2.running_var', 'encoder_k.block4.0.conv2.weight', 'encoder_k.block4.0.conv2.bias', 'encoder_k.block5.0.bn1.weight', 'encoder_k.block5.0.bn1.bias', 'encoder_k.block5.0.bn1.running_mean', 'encoder_k.block5.0.bn1.running_var', 'encoder_k.block5.0.conv1.weight', 'encoder_k.block5.0.conv1.bias', 'encoder_k.block5.0.bn2.weight', 'encoder_k.block5.0.bn2.bias', 'encoder_k.block5.0.bn2.running_mean', 'encoder_k.block5.0.bn2.running_var', 'encoder_k.block5.0.conv2.weight', 'encoder_k.block5.0.conv2.bias', 'encoder_k.fc_attention0.0.weight', 'encoder_k.fc_attention0.0.bias', 'encoder_k.fc_attention1.0.weight', 'encoder_k.fc_attention1.0.bias', 'encoder_k.fc_attention2.0.weight', 'encoder_k.fc_attention2.0.bias', 'encoder_k.fc_attention3.0.weight', 'encoder_k.fc_attention3.0.bias', 'encoder_k.fc_attention4.0.weight', 'encoder_k.fc_attention4.0.bias', 'encoder_k.fc_attention5.0.weight', 'encoder_k.fc_attention5.0.bias', 'encoder_k.bn_before_gru.weight', 'encoder_k.bn_before_gru.bias', 'encoder_k.bn_before_gru.running_mean', 'encoder_k.bn_before_gru.running_var', 'encoder_k.gru.weight_ih_l0', 'encoder_k.gru.weight_hh_l0', 'encoder_k.gru.bias_ih_l0', 'encoder_k.gru.bias_hh_l0', 'encoder_k.gru.weight_ih_l1', 'encoder_k.gru.weight_hh_l1', 'encoder_k.gru.bias_ih_l1', 'encoder_k.gru.bias_hh_l1', 'encoder_k.gru.weight_ih_l2', 'encoder_k.gru.weight_hh_l2', 'encoder_k.gru.bias_ih_l2', 'encoder_k.gru.bias_hh_l2', 'encoder_k.fc1_gru.weight', 'encoder_k.fc1_gru.bias', 'encoder_k.fc2_gru.weight', 'encoder_k.fc2_gru.bias', 'projection_head_q.0.weight', 'projection_head_q.0.bias', 'projection_head_q.2.weight', 'projection_head_q.2.bias', 'projection_head_k.0.weight', 'projection_head_k.0.bias', 'projection_head_k.2.weight', 'projection_head_k.2.bias']\n",
            "Unexpected keys: ['encoder.pos_S', 'encoder.master1', 'encoder.master2', 'encoder.first_bn.weight', 'encoder.first_bn.bias', 'encoder.first_bn.running_mean', 'encoder.first_bn.running_var', 'encoder.first_bn.num_batches_tracked', 'encoder.encoder.0.0.conv1.weight', 'encoder.encoder.0.0.conv1.bias', 'encoder.encoder.0.0.bn2.weight', 'encoder.encoder.0.0.bn2.bias', 'encoder.encoder.0.0.bn2.running_mean', 'encoder.encoder.0.0.bn2.running_var', 'encoder.encoder.0.0.bn2.num_batches_tracked', 'encoder.encoder.0.0.conv2.weight', 'encoder.encoder.0.0.conv2.bias', 'encoder.encoder.0.0.conv_downsample.weight', 'encoder.encoder.0.0.conv_downsample.bias', 'encoder.encoder.1.0.bn1.weight', 'encoder.encoder.1.0.bn1.bias', 'encoder.encoder.1.0.bn1.running_mean', 'encoder.encoder.1.0.bn1.running_var', 'encoder.encoder.1.0.bn1.num_batches_tracked', 'encoder.encoder.1.0.conv1.weight', 'encoder.encoder.1.0.conv1.bias', 'encoder.encoder.1.0.bn2.weight', 'encoder.encoder.1.0.bn2.bias', 'encoder.encoder.1.0.bn2.running_mean', 'encoder.encoder.1.0.bn2.running_var', 'encoder.encoder.1.0.bn2.num_batches_tracked', 'encoder.encoder.1.0.conv2.weight', 'encoder.encoder.1.0.conv2.bias', 'encoder.encoder.2.0.bn1.weight', 'encoder.encoder.2.0.bn1.bias', 'encoder.encoder.2.0.bn1.running_mean', 'encoder.encoder.2.0.bn1.running_var', 'encoder.encoder.2.0.bn1.num_batches_tracked', 'encoder.encoder.2.0.conv1.weight', 'encoder.encoder.2.0.conv1.bias', 'encoder.encoder.2.0.bn2.weight', 'encoder.encoder.2.0.bn2.bias', 'encoder.encoder.2.0.bn2.running_mean', 'encoder.encoder.2.0.bn2.running_var', 'encoder.encoder.2.0.bn2.num_batches_tracked', 'encoder.encoder.2.0.conv2.weight', 'encoder.encoder.2.0.conv2.bias', 'encoder.encoder.2.0.conv_downsample.weight', 'encoder.encoder.2.0.conv_downsample.bias', 'encoder.encoder.3.0.bn1.weight', 'encoder.encoder.3.0.bn1.bias', 'encoder.encoder.3.0.bn1.running_mean', 'encoder.encoder.3.0.bn1.running_var', 'encoder.encoder.3.0.bn1.num_batches_tracked', 'encoder.encoder.3.0.conv1.weight', 'encoder.encoder.3.0.conv1.bias', 'encoder.encoder.3.0.bn2.weight', 'encoder.encoder.3.0.bn2.bias', 'encoder.encoder.3.0.bn2.running_mean', 'encoder.encoder.3.0.bn2.running_var', 'encoder.encoder.3.0.bn2.num_batches_tracked', 'encoder.encoder.3.0.conv2.weight', 'encoder.encoder.3.0.conv2.bias', 'encoder.encoder.4.0.bn1.weight', 'encoder.encoder.4.0.bn1.bias', 'encoder.encoder.4.0.bn1.running_mean', 'encoder.encoder.4.0.bn1.running_var', 'encoder.encoder.4.0.bn1.num_batches_tracked', 'encoder.encoder.4.0.conv1.weight', 'encoder.encoder.4.0.conv1.bias', 'encoder.encoder.4.0.bn2.weight', 'encoder.encoder.4.0.bn2.bias', 'encoder.encoder.4.0.bn2.running_mean', 'encoder.encoder.4.0.bn2.running_var', 'encoder.encoder.4.0.bn2.num_batches_tracked', 'encoder.encoder.4.0.conv2.weight', 'encoder.encoder.4.0.conv2.bias', 'encoder.encoder.5.0.bn1.weight', 'encoder.encoder.5.0.bn1.bias', 'encoder.encoder.5.0.bn1.running_mean', 'encoder.encoder.5.0.bn1.running_var', 'encoder.encoder.5.0.bn1.num_batches_tracked', 'encoder.encoder.5.0.conv1.weight', 'encoder.encoder.5.0.conv1.bias', 'encoder.encoder.5.0.bn2.weight', 'encoder.encoder.5.0.bn2.bias', 'encoder.encoder.5.0.bn2.running_mean', 'encoder.encoder.5.0.bn2.running_var', 'encoder.encoder.5.0.bn2.num_batches_tracked', 'encoder.encoder.5.0.conv2.weight', 'encoder.encoder.5.0.conv2.bias', 'encoder.GAT_layer_S.att_weight', 'encoder.GAT_layer_S.att_proj.weight', 'encoder.GAT_layer_S.att_proj.bias', 'encoder.GAT_layer_S.proj_with_att.weight', 'encoder.GAT_layer_S.proj_with_att.bias', 'encoder.GAT_layer_S.proj_without_att.weight', 'encoder.GAT_layer_S.proj_without_att.bias', 'encoder.GAT_layer_S.bn.weight', 'encoder.GAT_layer_S.bn.bias', 'encoder.GAT_layer_S.bn.running_mean', 'encoder.GAT_layer_S.bn.running_var', 'encoder.GAT_layer_S.bn.num_batches_tracked', 'encoder.GAT_layer_T.att_weight', 'encoder.GAT_layer_T.att_proj.weight', 'encoder.GAT_layer_T.att_proj.bias', 'encoder.GAT_layer_T.proj_with_att.weight', 'encoder.GAT_layer_T.proj_with_att.bias', 'encoder.GAT_layer_T.proj_without_att.weight', 'encoder.GAT_layer_T.proj_without_att.bias', 'encoder.GAT_layer_T.bn.weight', 'encoder.GAT_layer_T.bn.bias', 'encoder.GAT_layer_T.bn.running_mean', 'encoder.GAT_layer_T.bn.running_var', 'encoder.GAT_layer_T.bn.num_batches_tracked', 'encoder.HtrgGAT_layer_ST11.att_weight11', 'encoder.HtrgGAT_layer_ST11.att_weight22', 'encoder.HtrgGAT_layer_ST11.att_weight12', 'encoder.HtrgGAT_layer_ST11.att_weightM', 'encoder.HtrgGAT_layer_ST11.proj_type1.weight', 'encoder.HtrgGAT_layer_ST11.proj_type1.bias', 'encoder.HtrgGAT_layer_ST11.proj_type2.weight', 'encoder.HtrgGAT_layer_ST11.proj_type2.bias', 'encoder.HtrgGAT_layer_ST11.att_proj.weight', 'encoder.HtrgGAT_layer_ST11.att_proj.bias', 'encoder.HtrgGAT_layer_ST11.att_projM.weight', 'encoder.HtrgGAT_layer_ST11.att_projM.bias', 'encoder.HtrgGAT_layer_ST11.proj_with_att.weight', 'encoder.HtrgGAT_layer_ST11.proj_with_att.bias', 'encoder.HtrgGAT_layer_ST11.proj_without_att.weight', 'encoder.HtrgGAT_layer_ST11.proj_without_att.bias', 'encoder.HtrgGAT_layer_ST11.proj_with_attM.weight', 'encoder.HtrgGAT_layer_ST11.proj_with_attM.bias', 'encoder.HtrgGAT_layer_ST11.proj_without_attM.weight', 'encoder.HtrgGAT_layer_ST11.proj_without_attM.bias', 'encoder.HtrgGAT_layer_ST11.bn.weight', 'encoder.HtrgGAT_layer_ST11.bn.bias', 'encoder.HtrgGAT_layer_ST11.bn.running_mean', 'encoder.HtrgGAT_layer_ST11.bn.running_var', 'encoder.HtrgGAT_layer_ST11.bn.num_batches_tracked', 'encoder.HtrgGAT_layer_ST12.att_weight11', 'encoder.HtrgGAT_layer_ST12.att_weight22', 'encoder.HtrgGAT_layer_ST12.att_weight12', 'encoder.HtrgGAT_layer_ST12.att_weightM', 'encoder.HtrgGAT_layer_ST12.proj_type1.weight', 'encoder.HtrgGAT_layer_ST12.proj_type1.bias', 'encoder.HtrgGAT_layer_ST12.proj_type2.weight', 'encoder.HtrgGAT_layer_ST12.proj_type2.bias', 'encoder.HtrgGAT_layer_ST12.att_proj.weight', 'encoder.HtrgGAT_layer_ST12.att_proj.bias', 'encoder.HtrgGAT_layer_ST12.att_projM.weight', 'encoder.HtrgGAT_layer_ST12.att_projM.bias', 'encoder.HtrgGAT_layer_ST12.proj_with_att.weight', 'encoder.HtrgGAT_layer_ST12.proj_with_att.bias', 'encoder.HtrgGAT_layer_ST12.proj_without_att.weight', 'encoder.HtrgGAT_layer_ST12.proj_without_att.bias', 'encoder.HtrgGAT_layer_ST12.proj_with_attM.weight', 'encoder.HtrgGAT_layer_ST12.proj_with_attM.bias', 'encoder.HtrgGAT_layer_ST12.proj_without_attM.weight', 'encoder.HtrgGAT_layer_ST12.proj_without_attM.bias', 'encoder.HtrgGAT_layer_ST12.bn.weight', 'encoder.HtrgGAT_layer_ST12.bn.bias', 'encoder.HtrgGAT_layer_ST12.bn.running_mean', 'encoder.HtrgGAT_layer_ST12.bn.running_var', 'encoder.HtrgGAT_layer_ST12.bn.num_batches_tracked', 'encoder.HtrgGAT_layer_ST21.att_weight11', 'encoder.HtrgGAT_layer_ST21.att_weight22', 'encoder.HtrgGAT_layer_ST21.att_weight12', 'encoder.HtrgGAT_layer_ST21.att_weightM', 'encoder.HtrgGAT_layer_ST21.proj_type1.weight', 'encoder.HtrgGAT_layer_ST21.proj_type1.bias', 'encoder.HtrgGAT_layer_ST21.proj_type2.weight', 'encoder.HtrgGAT_layer_ST21.proj_type2.bias', 'encoder.HtrgGAT_layer_ST21.att_proj.weight', 'encoder.HtrgGAT_layer_ST21.att_proj.bias', 'encoder.HtrgGAT_layer_ST21.att_projM.weight', 'encoder.HtrgGAT_layer_ST21.att_projM.bias', 'encoder.HtrgGAT_layer_ST21.proj_with_att.weight', 'encoder.HtrgGAT_layer_ST21.proj_with_att.bias', 'encoder.HtrgGAT_layer_ST21.proj_without_att.weight', 'encoder.HtrgGAT_layer_ST21.proj_without_att.bias', 'encoder.HtrgGAT_layer_ST21.proj_with_attM.weight', 'encoder.HtrgGAT_layer_ST21.proj_with_attM.bias', 'encoder.HtrgGAT_layer_ST21.proj_without_attM.weight', 'encoder.HtrgGAT_layer_ST21.proj_without_attM.bias', 'encoder.HtrgGAT_layer_ST21.bn.weight', 'encoder.HtrgGAT_layer_ST21.bn.bias', 'encoder.HtrgGAT_layer_ST21.bn.running_mean', 'encoder.HtrgGAT_layer_ST21.bn.running_var', 'encoder.HtrgGAT_layer_ST21.bn.num_batches_tracked', 'encoder.HtrgGAT_layer_ST22.att_weight11', 'encoder.HtrgGAT_layer_ST22.att_weight22', 'encoder.HtrgGAT_layer_ST22.att_weight12', 'encoder.HtrgGAT_layer_ST22.att_weightM', 'encoder.HtrgGAT_layer_ST22.proj_type1.weight', 'encoder.HtrgGAT_layer_ST22.proj_type1.bias', 'encoder.HtrgGAT_layer_ST22.proj_type2.weight', 'encoder.HtrgGAT_layer_ST22.proj_type2.bias', 'encoder.HtrgGAT_layer_ST22.att_proj.weight', 'encoder.HtrgGAT_layer_ST22.att_proj.bias', 'encoder.HtrgGAT_layer_ST22.att_projM.weight', 'encoder.HtrgGAT_layer_ST22.att_projM.bias', 'encoder.HtrgGAT_layer_ST22.proj_with_att.weight', 'encoder.HtrgGAT_layer_ST22.proj_with_att.bias', 'encoder.HtrgGAT_layer_ST22.proj_without_att.weight', 'encoder.HtrgGAT_layer_ST22.proj_without_att.bias', 'encoder.HtrgGAT_layer_ST22.proj_with_attM.weight', 'encoder.HtrgGAT_layer_ST22.proj_with_attM.bias', 'encoder.HtrgGAT_layer_ST22.proj_without_attM.weight', 'encoder.HtrgGAT_layer_ST22.proj_without_attM.bias', 'encoder.HtrgGAT_layer_ST22.bn.weight', 'encoder.HtrgGAT_layer_ST22.bn.bias', 'encoder.HtrgGAT_layer_ST22.bn.running_mean', 'encoder.HtrgGAT_layer_ST22.bn.running_var', 'encoder.HtrgGAT_layer_ST22.bn.num_batches_tracked', 'encoder.pool_S.proj.weight', 'encoder.pool_S.proj.bias', 'encoder.pool_T.proj.weight', 'encoder.pool_T.proj.bias', 'encoder.pool_hS1.proj.weight', 'encoder.pool_hS1.proj.bias', 'encoder.pool_hT1.proj.weight', 'encoder.pool_hT1.proj.bias', 'encoder.pool_hS2.proj.weight', 'encoder.pool_hS2.proj.bias', 'encoder.pool_hT2.proj.weight', 'encoder.pool_hT2.proj.bias', 'encoder.out_layer.weight', 'encoder.out_layer.bias', 'fc.weight', 'fc.bias']\n",
            "CLAD model loaded and ready.\n",
            "Encoder output shape: (2, 1024)\n",
            "Probed feature dim D = 1024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# Build a minimal Dataset and train_loader for pretraining.\n",
        "# It expects a train split with two folders: .../train/real and .../train/fake\n",
        "# It returns (waveform[T], label) where label: 1=real, 0=fake.\n",
        "# ------------------------------------------------------------\n",
        "import os\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchaudio\n",
        "\n",
        "SR = 16000\n",
        "N_SAMPLES = 64600\n",
        "BATCH_SIZE = 24\n",
        "NUM_WORKERS = 2\n",
        "REAL_CLASS_ID = 1  # keep consistent with your pretraining cell\n",
        "\n",
        "# Try common train-split locations; pick the first that exists\n",
        "_CANDIDATE_TRAIN_DIRS = [\n",
        "    \"/content/my_audio_split/train\",\n",
        "    \"/content/my_audio/train\",\n",
        "    \"/content/my_audio_splits/train\",\n",
        "]\n",
        "TRAIN_DIR = None\n",
        "for _p in _CANDIDATE_TRAIN_DIRS:\n",
        "    if Path(_p).exists():\n",
        "        TRAIN_DIR = _p\n",
        "        break\n",
        "if TRAIN_DIR is None:\n",
        "    raise FileNotFoundError(\n",
        "        \"Could not find a train split. Expected one of: \" + \", \".join(_CANDIDATE_TRAIN_DIRS)\n",
        "    )\n",
        "\n",
        "CLASS_TO_ID = {\"fake\": 0, \"real\": 1}  # adjust only if your naming differs\n",
        "\n",
        "def _pad_trim(x: torch.Tensor, target_len: int) -> torch.Tensor:\n",
        "    # x: [T]\n",
        "    T = x.size(0)\n",
        "    if T == target_len:\n",
        "        return x\n",
        "    if T < target_len:\n",
        "        pad = target_len - T\n",
        "        return torch.nn.functional.pad(x, (0, pad))\n",
        "    return x[:target_len]\n",
        "\n",
        "class AudioFolderDataset(Dataset):\n",
        "    \"\"\"Walks train_dir/{real,fake}/**/*.wav and yields (waveform[T], label).\"\"\"\n",
        "    def __init__(self, train_dir: str, sr: int = SR, n_samples: int = N_SAMPLES):\n",
        "        self.train_dir = Path(train_dir)\n",
        "        self.sr = sr\n",
        "        self.n_samples = n_samples\n",
        "        self.items = []  # list of (path, label)\n",
        "        for cls in CLASS_TO_ID.keys():\n",
        "            cls_dir = self.train_dir / cls\n",
        "            if not cls_dir.exists():\n",
        "                continue\n",
        "            for p in cls_dir.rglob(\"*.wav\"):\n",
        "                self.items.append((p, CLASS_TO_ID[cls]))\n",
        "        if len(self.items) == 0:\n",
        "            raise RuntimeError(f\"No .wav files found under {self.train_dir}/(real|fake)\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        path, label = self.items[idx]\n",
        "        # load audio as mono at SR\n",
        "        wav, sr = torchaudio.load(str(path))  # [C,T]\n",
        "        if wav.size(0) > 1:\n",
        "            wav = wav.mean(dim=0, keepdim=True)  # to mono\n",
        "        if sr != self.sr:\n",
        "            wav = torchaudio.functional.resample(wav, sr, self.sr)\n",
        "        wav = wav.squeeze(0)  # [T]\n",
        "        wav = _pad_trim(wav, self.n_samples)   # [N_SAMPLES]\n",
        "        wav = wav.clamp(-1.0, 1.0).float()\n",
        "        return wav, int(label)\n",
        "\n",
        "# Build dataset and loader only if missing\n",
        "if 'train_loader' not in globals():\n",
        "    train_ds = AudioFolderDataset(TRAIN_DIR, sr=SR, n_samples=N_SAMPLES)\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True,\n",
        "        drop_last=True,      # helpful for contrastive batches\n",
        "    )\n",
        "    print(f\"✅ train_loader is ready from: {TRAIN_DIR}\")\n",
        "    print(f\"   samples: {len(train_ds)} | batch_size: {BATCH_SIZE}\")\n",
        "else:\n",
        "    print(\"ℹ️ train_loader already defined; skipping rebuild.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiUOY7-W2oWA",
        "outputId": "53a624f0-338e-45cb-a9c5-4c250b4c30ad"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ train_loader is ready from: /content/my_audio_split/train\n",
            "   samples: 1240 | batch_size: 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# CLAD-style pretraining cell (MoCo + Length Loss) — self-contained\n",
        "# ------------------------------------------------\n",
        "# What this cell does (short):\n",
        "# 1) Builds a momentum contrast pretraining loop around your existing encoders:\n",
        "#    - encoder_q: updated by optimizer\n",
        "#    - encoder_k: updated by momentum (no gradients)\n",
        "# 2) Generates two augmented \"views\" per waveform using audio augmentations.\n",
        "# 3) Computes InfoNCE contrastive loss against a memory queue of negatives.\n",
        "# 4) Adds the CLAD \"Length Loss\" term to shape embedding norms w.r.t. real/fake labels.\n",
        "# 5) Trains for EPOCHS, saves a checkpoint, and keeps dimensions consistent (D=1024).\n",
        "#\n",
        "# Assumptions / Integration:\n",
        "# - Variables already defined earlier: `encoder_q`, `encoder_k`, `device`, `train_loader`.\n",
        "# - Input shape is either [B,T] or [B,1,T]. This cell handles both.\n",
        "# - Output features have dim D=1024 (matches your earlier forward check).\n",
        "# - Labels in `train_loader` are integers; set REAL_CLASS_ID below to match your dataset.\n",
        "#\n",
        "# If you later want to switch to end-to-end supervised training, you can skip this cell.\n",
        "# ================================================================\n",
        "\n",
        "import math\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "# ----------------------------\n",
        "# Hyperparameters (match CLAD paper defaults where applicable)\n",
        "# ----------------------------\n",
        "SR                = 16000\n",
        "N_SAMPLES         = 64600          # keep consistent with your preprocessing\n",
        "D                 = 1024           # encoder output dim (verified by your forward)\n",
        "QUEUE_SIZE        = 6144           # memory queue length\n",
        "TAU               = 0.07           # temperature for InfoNCE\n",
        "MOMENTUM          = 0.999          # key encoder EMA\n",
        "LR                = 5e-4           # Adam learning rate for encoder_q\n",
        "WEIGHT_DECAY      = 1e-4\n",
        "EPOCHS            = 150            # adjust if you want shorter runs first\n",
        "REAL_CLASS_ID     = 1              # change if your dataset encodes REAL differently\n",
        "\n",
        "# Length Loss (CLAD)\n",
        "ALPHA             = 2.0            # weight for length loss\n",
        "MARGIN            = 4.0\n",
        "W_REAL            = 9.0            # stronger pull for real embeddings\n",
        "\n",
        "# Checkpoints\n",
        "CKPT_DIR          = \"/content/checkpoints\"\n",
        "CKPT_PATH         = f\"{CKPT_DIR}/clad_pretrain_moco_lenloss.pth\"\n",
        "\n",
        "# ----------------------------\n",
        "# Utility: simple audio augmentations (torch-only; lightweight)\n",
        "# ----------------------------\n",
        "class AudioAug:\n",
        "    \"\"\"Returns two independently augmented views for each waveform in a batch.\"\"\"\n",
        "    def __init__(self, sr=SR, n_samples=N_SAMPLES):\n",
        "        self.sr = sr\n",
        "        self.n_samples = n_samples\n",
        "\n",
        "    def random_gain(self, x, min_db=-6.0, max_db=6.0):\n",
        "        # x: [B,T]\n",
        "        g_db = (torch.rand(x.size(0), device=x.device) * (max_db - min_db) + min_db)\n",
        "        g = 10.0 ** (g_db / 20.0)\n",
        "        return x * g.unsqueeze(1)\n",
        "\n",
        "    def random_time_shift(self, x, max_shift=int(0.05 * SR)):\n",
        "        # circular shift in time\n",
        "        if max_shift <= 0:\n",
        "            return x\n",
        "        B, T = x.shape\n",
        "        shift = torch.randint(low=-max_shift, high=max_shift + 1, size=(B,), device=x.device)\n",
        "        idx = (torch.arange(T, device=x.device).unsqueeze(0) - shift.unsqueeze(1)) % T\n",
        "        return torch.gather(x, dim=1, index=idx)\n",
        "\n",
        "    def random_fade(self, x, p=0.5):\n",
        "        # linear fade-in/out with random length\n",
        "        if torch.rand(1).item() > p:\n",
        "            return x\n",
        "        B, T = x.shape\n",
        "        fade_len = torch.randint(int(0.01 * T), int(0.1 * T) + 1, (B,), device=x.device)\n",
        "        out = x.clone()\n",
        "        for b in range(B):\n",
        "            if torch.rand(1).item() < 0.5:\n",
        "                # fade-in\n",
        "                L = fade_len[b].item()\n",
        "                ramp = torch.linspace(0, 1, L, device=x.device)\n",
        "                out[b, :L] = out[b, :L] * ramp\n",
        "            else:\n",
        "                # fade-out\n",
        "                L = fade_len[b].item()\n",
        "                ramp = torch.linspace(1, 0, L, device=x.device)\n",
        "                out[b, -L:] = out[b, -L:] * ramp\n",
        "        return out\n",
        "\n",
        "    def random_white_noise(self, x, snr_db_min=10.0, snr_db_max=30.0, p=0.8):\n",
        "        # additive white noise at random SNR\n",
        "        if torch.rand(1).item() > p:\n",
        "            return x\n",
        "        B, T = x.shape\n",
        "        power = (x**2).mean(dim=1, keepdim=True).clamp_min(1e-9)\n",
        "        snr_db = (torch.rand(B, 1, device=x.device) * (snr_db_max - snr_db_min) + snr_db_min)\n",
        "        snr = 10.0 ** (snr_db / 10.0)\n",
        "        noise_power = (power / snr).clamp_min(1e-12)\n",
        "        noise = torch.randn_like(x) * torch.sqrt(noise_power)\n",
        "        return (x + noise).clamp_(-1.0, 1.0)\n",
        "\n",
        "    def _one_view(self, x):\n",
        "        # compose a lightweight chain; extend as needed\n",
        "        x = self.random_gain(x)\n",
        "        x = self.random_time_shift(x)\n",
        "        x = self.random_fade(x, p=0.5)\n",
        "        x = self.random_white_noise(x, p=0.8)\n",
        "        return x\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # return two independent augmented views\n",
        "        v1 = self._one_view(x)\n",
        "        v2 = self._one_view(x.clone())\n",
        "        return v1, v2\n",
        "\n",
        "AUG = AudioAug()\n",
        "\n",
        "# ----------------------------\n",
        "# Memory queue for negatives (MoCo style)\n",
        "# ----------------------------\n",
        "class MemoryQueue(nn.Module):\n",
        "    def __init__(self, dim=D, K=QUEUE_SIZE, device=\"cpu\"):\n",
        "        super().__init__()\n",
        "        self.K = K\n",
        "        self.register_buffer(\"queue\", F.normalize(torch.randn(K, dim, device=device), dim=1))\n",
        "        self.register_buffer(\"ptr\", torch.zeros(1, dtype=torch.long, device=device))\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def enqueue(self, keys):  # keys: [B,D], already normalized\n",
        "        B = keys.size(0)\n",
        "        K = self.K\n",
        "        ptr = int(self.ptr.item())\n",
        "        if ptr + B <= K:\n",
        "            self.queue[ptr:ptr+B] = keys\n",
        "            ptr = (ptr + B) % K\n",
        "        else:\n",
        "            n1 = K - ptr\n",
        "            self.queue[ptr:] = keys[:n1]\n",
        "            self.queue[:B-n1] = keys[n1:]\n",
        "            ptr = (B - n1) % K\n",
        "        self.ptr[0] = ptr\n",
        "\n",
        "    def get(self):\n",
        "        return self.queue  # [K,D]\n",
        "\n",
        "QUEUE = MemoryQueue(dim=D, K=QUEUE_SIZE, device=device).to(device)\n",
        "\n",
        "# ----------------------------\n",
        "# Optimizer + scheduler for encoder_q only (encoder_k is momentum-updated)\n",
        "# ----------------------------\n",
        "opt = torch.optim.Adam(encoder_q.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "sched = CosineAnnealingLR(opt, T_max=EPOCHS, eta_min=LR * 0.1)\n",
        "\n",
        "# Ensure key encoder starts as a copy of query encoder; no gradients for encoder_k\n",
        "encoder_q.to(device).train()\n",
        "encoder_k.to(device).eval()\n",
        "for p in encoder_k.parameters():\n",
        "    p.requires_grad_(False)\n",
        "with torch.no_grad():\n",
        "    for p_k, p_q in zip(encoder_k.parameters(), encoder_q.parameters()):\n",
        "        p_k.data.copy_(p_q.data)\n",
        "\n",
        "# ----------------------------\n",
        "# Helper: forward encoder and reduce features to [B,D]\n",
        "# ----------------------------\n",
        "def encode_reduce(encoder, x):\n",
        "    # x can be [B,T] or [B,1,T] — convert to [B,T]\n",
        "    if x.dim() == 3 and x.size(1) == 1:\n",
        "        x = x[:, 0, :]\n",
        "    # forward\n",
        "    z = encoder(x)  # expected [B,D] or [B,D,L,...]\n",
        "    if z.dim() == 3:           # [B,D,L] -> mean over L\n",
        "        z = z.mean(dim=2)\n",
        "    elif z.dim() > 3:          # [B,D,H,W,...] -> mean over extras\n",
        "        z = z.mean(dim=tuple(range(2, z.dim())))\n",
        "    return z  # [B,D]\n",
        "\n",
        "# ----------------------------\n",
        "# Training loop\n",
        "# ----------------------------\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    encoder_q.train()\n",
        "    total_loss, total_contrast, total_len = 0.0, 0.0, 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    for wavs, labels in train_loader:\n",
        "        # Move to device and ensure float + proper shape\n",
        "        x = wavs.to(device).float()      # [B, T] or [B, 1, T]\n",
        "        y = labels.to(device).long()     # [B]\n",
        "\n",
        "        # Make sure each sample has length N_SAMPLES (guard; your preprocess already does this)\n",
        "        if x.dim() == 3 and x.size(-1) != N_SAMPLES:\n",
        "            T = x.size(-1)\n",
        "            if T < N_SAMPLES:\n",
        "                x = F.pad(x, (0, N_SAMPLES - T))\n",
        "            else:\n",
        "                x = x[..., :N_SAMPLES]\n",
        "        elif x.dim() == 2 and x.size(-1) != N_SAMPLES:\n",
        "            T = x.size(-1)\n",
        "            if T < N_SAMPLES:\n",
        "                x = F.pad(x, (0, N_SAMPLES - T))\n",
        "            else:\n",
        "                x = x[:, :N_SAMPLES]\n",
        "\n",
        "        # Create two augmented views from [B,T]\n",
        "        xb = x[:, 0, :] if (x.dim() == 3 and x.size(1) == 1) else (x if x.dim() == 2 else x.squeeze(1))\n",
        "        v1, v2 = AUG(xb)  # both are [B,T]\n",
        "\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=torch.cuda.is_available()):\n",
        "            # Query and Key embeddings\n",
        "            q = encode_reduce(encoder_q, v1)     # [B,D]\n",
        "            with torch.no_grad():\n",
        "                k = encode_reduce(encoder_k, v2) # [B,D]\n",
        "\n",
        "            # Normalize for InfoNCE\n",
        "            q = F.normalize(q, dim=1)\n",
        "            k = F.normalize(k, dim=1)\n",
        "\n",
        "            # Positive logits: q·k+; Negative logits: q·Q (queue)\n",
        "            l_pos = torch.einsum('bd,bd->b', q, k).unsqueeze(1) / TAU           # [B,1]\n",
        "            q_neg = QUEUE.get()                                                 # [K,D]\n",
        "            l_neg = (q @ q_neg.T) / TAU                                         # [B,K]\n",
        "            logits = torch.cat([l_pos, l_neg], dim=1)                           # [B,1+K]\n",
        "            targets = torch.zeros(q.size(0), dtype=torch.long, device=device)   # positives at index 0\n",
        "            L_contrast = F.cross_entropy(logits, targets)\n",
        "\n",
        "            # Length Loss: encourage larger ||z|| for real, and below margin for fake\n",
        "            # Use non-normalized features for length (recompute from encoder_q output without L2 norm)\n",
        "            z_nonorm = encode_reduce(encoder_q, v1.detach())\n",
        "            z_norms  = torch.norm(z_nonorm, p=2, dim=1)       # [B]\n",
        "            y_real   = (y == REAL_CLASS_ID).float()           # [B] in {0,1}\n",
        "            L_len_real = y_real * W_REAL * z_norms\n",
        "            L_len_fake = (1.0 - y_real) * torch.clamp(MARGIN - z_norms, min=0.0)\n",
        "            L_len = (L_len_real + L_len_fake).mean()\n",
        "\n",
        "            loss = L_contrast + ALPHA * L_len\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(opt)\n",
        "        scaler.update()\n",
        "\n",
        "        # Momentum update for encoder_k (EMA)\n",
        "        with torch.no_grad():\n",
        "            for p_k, p_q in zip(encoder_k.parameters(), encoder_q.parameters()):\n",
        "                p_k.data.mul_(MOMENTUM).add_(p_q.data, alpha=1.0 - MOMENTUM)\n",
        "\n",
        "        # Update the memory queue with current keys\n",
        "        with torch.no_grad():\n",
        "            QUEUE.enqueue(k)\n",
        "\n",
        "        total_loss     += loss.item()\n",
        "        total_contrast += L_contrast.item()\n",
        "        total_len      += L_len.item()\n",
        "        n_batches      += 1\n",
        "\n",
        "    sched.step()\n",
        "    avg_loss = total_loss / max(n_batches, 1)\n",
        "    avg_cl   = total_contrast / max(n_batches, 1)\n",
        "    avg_ll   = total_len / max(n_batches, 1)\n",
        "    print(f\"[Pretrain][Epoch {epoch+1:03d}/{EPOCHS}] \"\n",
        "          f\"loss={avg_loss:.4f} (InfoNCE={avg_cl:.4f} + {ALPHA}*Len={avg_ll:.4f}), \"\n",
        "          f\"tau={TAU}, m={MOMENTUM}, lr={sched.get_last_lr()[0]:.2e}\")\n",
        "\n",
        "# ----------------------------\n",
        "# Save encoder_q (and optionally encoder_k) after pretraining\n",
        "# ----------------------------\n",
        "os.makedirs(CKPT_DIR, exist_ok=True)\n",
        "torch.save(\n",
        "    {\n",
        "        \"encoder_q\": encoder_q.state_dict(),\n",
        "        \"encoder_k\": encoder_k.state_dict(),\n",
        "        \"dim\": D,\n",
        "        \"queue_size\": QUEUE_SIZE,\n",
        "        \"tau\": TAU,\n",
        "        \"momentum\": MOMENTUM,\n",
        "        \"alpha\": ALPHA,\n",
        "        \"margin\": MARGIN,\n",
        "        \"w_real\": W_REAL,\n",
        "        \"sr\": SR,\n",
        "        \"n_samples\": N_SAMPLES,\n",
        "        \"real_class_id\": REAL_CLASS_ID,\n",
        "        \"epochs\": EPOCHS,\n",
        "    },\n",
        "    CKPT_PATH\n",
        ")\n",
        "print(f\"✅ CLAD-style pretraining finished. Checkpoint saved to: {CKPT_PATH}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "id": "ZKNDuoV21gUW",
        "outputId": "c565fd39-c8e8-4409-a010-88bd5fd3c5e3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Pretrain][Epoch 001/150] loss=13.6883 (InfoNCE=6.2316 + 2.0*Len=3.7284), tau=0.07, m=0.999, lr=5.00e-04\n",
            "[Pretrain][Epoch 002/150] loss=8.8532 (InfoNCE=5.6195 + 2.0*Len=1.6169), tau=0.07, m=0.999, lr=5.00e-04\n",
            "[Pretrain][Epoch 003/150] loss=8.6509 (InfoNCE=5.8139 + 2.0*Len=1.4185), tau=0.07, m=0.999, lr=5.00e-04\n",
            "[Pretrain][Epoch 004/150] loss=8.8190 (InfoNCE=6.2575 + 2.0*Len=1.2808), tau=0.07, m=0.999, lr=4.99e-04\n",
            "[Pretrain][Epoch 005/150] loss=7.9347 (InfoNCE=6.5444 + 2.0*Len=0.6952), tau=0.07, m=0.999, lr=4.99e-04\n",
            "[Pretrain][Epoch 006/150] loss=9.0248 (InfoNCE=6.8381 + 2.0*Len=1.0933), tau=0.07, m=0.999, lr=4.98e-04\n",
            "[Pretrain][Epoch 007/150] loss=8.2718 (InfoNCE=6.7855 + 2.0*Len=0.7432), tau=0.07, m=0.999, lr=4.98e-04\n",
            "[Pretrain][Epoch 008/150] loss=7.7041 (InfoNCE=6.7875 + 2.0*Len=0.4583), tau=0.07, m=0.999, lr=4.97e-04\n",
            "[Pretrain][Epoch 009/150] loss=7.8693 (InfoNCE=6.8268 + 2.0*Len=0.5212), tau=0.07, m=0.999, lr=4.96e-04\n",
            "[Pretrain][Epoch 010/150] loss=9.1937 (InfoNCE=7.1404 + 2.0*Len=1.0267), tau=0.07, m=0.999, lr=4.95e-04\n",
            "[Pretrain][Epoch 011/150] loss=9.4275 (InfoNCE=7.1106 + 2.0*Len=1.1585), tau=0.07, m=0.999, lr=4.94e-04\n",
            "[Pretrain][Epoch 012/150] loss=7.7381 (InfoNCE=6.8268 + 2.0*Len=0.4556), tau=0.07, m=0.999, lr=4.93e-04\n",
            "[Pretrain][Epoch 013/150] loss=7.5596 (InfoNCE=6.8226 + 2.0*Len=0.3685), tau=0.07, m=0.999, lr=4.92e-04\n",
            "[Pretrain][Epoch 014/150] loss=7.6671 (InfoNCE=6.8871 + 2.0*Len=0.3900), tau=0.07, m=0.999, lr=4.90e-04\n",
            "[Pretrain][Epoch 015/150] loss=7.5556 (InfoNCE=6.8760 + 2.0*Len=0.3398), tau=0.07, m=0.999, lr=4.89e-04\n",
            "[Pretrain][Epoch 016/150] loss=7.4872 (InfoNCE=6.8378 + 2.0*Len=0.3247), tau=0.07, m=0.999, lr=4.87e-04\n",
            "[Pretrain][Epoch 017/150] loss=7.5316 (InfoNCE=6.7966 + 2.0*Len=0.3675), tau=0.07, m=0.999, lr=4.86e-04\n",
            "[Pretrain][Epoch 018/150] loss=7.2085 (InfoNCE=6.6499 + 2.0*Len=0.2793), tau=0.07, m=0.999, lr=4.84e-04\n",
            "[Pretrain][Epoch 019/150] loss=6.9269 (InfoNCE=6.4491 + 2.0*Len=0.2389), tau=0.07, m=0.999, lr=4.82e-04\n",
            "[Pretrain][Epoch 020/150] loss=6.6280 (InfoNCE=6.2394 + 2.0*Len=0.1943), tau=0.07, m=0.999, lr=4.81e-04\n",
            "[Pretrain][Epoch 021/150] loss=6.4583 (InfoNCE=6.1055 + 2.0*Len=0.1764), tau=0.07, m=0.999, lr=4.79e-04\n",
            "[Pretrain][Epoch 022/150] loss=6.3250 (InfoNCE=5.9262 + 2.0*Len=0.1994), tau=0.07, m=0.999, lr=4.77e-04\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1508834517.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# [B,D]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                 \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [B,D]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;31m# Normalize for InfoNCE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1508834517.py\u001b[0m in \u001b[0;36mencode_reduce\u001b[0;34m(encoder, x)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;31m# forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# expected [B,D] or [B,D,L,...]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m           \u001b[0;31m# [B,D,L] -> mean over L\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/CLAD/Model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0my3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# torch.Size([batch, filter])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0my3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_attention3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0my3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# torch.Size([batch, filter, 1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my3\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my3\u001b[0m \u001b[0;31m# (batch, filter, time) x (batch, filter, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1694\u001b[0m     \u001b[0;31m# See full discussion on the problems with returning `Union` here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1695\u001b[0m     \u001b[0;31m# https://github.com/microsoft/pyright/issues/4213\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1696\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1697\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_parameters'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1698\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================== Single-Cell: Flexible checkpoint loading (no other code changes) ==================\n",
        "import torch\n",
        "from collections import OrderedDict\n",
        "from pathlib import Path\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ---- choose ONE of the modes below (no other code changes needed) ----\n",
        "# \"none\"                -> לא טוען כלום (אימון מאפס)\n",
        "# \"moco_full\"           -> מנסה לטעון state_dict מלא של המודל שלך (MoCo_v2 + RawNet) שנשמר בעבר ממך\n",
        "# \"rawnet_encoder_only\" -> מנסה לטעון רק את המקודד encoder_q מ-ckpt חיצוני שדומה ל-RawNet (לא MoCo),\n",
        "#                          עם מיפוי שמות פשוט. אם לא יתאים — מדלג.\n",
        "CKPT_MODE = \"none\"              # ← שנה ל-\"moco_full\" או \"rawnet_encoder_only\" אם יש לך קובץ מתאים\n",
        "CKPT_PATH = \"checkpoints/my_moco_rawnet_epXX.pth\"  # לשימוש עם \"moco_full\" (state_dict ששמרת מהמודל הזה)\n",
        "RAWNET_CKPT_PATH = \"checkpoints/rawnet_encoder_only.pth\"  # לשימוש עם \"rawnet_encoder_only\" (אם יש)\n",
        "\n",
        "MIN_MATCH_RATIO = 0.80  # פחות מזה = מדלגים אוטומטית\n",
        "\n",
        "def _strip_prefix(d, prefix=\"module.\"):\n",
        "    return { (k[len(prefix):] if k.startswith(prefix) else k): v for k,v in d.items() }\n",
        "\n",
        "def _load_state(path):\n",
        "    obj = torch.load(path, map_location=\"cpu\")\n",
        "    return obj.get(\"state_dict\", obj)\n",
        "\n",
        "def _try_load_full_moco(model, path):\n",
        "    \"\"\"Expect a state_dict שנשמר מהמול מודל שלך (אותו MoCo_v2 + RawNet + projection heads).\"\"\"\n",
        "    state = _load_state(path)\n",
        "    state = _strip_prefix(state, \"module.\")\n",
        "    msd = model.state_dict()\n",
        "    new_sd = OrderedDict(msd)\n",
        "    matched = skipped = 0\n",
        "    for k, v in state.items():\n",
        "        if k in new_sd and new_sd[k].shape == v.shape:\n",
        "            new_sd[k] = v; matched += 1\n",
        "        else:\n",
        "            skipped += 1\n",
        "    model.load_state_dict(new_sd)\n",
        "    ratio = matched / (matched + skipped) if (matched + skipped) else 0.0\n",
        "    print(f\"[CKPT full] matched={matched}, skipped={skipped}, ratio={ratio:.2%}\")\n",
        "    return ratio\n",
        "\n",
        "def _try_load_rawnet_encoder_only(model, path):\n",
        "    \"\"\"\n",
        "    Load only encoder_q.* from a rawnet-like checkpoint (NOT MoCo).\n",
        "    מיפוי בסיסי: 'encoder.' -> 'encoder_q.'; אם שמות אחרים – יתאים רק מה שבאמת זהה בצורה.\n",
        "    \"\"\"\n",
        "    state = _load_state(path)\n",
        "    state = _strip_prefix(state, \"module.\")\n",
        "    mapped = OrderedDict()\n",
        "    for k, v in state.items():\n",
        "        nk = k\n",
        "        if nk.startswith(\"encoder.\"):                # נפוץ בצ'קפוינטים של מקודד בלבד\n",
        "            nk = \"encoder_q.\" + nk[len(\"encoder.\"):]\n",
        "        elif nk.startswith(\"rawnet.\") or nk.startswith(\"model.encoder.\"):\n",
        "            # הוספת כללים לפי הצורך; נשאיר שמרני כברירת מחדל\n",
        "            nk = \"encoder_q.\" + nk.split(\".\", 1)[-1]\n",
        "        mapped[nk] = v\n",
        "\n",
        "    msd = model.state_dict()\n",
        "    new_sd = OrderedDict(msd)\n",
        "    matched = skipped = 0\n",
        "    for k, v in mapped.items():\n",
        "        if k.startswith(\"encoder_q.\") and (k in new_sd) and (new_sd[k].shape == v.shape):\n",
        "            new_sd[k] = v; matched += 1\n",
        "        else:\n",
        "            skipped += 1\n",
        "\n",
        "    model.load_state_dict(new_sd)\n",
        "    ratio = matched / (matched + skipped) if (matched + skipped) else 0.0\n",
        "    print(f\"[CKPT rawnet-encoder] matched={matched}, skipped={skipped}, ratio={ratio:.2%}\")\n",
        "    return ratio\n",
        "\n",
        "def _copy_q_to_k(model):\n",
        "    sd = model.state_dict()\n",
        "    updates = {}\n",
        "    for k, v in sd.items():\n",
        "        if k.startswith(\"encoder_q.\"):\n",
        "            twin = \"encoder_k.\" + k[len(\"encoder_q.\"):]\n",
        "            if twin in sd and sd[twin].shape == v.shape:\n",
        "                updates[twin] = v.clone()\n",
        "    sd.update(updates)\n",
        "    model.load_state_dict(sd)\n",
        "    print(f\"Synced encoder_q → encoder_k for {len(updates)} tensors.\")\n",
        "\n",
        "# ---------------- run chosen mode ----------------\n",
        "loaded = False\n",
        "if CKPT_MODE == \"moco_full\" and Path(CKPT_PATH).exists():\n",
        "    ratio = _try_load_full_moco(model, CKPT_PATH)\n",
        "    if ratio >= MIN_MATCH_RATIO:\n",
        "        _copy_q_to_k(model)\n",
        "        loaded = True\n",
        "    else:\n",
        "        print(\"[FALLBACK] Low match ratio — skipping pretrained load.\")\n",
        "\n",
        "elif CKPT_MODE == \"rawnet_encoder_only\" and Path(RAWNET_CKPT_PATH).exists():\n",
        "    ratio = _try_load_rawnet_encoder_only(model, RAWNET_CKPT_PATH)\n",
        "    if ratio >= MIN_MATCH_RATIO:\n",
        "        _copy_q_to_k(model)   # נוח להתחיל q וק באותו מצב\n",
        "        loaded = True\n",
        "    else:\n",
        "        print(\"[FALLBACK] Low match ratio — skipping encoder_q preload.\")\n",
        "\n",
        "else:\n",
        "    if CKPT_MODE != \"none\":\n",
        "        print(f\"[INFO] CKPT_MODE={CKPT_MODE} but file not found. Skipping.\")\n",
        "\n",
        "# ---- Advice: if no pretrained was loaded, do NOT freeze the encoder ----\n",
        "if not loaded:\n",
        "    for p in encoder_q.parameters():\n",
        "        p.requires_grad = True\n",
        "    print(\"No compatible checkpoint loaded → training from scratch (encoder unfrozen).\")\n",
        "\n",
        "# ---- Quick smoke test (keeps your shapes consistent) ----\n",
        "model.eval(); encoder_q.eval()\n",
        "with torch.no_grad():\n",
        "    dummy = torch.randn(2, 64600, device=device)  # [B,T] כמו בפרה-פרוסס שלך\n",
        "    try:\n",
        "        f = encoder_q(dummy)\n",
        "        if f.dim()==3:  f = f.mean(2)\n",
        "        elif f.dim()>3: f = f.mean(dim=tuple(range(2,f.dim())))\n",
        "        print(\"Forward OK. Encoder feature shape:\", tuple(f.shape))\n",
        "    except Exception as e:\n",
        "        print(\"Forward FAILED:\", repr(e))\n",
        "# =====================================================================================================\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGKNynacy40F",
        "outputId": "a5691193-f044-4ee1-9902-831213194676"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No compatible checkpoint loaded → training from scratch (encoder unfrozen).\n",
            "Forward OK. Encoder feature shape: (2, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== QUICK FORWARD SMOKE TEST =====\n",
        "model.eval()\n",
        "encoder_q.eval()\n",
        "with torch.no_grad():\n",
        "    dummy = torch.randn(2, 64600, device=device)  # [B,T] לפי ה-preprocess שלך\n",
        "    f = encoder_q(dummy)                          # RawNet expects [B, T]\n",
        "    if f.dim() == 3:\n",
        "        f = f.mean(dim=2)\n",
        "    elif f.dim() > 3:\n",
        "        f = f.mean(dim=tuple(range(2, f.dim())))\n",
        "    print(\"Encoder feature shape:\", tuple(f.shape))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LA7m_9pzMeE",
        "outputId": "b5a34231-9fbc-4e74-82b8-e62e1ba9b124"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder feature shape: (2, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell performs a one-batch overfitting sanity check.\n",
        "# We FREEZE the pretrained encoder (`encoder_q`), extract fixed features `f` from a single batch,\n",
        "# optionally normalize them, and then train a small probe head (`probe`) with cross-entropy to drive\n",
        "# the batch accuracy to ~100%. If accuracy does not approach 1.0, gradients or features may be faulty.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# -------------------------------\n",
        "# 1) Get one batch and move to device\n",
        "# -------------------------------\n",
        "wavs, labels = next(iter(train_loader))     # uses existing train_loader\n",
        "x = wavs.to(device).float()                 # (B, 1, T) or similar; keep dtype consistent\n",
        "y = labels.to(device).long()                # class indices as long for CE\n",
        "\n",
        "# -------------------------------\n",
        "# 2) Freeze encoder_q and set to eval to fix BN/Dropout behavior\n",
        "# -------------------------------\n",
        "encoder_q.eval()\n",
        "for p in encoder_q.parameters():\n",
        "    p.requires_grad_(False)\n",
        "\n",
        "# -------------------------------\n",
        "# 3) Extract features once (no gradient through encoder)\n",
        "#    Handle variable tensor ranks robustly and reduce temporal/extra dims by mean where needed\n",
        "# -------------------------------\n",
        "with torch.no_grad():\n",
        "    f = encoder_q(x)                        # expected shape (B, D) or (B, D, L, ...)\n",
        "    if f.dim() == 3:\n",
        "        # e.g., (B, D, L) -> mean over temporal axis\n",
        "        f = f.mean(dim=2)\n",
        "    elif f.dim() > 3:\n",
        "        # e.g., (B, D, H, W, ...) -> mean over all non-(B,D) axes\n",
        "        reduce_axes = tuple(range(2, f.dim()))\n",
        "        f = f.mean(dim=reduce_axes)\n",
        "# ensure features are detached (safety; already in no_grad)\n",
        "f = f.detach()\n",
        "\n",
        "# -------------------------------\n",
        "# 4) Optional feature normalization (per-dimension z-score)\n",
        "#    Clamp std to avoid division by zero on tiny batches\n",
        "# -------------------------------\n",
        "f = (f - f.mean(dim=0, keepdim=True)) / (f.std(dim=0, keepdim=True).clamp_min(1e-5))\n",
        "\n",
        "# -------------------------------\n",
        "# 5) Define probe head, optimizer, and loss (keep variable names consistent)\n",
        "# -------------------------------\n",
        "probe = nn.Sequential(\n",
        "    nn.Linear(f.shape[1], 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 2)                       # binary: real vs fake\n",
        ").to(device)\n",
        "\n",
        "opt  = torch.optim.AdamW(probe.parameters(), lr=1e-2, weight_decay=0.0)\n",
        "crit = nn.CrossEntropyLoss()\n",
        "\n",
        "# -------------------------------\n",
        "# 6) Train the probe on the single batch for several steps\n",
        "# -------------------------------\n",
        "torch.manual_seed(0)                        # reproducibility for this check\n",
        "\n",
        "for step in range(1000):                    # increase to 2000 if needed\n",
        "    logits = probe(f)                       # (B, 2); no no_grad here to allow backprop into probe\n",
        "    loss = crit(logits, y)\n",
        "\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    if (step + 1) % 200 == 0:\n",
        "        with torch.no_grad():\n",
        "            acc = (logits.argmax(dim=1) == y).float().mean().item()\n",
        "        print(f\"step {step+1}: loss {loss.item():.4f}, acc {acc:.3f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 7) Final accuracy report on the same batch\n",
        "# -------------------------------\n",
        "with torch.no_grad():\n",
        "    final_acc = (logits.argmax(dim=1) == y).float().mean().item()\n",
        "print(\"Final overfit-batch acc:\", final_acc)\n",
        "\n",
        "# Notes:\n",
        "# - This cell preserves existing variable names: wavs, labels, x, y, f, probe, opt, crit, logits.\n",
        "# - encoder_q remains frozen and in eval() throughout; only `probe` is trained here.\n"
      ],
      "metadata": {
        "id": "LzSDeeKf4WFi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d2f6ec4-5b64-4636-f8bc-0e2dc2ab9116"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 200: loss 0.0000, acc 1.000\n",
            "step 400: loss 0.0000, acc 1.000\n",
            "step 600: loss 0.0000, acc 1.000\n",
            "step 800: loss 0.0000, acc 1.000\n",
            "step 1000: loss 0.0000, acc 1.000\n",
            "Final overfit-batch acc: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1 =========================================\n",
        "# Fine-tuning: linear classifier on top of FROZEN encoder_q\n",
        "# (RawNet encoder_q expects input shape [B, T], not [B,1,T]!)\n",
        "# =========================================\n",
        "from pathlib import Path\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchaudio\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "\n",
        "# Change directory to CLAD\n",
        "os.chdir('/content/CLAD')\n",
        "\n",
        "from Model import MoCo_v2, RawNetEncoderBaseline\n",
        "\n",
        "# Step 1: Define the RawNet encoder configuration\n",
        "d_args = {\n",
        "    \"in_channels\": 1,\n",
        "    \"first_conv\": 251,\n",
        "    \"filts\": [\n",
        "        128,  # output channels for sinc conv\n",
        "        [128, 128],  # block0 and block1\n",
        "        [128, 256],  # block2\n",
        "        [256, 256]   # block3-5\n",
        "    ],\n",
        "    \"nb_fc_node\": 1024,\n",
        "    \"gru_node\": 1024,\n",
        "    \"nb_gru_layer\": 3,\n",
        "    \"nb_classes\": 2\n",
        "}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Step 2: Create both encoders\n",
        "encoder_q = RawNetEncoderBaseline(d_args, device)\n",
        "encoder_k = RawNetEncoderBaseline(d_args, device)\n",
        "\n",
        "# Step 3: Create the MoCo_v2 model\n",
        "model = MoCo_v2(\n",
        "    encoder_q=encoder_q,\n",
        "    encoder_k=encoder_k,\n",
        "    queue_feature_dim=1024,  # matches encoder output\n",
        "    mlp=True,\n",
        "    return_q=True\n",
        ")\n",
        "\n",
        "# Step 4: Load pretrained weights\n",
        "ckpt_path = Path(\"pretrained_models/CLAD_150_10_2310.pth.tar\")\n",
        "if not ckpt_path.exists():\n",
        "    raise FileNotFoundError(f\"Checkpoint not found: {ckpt_path}\")\n",
        "\n",
        "ckpt = torch.load(ckpt_path, map_location='cpu')\n",
        "state_dict = ckpt.get(\"state_dict\", ckpt)\n",
        "\n",
        "missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n",
        "print(\"Missing keys:\", missing_keys)\n",
        "print(\"Unexpected keys:\", unexpected_keys)\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"CLAD model loaded and ready.\")\n",
        "\n",
        "\n",
        "# -------- Config --------\n",
        "TARGET_SR  = 16000\n",
        "TARGET_LEN = 64600\n",
        "LABELS     = {\"real\": 0, \"fake\": 1}\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS     = 8\n",
        "LR         = 1e-3\n",
        "\n",
        "# -------- Dataset: returns [T] (mono 16k, fixed length) --------\n",
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, root_dir: str):\n",
        "        self.samples = []\n",
        "        root = Path(root_dir)\n",
        "        for cls in (\"real\", \"fake\"):\n",
        "            base = root / cls\n",
        "            if not base.exists():\n",
        "                raise FileNotFoundError(f\"Missing directory: {base}\")\n",
        "            for p in base.rglob(\"*.wav\"):\n",
        "                if p.is_file():\n",
        "                    self.samples.append((p, LABELS[cls]))\n",
        "        if len(self.samples) == 0:\n",
        "            raise ValueError(f\"No .wav files found under {root_dir}. Ensure split+preprocess ran correctly.\")\n",
        "        self._resamplers = {}\n",
        "\n",
        "    def _fix(self, path: Path) -> torch.Tensor:\n",
        "        wav, sr = torchaudio.load(str(path))  # [C,T]\n",
        "        wav = wav.float()\n",
        "        if wav.shape[0] > 1:\n",
        "            wav = wav.mean(dim=0, keepdim=True)       # [1,T]\n",
        "        if sr != TARGET_SR:\n",
        "            if sr not in self._resamplers:\n",
        "                self._resamplers[sr] = torchaudio.transforms.Resample(orig_freq=sr, new_freq=TARGET_SR)\n",
        "            wav = self._resamplers[sr](wav)           # [1,T']\n",
        "        T = wav.shape[-1]\n",
        "        if T < TARGET_LEN:\n",
        "            wav = F.pad(wav, (0, TARGET_LEN - T))\n",
        "        else:\n",
        "            wav = wav[..., :TARGET_LEN]\n",
        "        return wav.squeeze(0)                         # -> [T]\n",
        "\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        return self._fix(path), label                 # ([T], label)\n",
        "\n",
        "# -------- DataLoaders --------\n",
        "train_ds = AudioDataset(\"/content/my_audio_split/train\")\n",
        "val_ds   = AudioDataset(\"/content/my_audio_split/val\")\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  drop_last=False)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
        "\n",
        "# -------- Freeze encoder --------\n",
        "encoder_q.to(device).eval()\n",
        "for p in encoder_q.parameters():\n",
        "    p.requires_grad = False\n",
        "encoder_q.eval().to(device)\n",
        "\n",
        "# -------- Infer feature dimension D (NOTE: pass [B,T] to encoder_q) --------\n",
        "with torch.no_grad():\n",
        "    wavs0, _ = next(iter(train_loader))     # wavs0: [B, T]\n",
        "    x0 = wavs0.to(device)                   # [B, T]  <-- no unsqueeze\n",
        "    feat0 = encoder_q(x0)                   # RawNet expects [B, T]\n",
        "    # If encoder returns >2D (e.g., [B, C, T']), reduce over time dims:\n",
        "    if feat0.dim() == 3:\n",
        "        feat0 = feat0.mean(dim=2)           # [B, C]\n",
        "    elif feat0.dim() > 3:\n",
        "        feat0 = feat0.mean(dim=tuple(range(2, feat0.dim())))\n",
        "    D = feat0.shape[1]\n",
        "\n",
        "# -------- Classifier, loss, optimizer --------\n",
        "classifier = nn.Sequential(\n",
        "    nn.Linear(D, 64),  # use your probed feature dim here\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.2),\n",
        "    nn.Linear(64, 2)\n",
        ").to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(classifier.parameters(), lr=2e-3, weight_decay=1e-4)\n",
        "\n",
        "# -------- Train + Validate --------\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    # ---- Train ----\n",
        "    classifier.train()\n",
        "    tr_loss = tr_correct = tr_total = 0.0\n",
        "\n",
        "    for wavs, labels in train_loader:\n",
        "        x = wavs.to(device).float()          # [B, T]\n",
        "        y = labels.to(device).long()         # 0=real, 1=fake\n",
        "\n",
        "        # encoder forward (NO grad here)\n",
        "        with torch.no_grad():\n",
        "            f = encoder_q(x)                 # (B, D) or (B, D, T, ...)\n",
        "            if f.dim() == 3:\n",
        "                f = f.mean(dim=2)            # pool time -> (B, D)\n",
        "            elif f.dim() > 3:\n",
        "                f = f.mean(dim=tuple(range(2, f.dim())))  # reduce extra dims\n",
        "\n",
        "        # head forward WITH grad\n",
        "        logits = classifier(f)               # (B, 2)\n",
        "        loss = criterion(logits, y)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        tr_loss   += loss.item() * x.size(0)\n",
        "        tr_correct += (logits.argmax(dim=1) == y).sum().item()\n",
        "        tr_total  += x.size(0)\n",
        "\n",
        "    # ---- Val ----\n",
        "    classifier.eval()\n",
        "    va_loss, va_correct, va_total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "      for wavs, labels in val_loader:\n",
        "          x = wavs.to(device).float()\n",
        "          y = labels.to(device).long()\n",
        "          f = encoder_q(x)\n",
        "          if f.dim() == 3:\n",
        "              f = f.mean(dim=2)\n",
        "          elif f.dim() > 3:\n",
        "              f = f.mean(dim=tuple(range(2, f.dim())))\n",
        "          logits = classifier(f)\n",
        "          va_loss   += criterion(logits, y).item() * x.size(0)\n",
        "          va_correct += (logits.argmax(dim=1) == y).sum().item()\n",
        "          va_total  += x.size(0)\n",
        "\n",
        "    print(f\"Epoch {epoch}/{EPOCHS} | \"\n",
        "          f\"Train Loss: {tr_loss/tr_total:.4f}, Acc: {tr_correct/tr_total:.3f} | \"\n",
        "          f\"Val Loss: {va_loss/va_total:.4f}, Acc: {va_correct/va_total:.3f}\")"
      ],
      "metadata": {
        "id": "LLtOZnD1rsQx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b498cbb6-590e-465a-9972-7c097cfcc2a6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing keys: ['encoder_q.first_bn.weight', 'encoder_q.first_bn.bias', 'encoder_q.first_bn.running_mean', 'encoder_q.first_bn.running_var', 'encoder_q.block0.0.conv1.weight', 'encoder_q.block0.0.conv1.bias', 'encoder_q.block0.0.bn2.weight', 'encoder_q.block0.0.bn2.bias', 'encoder_q.block0.0.bn2.running_mean', 'encoder_q.block0.0.bn2.running_var', 'encoder_q.block0.0.conv2.weight', 'encoder_q.block0.0.conv2.bias', 'encoder_q.block1.0.bn1.weight', 'encoder_q.block1.0.bn1.bias', 'encoder_q.block1.0.bn1.running_mean', 'encoder_q.block1.0.bn1.running_var', 'encoder_q.block1.0.conv1.weight', 'encoder_q.block1.0.conv1.bias', 'encoder_q.block1.0.bn2.weight', 'encoder_q.block1.0.bn2.bias', 'encoder_q.block1.0.bn2.running_mean', 'encoder_q.block1.0.bn2.running_var', 'encoder_q.block1.0.conv2.weight', 'encoder_q.block1.0.conv2.bias', 'encoder_q.block2.0.bn1.weight', 'encoder_q.block2.0.bn1.bias', 'encoder_q.block2.0.bn1.running_mean', 'encoder_q.block2.0.bn1.running_var', 'encoder_q.block2.0.conv1.weight', 'encoder_q.block2.0.conv1.bias', 'encoder_q.block2.0.bn2.weight', 'encoder_q.block2.0.bn2.bias', 'encoder_q.block2.0.bn2.running_mean', 'encoder_q.block2.0.bn2.running_var', 'encoder_q.block2.0.conv2.weight', 'encoder_q.block2.0.conv2.bias', 'encoder_q.block2.0.conv_downsample.weight', 'encoder_q.block2.0.conv_downsample.bias', 'encoder_q.block3.0.bn1.weight', 'encoder_q.block3.0.bn1.bias', 'encoder_q.block3.0.bn1.running_mean', 'encoder_q.block3.0.bn1.running_var', 'encoder_q.block3.0.conv1.weight', 'encoder_q.block3.0.conv1.bias', 'encoder_q.block3.0.bn2.weight', 'encoder_q.block3.0.bn2.bias', 'encoder_q.block3.0.bn2.running_mean', 'encoder_q.block3.0.bn2.running_var', 'encoder_q.block3.0.conv2.weight', 'encoder_q.block3.0.conv2.bias', 'encoder_q.block4.0.bn1.weight', 'encoder_q.block4.0.bn1.bias', 'encoder_q.block4.0.bn1.running_mean', 'encoder_q.block4.0.bn1.running_var', 'encoder_q.block4.0.conv1.weight', 'encoder_q.block4.0.conv1.bias', 'encoder_q.block4.0.bn2.weight', 'encoder_q.block4.0.bn2.bias', 'encoder_q.block4.0.bn2.running_mean', 'encoder_q.block4.0.bn2.running_var', 'encoder_q.block4.0.conv2.weight', 'encoder_q.block4.0.conv2.bias', 'encoder_q.block5.0.bn1.weight', 'encoder_q.block5.0.bn1.bias', 'encoder_q.block5.0.bn1.running_mean', 'encoder_q.block5.0.bn1.running_var', 'encoder_q.block5.0.conv1.weight', 'encoder_q.block5.0.conv1.bias', 'encoder_q.block5.0.bn2.weight', 'encoder_q.block5.0.bn2.bias', 'encoder_q.block5.0.bn2.running_mean', 'encoder_q.block5.0.bn2.running_var', 'encoder_q.block5.0.conv2.weight', 'encoder_q.block5.0.conv2.bias', 'encoder_q.fc_attention0.0.weight', 'encoder_q.fc_attention0.0.bias', 'encoder_q.fc_attention1.0.weight', 'encoder_q.fc_attention1.0.bias', 'encoder_q.fc_attention2.0.weight', 'encoder_q.fc_attention2.0.bias', 'encoder_q.fc_attention3.0.weight', 'encoder_q.fc_attention3.0.bias', 'encoder_q.fc_attention4.0.weight', 'encoder_q.fc_attention4.0.bias', 'encoder_q.fc_attention5.0.weight', 'encoder_q.fc_attention5.0.bias', 'encoder_q.bn_before_gru.weight', 'encoder_q.bn_before_gru.bias', 'encoder_q.bn_before_gru.running_mean', 'encoder_q.bn_before_gru.running_var', 'encoder_q.gru.weight_ih_l0', 'encoder_q.gru.weight_hh_l0', 'encoder_q.gru.bias_ih_l0', 'encoder_q.gru.bias_hh_l0', 'encoder_q.gru.weight_ih_l1', 'encoder_q.gru.weight_hh_l1', 'encoder_q.gru.bias_ih_l1', 'encoder_q.gru.bias_hh_l1', 'encoder_q.gru.weight_ih_l2', 'encoder_q.gru.weight_hh_l2', 'encoder_q.gru.bias_ih_l2', 'encoder_q.gru.bias_hh_l2', 'encoder_q.fc1_gru.weight', 'encoder_q.fc1_gru.bias', 'encoder_q.fc2_gru.weight', 'encoder_q.fc2_gru.bias', 'encoder_k.first_bn.weight', 'encoder_k.first_bn.bias', 'encoder_k.first_bn.running_mean', 'encoder_k.first_bn.running_var', 'encoder_k.block0.0.conv1.weight', 'encoder_k.block0.0.conv1.bias', 'encoder_k.block0.0.bn2.weight', 'encoder_k.block0.0.bn2.bias', 'encoder_k.block0.0.bn2.running_mean', 'encoder_k.block0.0.bn2.running_var', 'encoder_k.block0.0.conv2.weight', 'encoder_k.block0.0.conv2.bias', 'encoder_k.block1.0.bn1.weight', 'encoder_k.block1.0.bn1.bias', 'encoder_k.block1.0.bn1.running_mean', 'encoder_k.block1.0.bn1.running_var', 'encoder_k.block1.0.conv1.weight', 'encoder_k.block1.0.conv1.bias', 'encoder_k.block1.0.bn2.weight', 'encoder_k.block1.0.bn2.bias', 'encoder_k.block1.0.bn2.running_mean', 'encoder_k.block1.0.bn2.running_var', 'encoder_k.block1.0.conv2.weight', 'encoder_k.block1.0.conv2.bias', 'encoder_k.block2.0.bn1.weight', 'encoder_k.block2.0.bn1.bias', 'encoder_k.block2.0.bn1.running_mean', 'encoder_k.block2.0.bn1.running_var', 'encoder_k.block2.0.conv1.weight', 'encoder_k.block2.0.conv1.bias', 'encoder_k.block2.0.bn2.weight', 'encoder_k.block2.0.bn2.bias', 'encoder_k.block2.0.bn2.running_mean', 'encoder_k.block2.0.bn2.running_var', 'encoder_k.block2.0.conv2.weight', 'encoder_k.block2.0.conv2.bias', 'encoder_k.block2.0.conv_downsample.weight', 'encoder_k.block2.0.conv_downsample.bias', 'encoder_k.block3.0.bn1.weight', 'encoder_k.block3.0.bn1.bias', 'encoder_k.block3.0.bn1.running_mean', 'encoder_k.block3.0.bn1.running_var', 'encoder_k.block3.0.conv1.weight', 'encoder_k.block3.0.conv1.bias', 'encoder_k.block3.0.bn2.weight', 'encoder_k.block3.0.bn2.bias', 'encoder_k.block3.0.bn2.running_mean', 'encoder_k.block3.0.bn2.running_var', 'encoder_k.block3.0.conv2.weight', 'encoder_k.block3.0.conv2.bias', 'encoder_k.block4.0.bn1.weight', 'encoder_k.block4.0.bn1.bias', 'encoder_k.block4.0.bn1.running_mean', 'encoder_k.block4.0.bn1.running_var', 'encoder_k.block4.0.conv1.weight', 'encoder_k.block4.0.conv1.bias', 'encoder_k.block4.0.bn2.weight', 'encoder_k.block4.0.bn2.bias', 'encoder_k.block4.0.bn2.running_mean', 'encoder_k.block4.0.bn2.running_var', 'encoder_k.block4.0.conv2.weight', 'encoder_k.block4.0.conv2.bias', 'encoder_k.block5.0.bn1.weight', 'encoder_k.block5.0.bn1.bias', 'encoder_k.block5.0.bn1.running_mean', 'encoder_k.block5.0.bn1.running_var', 'encoder_k.block5.0.conv1.weight', 'encoder_k.block5.0.conv1.bias', 'encoder_k.block5.0.bn2.weight', 'encoder_k.block5.0.bn2.bias', 'encoder_k.block5.0.bn2.running_mean', 'encoder_k.block5.0.bn2.running_var', 'encoder_k.block5.0.conv2.weight', 'encoder_k.block5.0.conv2.bias', 'encoder_k.fc_attention0.0.weight', 'encoder_k.fc_attention0.0.bias', 'encoder_k.fc_attention1.0.weight', 'encoder_k.fc_attention1.0.bias', 'encoder_k.fc_attention2.0.weight', 'encoder_k.fc_attention2.0.bias', 'encoder_k.fc_attention3.0.weight', 'encoder_k.fc_attention3.0.bias', 'encoder_k.fc_attention4.0.weight', 'encoder_k.fc_attention4.0.bias', 'encoder_k.fc_attention5.0.weight', 'encoder_k.fc_attention5.0.bias', 'encoder_k.bn_before_gru.weight', 'encoder_k.bn_before_gru.bias', 'encoder_k.bn_before_gru.running_mean', 'encoder_k.bn_before_gru.running_var', 'encoder_k.gru.weight_ih_l0', 'encoder_k.gru.weight_hh_l0', 'encoder_k.gru.bias_ih_l0', 'encoder_k.gru.bias_hh_l0', 'encoder_k.gru.weight_ih_l1', 'encoder_k.gru.weight_hh_l1', 'encoder_k.gru.bias_ih_l1', 'encoder_k.gru.bias_hh_l1', 'encoder_k.gru.weight_ih_l2', 'encoder_k.gru.weight_hh_l2', 'encoder_k.gru.bias_ih_l2', 'encoder_k.gru.bias_hh_l2', 'encoder_k.fc1_gru.weight', 'encoder_k.fc1_gru.bias', 'encoder_k.fc2_gru.weight', 'encoder_k.fc2_gru.bias', 'projection_head_q.0.weight', 'projection_head_q.0.bias', 'projection_head_q.2.weight', 'projection_head_q.2.bias', 'projection_head_k.0.weight', 'projection_head_k.0.bias', 'projection_head_k.2.weight', 'projection_head_k.2.bias']\n",
            "Unexpected keys: ['encoder.pos_S', 'encoder.master1', 'encoder.master2', 'encoder.first_bn.weight', 'encoder.first_bn.bias', 'encoder.first_bn.running_mean', 'encoder.first_bn.running_var', 'encoder.first_bn.num_batches_tracked', 'encoder.encoder.0.0.conv1.weight', 'encoder.encoder.0.0.conv1.bias', 'encoder.encoder.0.0.bn2.weight', 'encoder.encoder.0.0.bn2.bias', 'encoder.encoder.0.0.bn2.running_mean', 'encoder.encoder.0.0.bn2.running_var', 'encoder.encoder.0.0.bn2.num_batches_tracked', 'encoder.encoder.0.0.conv2.weight', 'encoder.encoder.0.0.conv2.bias', 'encoder.encoder.0.0.conv_downsample.weight', 'encoder.encoder.0.0.conv_downsample.bias', 'encoder.encoder.1.0.bn1.weight', 'encoder.encoder.1.0.bn1.bias', 'encoder.encoder.1.0.bn1.running_mean', 'encoder.encoder.1.0.bn1.running_var', 'encoder.encoder.1.0.bn1.num_batches_tracked', 'encoder.encoder.1.0.conv1.weight', 'encoder.encoder.1.0.conv1.bias', 'encoder.encoder.1.0.bn2.weight', 'encoder.encoder.1.0.bn2.bias', 'encoder.encoder.1.0.bn2.running_mean', 'encoder.encoder.1.0.bn2.running_var', 'encoder.encoder.1.0.bn2.num_batches_tracked', 'encoder.encoder.1.0.conv2.weight', 'encoder.encoder.1.0.conv2.bias', 'encoder.encoder.2.0.bn1.weight', 'encoder.encoder.2.0.bn1.bias', 'encoder.encoder.2.0.bn1.running_mean', 'encoder.encoder.2.0.bn1.running_var', 'encoder.encoder.2.0.bn1.num_batches_tracked', 'encoder.encoder.2.0.conv1.weight', 'encoder.encoder.2.0.conv1.bias', 'encoder.encoder.2.0.bn2.weight', 'encoder.encoder.2.0.bn2.bias', 'encoder.encoder.2.0.bn2.running_mean', 'encoder.encoder.2.0.bn2.running_var', 'encoder.encoder.2.0.bn2.num_batches_tracked', 'encoder.encoder.2.0.conv2.weight', 'encoder.encoder.2.0.conv2.bias', 'encoder.encoder.2.0.conv_downsample.weight', 'encoder.encoder.2.0.conv_downsample.bias', 'encoder.encoder.3.0.bn1.weight', 'encoder.encoder.3.0.bn1.bias', 'encoder.encoder.3.0.bn1.running_mean', 'encoder.encoder.3.0.bn1.running_var', 'encoder.encoder.3.0.bn1.num_batches_tracked', 'encoder.encoder.3.0.conv1.weight', 'encoder.encoder.3.0.conv1.bias', 'encoder.encoder.3.0.bn2.weight', 'encoder.encoder.3.0.bn2.bias', 'encoder.encoder.3.0.bn2.running_mean', 'encoder.encoder.3.0.bn2.running_var', 'encoder.encoder.3.0.bn2.num_batches_tracked', 'encoder.encoder.3.0.conv2.weight', 'encoder.encoder.3.0.conv2.bias', 'encoder.encoder.4.0.bn1.weight', 'encoder.encoder.4.0.bn1.bias', 'encoder.encoder.4.0.bn1.running_mean', 'encoder.encoder.4.0.bn1.running_var', 'encoder.encoder.4.0.bn1.num_batches_tracked', 'encoder.encoder.4.0.conv1.weight', 'encoder.encoder.4.0.conv1.bias', 'encoder.encoder.4.0.bn2.weight', 'encoder.encoder.4.0.bn2.bias', 'encoder.encoder.4.0.bn2.running_mean', 'encoder.encoder.4.0.bn2.running_var', 'encoder.encoder.4.0.bn2.num_batches_tracked', 'encoder.encoder.4.0.conv2.weight', 'encoder.encoder.4.0.conv2.bias', 'encoder.encoder.5.0.bn1.weight', 'encoder.encoder.5.0.bn1.bias', 'encoder.encoder.5.0.bn1.running_mean', 'encoder.encoder.5.0.bn1.running_var', 'encoder.encoder.5.0.bn1.num_batches_tracked', 'encoder.encoder.5.0.conv1.weight', 'encoder.encoder.5.0.conv1.bias', 'encoder.encoder.5.0.bn2.weight', 'encoder.encoder.5.0.bn2.bias', 'encoder.encoder.5.0.bn2.running_mean', 'encoder.encoder.5.0.bn2.running_var', 'encoder.encoder.5.0.bn2.num_batches_tracked', 'encoder.encoder.5.0.conv2.weight', 'encoder.encoder.5.0.conv2.bias', 'encoder.GAT_layer_S.att_weight', 'encoder.GAT_layer_S.att_proj.weight', 'encoder.GAT_layer_S.att_proj.bias', 'encoder.GAT_layer_S.proj_with_att.weight', 'encoder.GAT_layer_S.proj_with_att.bias', 'encoder.GAT_layer_S.proj_without_att.weight', 'encoder.GAT_layer_S.proj_without_att.bias', 'encoder.GAT_layer_S.bn.weight', 'encoder.GAT_layer_S.bn.bias', 'encoder.GAT_layer_S.bn.running_mean', 'encoder.GAT_layer_S.bn.running_var', 'encoder.GAT_layer_S.bn.num_batches_tracked', 'encoder.GAT_layer_T.att_weight', 'encoder.GAT_layer_T.att_proj.weight', 'encoder.GAT_layer_T.att_proj.bias', 'encoder.GAT_layer_T.proj_with_att.weight', 'encoder.GAT_layer_T.proj_with_att.bias', 'encoder.GAT_layer_T.proj_without_att.weight', 'encoder.GAT_layer_T.proj_without_att.bias', 'encoder.GAT_layer_T.bn.weight', 'encoder.GAT_layer_T.bn.bias', 'encoder.GAT_layer_T.bn.running_mean', 'encoder.GAT_layer_T.bn.running_var', 'encoder.GAT_layer_T.bn.num_batches_tracked', 'encoder.HtrgGAT_layer_ST11.att_weight11', 'encoder.HtrgGAT_layer_ST11.att_weight22', 'encoder.HtrgGAT_layer_ST11.att_weight12', 'encoder.HtrgGAT_layer_ST11.att_weightM', 'encoder.HtrgGAT_layer_ST11.proj_type1.weight', 'encoder.HtrgGAT_layer_ST11.proj_type1.bias', 'encoder.HtrgGAT_layer_ST11.proj_type2.weight', 'encoder.HtrgGAT_layer_ST11.proj_type2.bias', 'encoder.HtrgGAT_layer_ST11.att_proj.weight', 'encoder.HtrgGAT_layer_ST11.att_proj.bias', 'encoder.HtrgGAT_layer_ST11.att_projM.weight', 'encoder.HtrgGAT_layer_ST11.att_projM.bias', 'encoder.HtrgGAT_layer_ST11.proj_with_att.weight', 'encoder.HtrgGAT_layer_ST11.proj_with_att.bias', 'encoder.HtrgGAT_layer_ST11.proj_without_att.weight', 'encoder.HtrgGAT_layer_ST11.proj_without_att.bias', 'encoder.HtrgGAT_layer_ST11.proj_with_attM.weight', 'encoder.HtrgGAT_layer_ST11.proj_with_attM.bias', 'encoder.HtrgGAT_layer_ST11.proj_without_attM.weight', 'encoder.HtrgGAT_layer_ST11.proj_without_attM.bias', 'encoder.HtrgGAT_layer_ST11.bn.weight', 'encoder.HtrgGAT_layer_ST11.bn.bias', 'encoder.HtrgGAT_layer_ST11.bn.running_mean', 'encoder.HtrgGAT_layer_ST11.bn.running_var', 'encoder.HtrgGAT_layer_ST11.bn.num_batches_tracked', 'encoder.HtrgGAT_layer_ST12.att_weight11', 'encoder.HtrgGAT_layer_ST12.att_weight22', 'encoder.HtrgGAT_layer_ST12.att_weight12', 'encoder.HtrgGAT_layer_ST12.att_weightM', 'encoder.HtrgGAT_layer_ST12.proj_type1.weight', 'encoder.HtrgGAT_layer_ST12.proj_type1.bias', 'encoder.HtrgGAT_layer_ST12.proj_type2.weight', 'encoder.HtrgGAT_layer_ST12.proj_type2.bias', 'encoder.HtrgGAT_layer_ST12.att_proj.weight', 'encoder.HtrgGAT_layer_ST12.att_proj.bias', 'encoder.HtrgGAT_layer_ST12.att_projM.weight', 'encoder.HtrgGAT_layer_ST12.att_projM.bias', 'encoder.HtrgGAT_layer_ST12.proj_with_att.weight', 'encoder.HtrgGAT_layer_ST12.proj_with_att.bias', 'encoder.HtrgGAT_layer_ST12.proj_without_att.weight', 'encoder.HtrgGAT_layer_ST12.proj_without_att.bias', 'encoder.HtrgGAT_layer_ST12.proj_with_attM.weight', 'encoder.HtrgGAT_layer_ST12.proj_with_attM.bias', 'encoder.HtrgGAT_layer_ST12.proj_without_attM.weight', 'encoder.HtrgGAT_layer_ST12.proj_without_attM.bias', 'encoder.HtrgGAT_layer_ST12.bn.weight', 'encoder.HtrgGAT_layer_ST12.bn.bias', 'encoder.HtrgGAT_layer_ST12.bn.running_mean', 'encoder.HtrgGAT_layer_ST12.bn.running_var', 'encoder.HtrgGAT_layer_ST12.bn.num_batches_tracked', 'encoder.HtrgGAT_layer_ST21.att_weight11', 'encoder.HtrgGAT_layer_ST21.att_weight22', 'encoder.HtrgGAT_layer_ST21.att_weight12', 'encoder.HtrgGAT_layer_ST21.att_weightM', 'encoder.HtrgGAT_layer_ST21.proj_type1.weight', 'encoder.HtrgGAT_layer_ST21.proj_type1.bias', 'encoder.HtrgGAT_layer_ST21.proj_type2.weight', 'encoder.HtrgGAT_layer_ST21.proj_type2.bias', 'encoder.HtrgGAT_layer_ST21.att_proj.weight', 'encoder.HtrgGAT_layer_ST21.att_proj.bias', 'encoder.HtrgGAT_layer_ST21.att_projM.weight', 'encoder.HtrgGAT_layer_ST21.att_projM.bias', 'encoder.HtrgGAT_layer_ST21.proj_with_att.weight', 'encoder.HtrgGAT_layer_ST21.proj_with_att.bias', 'encoder.HtrgGAT_layer_ST21.proj_without_att.weight', 'encoder.HtrgGAT_layer_ST21.proj_without_att.bias', 'encoder.HtrgGAT_layer_ST21.proj_with_attM.weight', 'encoder.HtrgGAT_layer_ST21.proj_with_attM.bias', 'encoder.HtrgGAT_layer_ST21.proj_without_attM.weight', 'encoder.HtrgGAT_layer_ST21.proj_without_attM.bias', 'encoder.HtrgGAT_layer_ST21.bn.weight', 'encoder.HtrgGAT_layer_ST21.bn.bias', 'encoder.HtrgGAT_layer_ST21.bn.running_mean', 'encoder.HtrgGAT_layer_ST21.bn.running_var', 'encoder.HtrgGAT_layer_ST21.bn.num_batches_tracked', 'encoder.HtrgGAT_layer_ST22.att_weight11', 'encoder.HtrgGAT_layer_ST22.att_weight22', 'encoder.HtrgGAT_layer_ST22.att_weight12', 'encoder.HtrgGAT_layer_ST22.att_weightM', 'encoder.HtrgGAT_layer_ST22.proj_type1.weight', 'encoder.HtrgGAT_layer_ST22.proj_type1.bias', 'encoder.HtrgGAT_layer_ST22.proj_type2.weight', 'encoder.HtrgGAT_layer_ST22.proj_type2.bias', 'encoder.HtrgGAT_layer_ST22.att_proj.weight', 'encoder.HtrgGAT_layer_ST22.att_proj.bias', 'encoder.HtrgGAT_layer_ST22.att_projM.weight', 'encoder.HtrgGAT_layer_ST22.att_projM.bias', 'encoder.HtrgGAT_layer_ST22.proj_with_att.weight', 'encoder.HtrgGAT_layer_ST22.proj_with_att.bias', 'encoder.HtrgGAT_layer_ST22.proj_without_att.weight', 'encoder.HtrgGAT_layer_ST22.proj_without_att.bias', 'encoder.HtrgGAT_layer_ST22.proj_with_attM.weight', 'encoder.HtrgGAT_layer_ST22.proj_with_attM.bias', 'encoder.HtrgGAT_layer_ST22.proj_without_attM.weight', 'encoder.HtrgGAT_layer_ST22.proj_without_attM.bias', 'encoder.HtrgGAT_layer_ST22.bn.weight', 'encoder.HtrgGAT_layer_ST22.bn.bias', 'encoder.HtrgGAT_layer_ST22.bn.running_mean', 'encoder.HtrgGAT_layer_ST22.bn.running_var', 'encoder.HtrgGAT_layer_ST22.bn.num_batches_tracked', 'encoder.pool_S.proj.weight', 'encoder.pool_S.proj.bias', 'encoder.pool_T.proj.weight', 'encoder.pool_T.proj.bias', 'encoder.pool_hS1.proj.weight', 'encoder.pool_hS1.proj.bias', 'encoder.pool_hT1.proj.weight', 'encoder.pool_hT1.proj.bias', 'encoder.pool_hS2.proj.weight', 'encoder.pool_hS2.proj.bias', 'encoder.pool_hT2.proj.weight', 'encoder.pool_hT2.proj.bias', 'encoder.out_layer.weight', 'encoder.out_layer.bias', 'fc.weight', 'fc.bias']\n",
            "CLAD model loaded and ready.\n",
            "Epoch 1/8 | Train Loss: 0.6947, Acc: 0.473 | Val Loss: 0.6932, Acc: 0.500\n",
            "Epoch 2/8 | Train Loss: 0.6934, Acc: 0.494 | Val Loss: 0.6931, Acc: 0.500\n",
            "Epoch 3/8 | Train Loss: 0.6933, Acc: 0.490 | Val Loss: 0.6931, Acc: 0.500\n",
            "Epoch 4/8 | Train Loss: 0.6933, Acc: 0.500 | Val Loss: 0.6932, Acc: 0.500\n",
            "Epoch 5/8 | Train Loss: 0.6932, Acc: 0.476 | Val Loss: 0.6931, Acc: 0.500\n",
            "Epoch 6/8 | Train Loss: 0.6933, Acc: 0.485 | Val Loss: 0.6931, Acc: 0.500\n",
            "Epoch 7/8 | Train Loss: 0.6934, Acc: 0.492 | Val Loss: 0.6932, Acc: 0.500\n",
            "Epoch 8/8 | Train Loss: 0.6934, Acc: 0.500 | Val Loss: 0.6932, Acc: 0.500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2 =========================================\n",
        "# Evaluate model on validation/test sets\n",
        "# =========================================\n",
        "from sklearn.metrics import classification_report\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def evaluate(loader, split_name=\"val\", model=None, classifier=None, device=None):\n",
        "    if model is None or classifier is None or device is None:\n",
        "        raise ValueError(\"model, classifier, and device must be provided to evaluate function.\")\n",
        "\n",
        "    # Ensure classifier is on the correct device\n",
        "    classifier.to(device)\n",
        "    classifier.eval()\n",
        "\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for wavs, labels in loader:\n",
        "            # Ensure data is on the correct device\n",
        "            wavs, labels = wavs.to(device), labels.to(device)\n",
        "\n",
        "            # Use the encoder from the loaded model\n",
        "            feats = model.encoder_q(wavs)\n",
        "            # Pool any extra time/freq dims so we have (B,D)\n",
        "            if feats.dim() == 3:\n",
        "                feats = feats.mean(dim=2)\n",
        "            elif feats.dim() > 3:\n",
        "                feats = feats.mean(dim=tuple(range(2, feats.dim())))\n",
        "\n",
        "            preds = classifier(feats)\n",
        "            all_preds.extend(torch.argmax(preds, dim=1).cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    print(f\"=== {split_name.upper()} RESULTS ===\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=[\"real\",\"fake\"]))\n",
        "\n",
        "# Run evaluation\n",
        "# Pass the necessary objects to the evaluate function\n",
        "evaluate(val_loader, \"val\", model=model, classifier=classifier, device=device)\n",
        "\n",
        "# Re-create test_loader as it might not be defined in the current runtime\n",
        "test_ds = AudioDataset(\"/content/my_audio_split/test\")\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
        "evaluate(test_loader, \"test\", model=model, classifier=classifier, device=device)"
      ],
      "metadata": {
        "id": "soJVwepYrs6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#במקום 12\n",
        "# ===== Unified predict_file: encoder_q + classifier (replaces any older predict_file) =====\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "\n",
        "TARGET_SR  = 16000\n",
        "TARGET_LEN = 64600\n",
        "\n",
        "def _load_and_fix(path: Path):\n",
        "    wav, sr = torchaudio.load(str(path))   # [C,T]\n",
        "    wav = wav.float()\n",
        "    if wav.shape[0] > 1:\n",
        "        wav = wav.mean(dim=0, keepdim=True)\n",
        "    if sr != TARGET_SR:\n",
        "        res = torchaudio.transforms.Resample(orig_freq=sr, new_freq=TARGET_SR)\n",
        "        wav = res(wav)                      # [1,T']\n",
        "    T = wav.shape[-1]\n",
        "    if T < TARGET_LEN:\n",
        "        wav = F.pad(wav, (0, TARGET_LEN - T))\n",
        "    else:\n",
        "        wav = wav[..., :TARGET_LEN]\n",
        "    return wav.squeeze(0)                   # -> [T]\n",
        "\n",
        "def predict_file(path: Path, encoder, classifier, device):\n",
        "    \"\"\"Return (label, fake_conf, real_conf, embedding) using encoder_q + classifier.\"\"\"\n",
        "    wav = _load_and_fix(path)               # [T]\n",
        "    x = wav.unsqueeze(0).to(device)         # [1,T]\n",
        "    encoder.eval(); classifier.eval()\n",
        "    with torch.no_grad():\n",
        "        feats = encoder(x)\n",
        "        if feats.dim() == 3:\n",
        "            feats = feats.mean(dim=2)\n",
        "        elif feats.dim() > 3:\n",
        "            feats = feats.mean(dim=tuple(range(2, feats.dim())))\n",
        "        logits = classifier(feats)          # [1,2]\n",
        "        probs = F.softmax(logits, dim=1)[0].detach().cpu().numpy()\n",
        "    label = \"fake\" if int(np.argmax(probs)) == 1 else \"real\"\n",
        "    return label, float(probs[1]), float(probs[0]), feats.squeeze(0).cpu().numpy()\n"
      ],
      "metadata": {
        "id": "7NSmsscAYgQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 12\n",
        "# import torch\n",
        "# import torchaudio\n",
        "# import numpy as np\n",
        "# import torch.nn.functional as F\n",
        "# from pathlib import Path\n",
        "\n",
        "# # Set device (important!)\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# # Function to load audio as 16k mono\n",
        "# def load_audio_16k_mono(path: Path):\n",
        "#     \"\"\"Load an audio file as mono 16kHz.\"\"\"\n",
        "#     waveform, sr = torchaudio.load(str(path))\n",
        "#     if waveform.shape[0] > 1:\n",
        "#         waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "#     waveform = waveform.squeeze().numpy()\n",
        "#     if sr != 16000:\n",
        "#         resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
        "#         waveform = resampler(torch.tensor(waveform).unsqueeze(0)).squeeze().numpy()\n",
        "#     return waveform, 16000\n",
        "\n",
        "# # Function to run prediction\n",
        "# def predict_file(path: Path):\n",
        "#     \"\"\"Run CLAD on one WAV file. Returns (pred_label, fake_conf, real_conf).\"\"\"\n",
        "#     y, sr = load_audio_16k_mono(path)\n",
        "#     x = torch.from_numpy(y).float().unsqueeze(0).unsqueeze(1).to(device)  # [B=1, C=1, T]\n",
        "#     with torch.no_grad():\n",
        "#         logits = model(x)  # expected shape [B, 2] for [real, fake]\n",
        "#         probs  = F.softmax(logits, dim=1)[0].detach().cpu().numpy()\n",
        "#     label = \"fake\" if int(np.argmax(probs)) == 1 else \"real\"\n",
        "#     fake_conf = float(probs[1])\n",
        "#     real_conf = float(probs[0])\n",
        "#     return label, fake_conf, real_conf\n"
      ],
      "metadata": {
        "id": "p33_bM57LfTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #13\n",
        "# # This cell scans your external real/fake folders, runs CLAD, and writes a CSV with results.\n",
        "# # It never writes inside the CLAD repo.\n",
        "# import torch\n",
        "# from pathlib import Path\n",
        "# import os\n",
        "# # os.chdir('/content/CLAD') # No need to change directory here for prediction\n",
        "\n",
        "# import glob\n",
        "# import pandas as pd\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# # Assuming 'model', 'classifier', and 'device' are defined in previous cells\n",
        "# # (Specifically, 'model' from loading CLAD and 'classifier' from fine-tuning)\n",
        "# if 'model' not in globals() or 'classifier' not in globals() or 'device' not in globals():\n",
        "#      raise RuntimeError(\"CLAD model, classifier, or device not found. Please run the preceding cells first.\")\n",
        "\n",
        "# # Ensure model and classifier are on the correct device and in eval mode\n",
        "# model.to(device).eval()\n",
        "# classifier.to(device).eval()\n",
        "\n",
        "# # Function to load audio as 16k mono and preprocess\n",
        "# def load_and_preprocess_audio(path: Path):\n",
        "#     \"\"\"Load an audio file, resample to 16kHz mono, pad/trim to TARGET_LEN.\"\"\"\n",
        "#     wav, sr = torchaudio.load(str(path))   # [C,T]\n",
        "#     wav = wav.float()\n",
        "#     if wav.shape[0] > 1:\n",
        "#         wav = wav.mean(dim=0, keepdim=True)   # [1,T]\n",
        "#     if sr != TARGET_SR:\n",
        "#         res = torchaudio.transforms.Resample(orig_freq=sr, new_freq=TARGET_SR)\n",
        "#         wav = res(wav)                         # [1,T']\n",
        "#     T = wav.shape[-1]\n",
        "#     if T < TARGET_LEN:\n",
        "#         wav = F.pad(wav, (0, TARGET_LEN - T))\n",
        "#     else:\n",
        "#         wav = wav[..., :TARGET_LEN]\n",
        "#     return wav.squeeze(0) # -> [T]\n",
        "\n",
        "\n",
        "# # Function to run prediction\n",
        "# def predict_file(path: Path, encoder, classifier, device):\n",
        "#     \"\"\"Run CLAD encoder + classifier on one WAV file. Returns (pred_label, fake_conf, real_conf).\"\"\"\n",
        "#     try:\n",
        "#         wav = load_and_preprocess_audio(path) # [T]\n",
        "#         x = wav.unsqueeze(0).to(device)       # [B=1, T]\n",
        "\n",
        "#         with torch.no_grad():\n",
        "#             # Use only the encoder part of the MoCo model\n",
        "#             feats = encoder(x) # RawNet expects [B, T]\n",
        "#             # Pool any extra time/freq dims if necessary\n",
        "#             if feats.dim() == 3:\n",
        "#                 feats = feats.mean(dim=2)\n",
        "#             elif feats.dim() > 3:\n",
        "#                  feats = feats.mean(dim=tuple(range(2, feats.dim())))\n",
        "\n",
        "#             # Pass features through the classifier\n",
        "#             logits = classifier(feats)        # [B=1, 2]\n",
        "#             probs  = F.softmax(logits, dim=1)[0].detach().cpu().numpy()\n",
        "\n",
        "#         label = \"fake\" if int(np.argmax(probs)) == 1 else \"real\"\n",
        "#         fake_conf = float(probs[1])\n",
        "#         real_conf = float(probs[0])\n",
        "#         return label, fake_conf, real_conf, feats.squeeze(0).cpu().numpy() # Also return embedding\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error processing file {path}: {e}\")\n",
        "#         return \"error\", 0.0, 0.0, None # Return None for embedding on error\n",
        "\n",
        "\n",
        "# # Function to extract embedding (optional)\n",
        "# def embed_file(path: Path, encoder, device):\n",
        "#     \"\"\"Extract embedding from CLAD encoder for one WAV file.\"\"\"\n",
        "#     try:\n",
        "#         wav = load_and_preprocess_audio(path) # [T]\n",
        "#         x = wav.unsqueeze(0).to(device)       # [B=1, T]\n",
        "#         with torch.no_grad():\n",
        "#             feats = encoder(x) # RawNet expects [B, T]\n",
        "#             # Pool any extra time/freq dims if necessary\n",
        "#             if feats.dim() == 3:\n",
        "#                 feats = feats.mean(dim=2)\n",
        "#             elif feats.dim() > 3:\n",
        "#                  feats = feats.mean(dim=tuple(range(2, feats.dim())))\n",
        "#         return feats.squeeze(0).cpu().numpy()\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error embedding file {path}: {e}\")\n",
        "#         return None\n",
        "\n",
        "\n",
        "# REAL_DIR = Path(\"/content/my_audio/real\")\n",
        "# FAKE_DIR = Path(\"/content/my_audio/fake\")\n",
        "# OUT_DIR  = Path(\"/content/my_audio_results\")\n",
        "# OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# real_files = sorted(REAL_DIR.rglob(\"*.wav\"))\n",
        "# fake_files = sorted(FAKE_DIR.rglob(\"*.wav\"))\n",
        "\n",
        "# print(f\"Found {len(real_files)} real and {len(fake_files)} fake WAVs.\")\n",
        "\n",
        "# rows = []\n",
        "# for f in real_files + fake_files:\n",
        "#     p = Path(f)\n",
        "#     # Pass encoder_q, classifier, and device to the prediction function\n",
        "#     pred, fake_conf, real_conf, emb = predict_file(p, model.encoder_q, classifier, device)\n",
        "#     rows.append({\n",
        "#         \"file\": str(p),\n",
        "#         \"pred\": pred,\n",
        "#         \"fake_conf\": fake_conf,\n",
        "#         \"real_conf\": real_conf,\n",
        "#         \"embedding_dim\": (len(emb) if emb is not None else None),\n",
        "#         \"embedding\": emb.tolist() if emb is not None else None # Convert numpy array to list for CSV\n",
        "#     })\n",
        "\n",
        "# df = pd.DataFrame(rows)\n",
        "# csv_path = OUT_DIR / \"clad_results.csv\"\n",
        "# df.to_csv(csv_path, index=False)\n",
        "# print(\"Saved CSV:\", csv_path)\n",
        "# display(df.head())"
      ],
      "metadata": {
        "id": "Fd6lt4Wyh9lJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#14\n",
        "# This cell is optional. It visualizes embeddings in 2D if they were extracted.\n",
        "# If embedding is None for all files (no encoder exposed), skip this cell.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "emb_rows = [r for r in rows if r[\"embedding\"] is not None]\n",
        "if len(emb_rows) >= 2:\n",
        "    X = np.vstack([np.array(r[\"embedding\"], dtype=np.float32) for r in emb_rows])\n",
        "    y = np.array([r[\"pred\"] for r in emb_rows])\n",
        "\n",
        "    X2d = TSNE(n_components=2, random_state=0, perplexity=min(15, len(emb_rows)-1)).fit_transform(X)\n",
        "\n",
        "    plt.figure(figsize=(6,5))\n",
        "    for cls, marker in [(\"real\", \"o\"), (\"fake\", \"x\")]:\n",
        "        mask = (y == cls)\n",
        "        plt.scatter(X2d[mask,0], X2d[mask,1], label=cls, marker=marker, alpha=0.85)\n",
        "    plt.title(\"CLAD embeddings (t-SNE)\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No embeddings available (encoder not exposed or too few samples).\")"
      ],
      "metadata": {
        "id": "S5TtL4-qiBNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hSsZUxPxmkeF"
      }
    }
  ]
}