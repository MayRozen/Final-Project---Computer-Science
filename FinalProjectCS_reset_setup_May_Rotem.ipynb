{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNnA0Nxx4ghf"
      },
      "source": [
        "# Project Setup (Colab)\n",
        "\n",
        "Run these cells from top to bottom to build a stable, reproducible environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWi7MCSN4ghg"
      },
      "outputs": [],
      "source": [
        "# ======================================\n",
        "# üîß INSTALL DEPENDENCIES (run once, then restart kernel if prompted)\n",
        "# ======================================\n",
        "%%capture\n",
        "%pip install -q --force-reinstall     numpy==1.26.4     scipy==1.13.1     torch==2.4.1     torchaudio==2.4.1     coqui-tts==0.23.1     pandas==2.2.3     matplotlib==3.9.2     scikit-learn==1.5.2     tqdm==4.66.5\n",
        "\n",
        "# After running this cell, restart the runtime by going to \"Runtime\" -> \"Restart session\" in the Colab menu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "u757FgZcRLNr",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# !pip uninstall -y numpy pandas scipy\n",
        "# !pip install --no-cache-dir --force-reinstall numpy==1.26.4 pandas==2.2.3 scipy==1.13.1\n",
        "# !pip install coqui-tts==0.23.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fbjc4yul4ghh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec1584c7-b268-4b5f-baf2-7e1cb8a7d27a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jieba/__init__.py:44: SyntaxWarning: invalid escape sequence '\\.'\n",
            "  re_han_default = re.compile(\"([\\u4E00-\\u9FD5a-zA-Z0-9+#&\\._%\\-]+)\", re.U)\n",
            "/usr/local/lib/python3.12/dist-packages/jieba/__init__.py:46: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  re_skip_default = re.compile(\"(\\r\\n|\\s)\", re.U)\n",
            "/usr/local/lib/python3.12/dist-packages/jieba/finalseg/__init__.py:78: SyntaxWarning: invalid escape sequence '\\.'\n",
            "  re_skip = re.compile(\"([a-zA-Z0-9]+(?:\\.\\d+)?%?)\")\n"
          ]
        }
      ],
      "source": [
        "# ======================================\n",
        "# üì¶ IMPORT LIBRARIES\n",
        "# ======================================\n",
        "import os, sys, glob, random, shutil, csv, itertools, threading, platform, importlib\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from collections import defaultdict\n",
        "\n",
        "# Core scientific stack\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Machine learning / audio\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Coqui Text-to-Speech\n",
        "from TTS.api import TTS\n",
        "\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "u4lUUUhj4ghh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cebe852b-1607-47c1-ff99-4f905ce82f42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed set to 42\n"
          ]
        }
      ],
      "source": [
        "# ======================================\n",
        "# üéØ REPRODUCIBILITY (Seed everything)\n",
        "# ======================================\n",
        "import random\n",
        "import numpy as _np\n",
        "import torch as _torch\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "_np.random.seed(SEED)\n",
        "_torch.manual_seed(SEED)\n",
        "if _torch.cuda.is_available():\n",
        "    _torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "print(f\"Seed set to {SEED}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vS6XSpNd4ghh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "580e8274-5b58-4338-b1ea-5e5a7c0d1381"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment is ready!\n",
            "Python: 3.12.12\n",
            "NumPy: 1.26.4 | SciPy: 1.13.1\n",
            "Torch: 2.8.0+cu126 | Torchaudio: 2.8.0+cu126\n",
            "Pandas: 2.2.3\n",
            "CUDA available: True\n",
            "CUDA device: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# ======================================\n",
        "# ‚úÖ ENVIRONMENT CHECK\n",
        "# ======================================\n",
        "import platform\n",
        "print(\"Environment is ready!\")\n",
        "print(f\"Python: {platform.python_version()}\")\n",
        "print(f\"NumPy: {np.__version__} | SciPy: {scipy.__version__}\")\n",
        "print(f\"Torch: {torch.__version__} | Torchaudio: {torchaudio.__version__}\")\n",
        "print(f\"Pandas: {pd.__version__}\")\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA device:\", torch.cuda.get_device_name(0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fwvECPac4ghi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a423a854-e4a8-4102-e977-60f34a26fafc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Saved lockfile to: /content/requirements_lock.txt\n",
            "Saved environment info to: /content/env_lock.json\n",
            "Also copied to Drive: /content/drive/MyDrive/ColabEnvLocks\n"
          ]
        }
      ],
      "source": [
        "# ======================================\n",
        "# üßä FREEZE ENVIRONMENT (lock file + system info)\n",
        "# ======================================\n",
        "import os, json, platform\n",
        "import numpy as _numpy\n",
        "import scipy as _scipy\n",
        "import torch as _torch\n",
        "import pandas as _pandas\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "LOCK_TXT = \"/content/requirements_lock.txt\"\n",
        "LOCK_JSON = \"/content/env_lock.json\"\n",
        "\n",
        "# Freeze exact package versions\n",
        "!pip freeze > \"$LOCK_TXT\"\n",
        "\n",
        "# Save system info + core libs versions\n",
        "env_info = {\n",
        "    \"python\": platform.python_version(),\n",
        "    \"platform\": platform.platform(),\n",
        "    \"cuda_available\": _torch.cuda.is_available(),\n",
        "    \"cuda_device\": (_torch.cuda.get_device_name(0) if _torch.cuda.is_available() else None),\n",
        "    \"versions\": {\n",
        "        \"numpy\": _numpy.__version__,\n",
        "        \"scipy\": _scipy.__version__,\n",
        "        \"torch\": _torch.__version__,\n",
        "        \"pandas\": _pandas.__version__,\n",
        "    }\n",
        "}\n",
        "with open(LOCK_JSON, \"w\") as f:\n",
        "    json.dump(env_info, f, indent=2)\n",
        "\n",
        "print(f\"Saved lockfile to: {LOCK_TXT}\")\n",
        "print(f\"Saved environment info to: {LOCK_JSON}\")\n",
        "\n",
        "# If Drive is mounted, also copy there for persistence\n",
        "drive_base = \"/content/drive/MyDrive/ColabEnvLocks\"\n",
        "if os.path.exists(\"/content/drive\"):\n",
        "    os.makedirs(drive_base, exist_ok=True)\n",
        "    !cp -f \"$LOCK_TXT\" \"$drive_base/requirements_lock.txt\"\n",
        "    !cp -f \"$LOCK_JSON\" \"$drive_base/env_lock.json\"\n",
        "    print(f\"Also copied to Drive: {drive_base}\")\n",
        "else:\n",
        "    print(\"Google Drive is not mounted; skipping Drive backup.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4Ep1xRDk4ghi"
      },
      "outputs": [],
      "source": [
        "# ======================================\n",
        "# üîÅ RESTORE ENV FROM LOCK (use on fresh runtimes)\n",
        "# ======================================\n",
        "%%capture\n",
        "# Prefer Drive lock if available, else local\n",
        "LOCK_TXT = \"/content/drive/MyDrive/ColabEnvLocks/requirements_lock.txt\"\n",
        "FALLBACK_LOCK = \"/content/requirements_lock.txt\"\n",
        "import os\n",
        "lock_to_use = LOCK_TXT if os.path.exists(LOCK_TXT) else FALLBACK_LOCK\n",
        "print(f\"Installing from lock: {lock_to_use}\")\n",
        "%pip install -q --no-deps -r \"$lock_to_use\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3fq-YA5E4ghi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "710197f7-1cc7-497d-9fa3-4d657585110d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/TTS/api.py:71: UserWarning: `gpu` will be deprecated. Please use `tts.to(device)` instead.\n",
            "  warnings.warn(\"`gpu` will be deprecated. Please use `tts.to(device)` instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model ready: tts_models/en/ljspeech/tacotron2-DDC\n",
            "Drive detected. Consider syncing ~/.local/share/tts to Drive for full persistence.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ======================================\n",
        "# ‚¨áÔ∏è OPTIONAL: PRE-DOWNLOAD TTS MODEL WEIGHTS (persist to Drive if mounted)\n",
        "# ======================================\n",
        "from TTS.api import TTS\n",
        "import os\n",
        "\n",
        "MODEL_NAME = \"tts_models/en/ljspeech/tacotron2-DDC\"  # change if you need a different model\n",
        "LOCAL_DIR = \"/content/models/tts\"\n",
        "os.makedirs(LOCAL_DIR, exist_ok=True)\n",
        "\n",
        "# Instantiate once to trigger download into cache; also synthesize a tiny file to ensure weights are present\n",
        "tts = TTS(model_name=MODEL_NAME, progress_bar=False, gpu=torch.cuda.is_available())\n",
        "tts.tts_to_file(text=\"setup\", file_path=f\"{LOCAL_DIR}/_warmup.wav\")\n",
        "print(\"Model ready:\", MODEL_NAME)\n",
        "\n",
        "# If Drive is mounted, copy cache for persistence\n",
        "if os.path.exists(\"/content/drive\"):\n",
        "    DRIVE_DIR = \"/content/drive/MyDrive/ColabModels/tts\"\n",
        "    os.makedirs(DRIVE_DIR, exist_ok=True)\n",
        "    print(\"Drive detected. Consider syncing ~/.local/share/tts to Drive for full persistence.\")\n",
        "else:\n",
        "    print(\"Drive not mounted ‚Äî model will be cached only in this runtime.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "K4MjHTZC4ghi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8f097f2-8a81-46c4-d69d-48048ba46753"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done. Files in data dir:\n"
          ]
        }
      ],
      "source": [
        "# ======================================\n",
        "# üì¶ OPTIONAL: EXTRACT ALL ZIP DATASETS IN /content\n",
        "# ======================================\n",
        "import zipfile, glob, os\n",
        "DATA_DIR = \"/content/data\"\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "zips = glob.glob(\"/content/*.zip\")\n",
        "for z in zips:\n",
        "    print(\"Extracting:\", z)\n",
        "    with zipfile.ZipFile(z, 'r') as zip_ref:\n",
        "        zip_ref.extractall(DATA_DIR)\n",
        "\n",
        "print(\"Done. Files in data dir:\")\n",
        "for root, dirs, files in os.walk(DATA_DIR):\n",
        "    for f in files[:50]:\n",
        "        print(os.path.join(root, f))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeCXF8rG9JdE"
      },
      "source": [
        "# extract_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsB_qTqinrA3"
      },
      "outputs": [],
      "source": [
        "# import shutil\n",
        "# import os\n",
        "\n",
        "# # ◊î◊†◊™◊ô◊ë ◊ú◊™◊ô◊ß◊ô◊ô◊î ◊©◊†◊ï◊¶◊®◊î ◊ë◊î◊®◊¶◊î ◊î◊ß◊ï◊ì◊û◊™\n",
        "# destination_folder = \"/content/vctk_full\"\n",
        "\n",
        "# # ◊ë◊ì◊ô◊ß◊î ◊ê◊ù ◊î◊™◊ô◊ß◊ô◊ô◊î ◊ß◊ô◊ô◊û◊™, ◊ï◊ê◊ñ ◊û◊ó◊ô◊ß◊î\n",
        "# if os.path.exists(destination_folder):\n",
        "#     shutil.rmtree(destination_folder)\n",
        "#     print(f\" ◊î◊™◊ô◊ß◊ô◊ô◊î '{destination_folder}' ◊†◊û◊ó◊ß◊î ◊ë◊î◊¶◊ú◊ó◊î.\")\n",
        "# else:\n",
        "#     print(f\"‚Ñπ ◊î◊™◊ô◊ß◊ô◊ô◊î '{destination_folder}' ◊ú◊ê ◊ß◊ô◊ô◊û◊™, ◊ê◊ô◊ü ◊û◊î ◊ú◊û◊ó◊ï◊ß.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9YxlgeQE9ufN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d74314ef-cb43-4be4-d9fd-ded8a26ced58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Contents of 'My Drive': ['Colab Notebooks', 'Deep learning', 'ColabEnvLocks', 'ColabModels', 'Colab_Data']\n",
            "Contents of 'Colab Notebooks': ['archive.zip', 'fake_audio.zip', 'Copy of Welcome To Colab', 'Untitled0.ipynb', 'Untitled1.ipynb', 'Untitled2.ipynb', 'FinalProjectCS.ipynb', 'FinalProjectCS_reset_setup_May_Rotem.ipynb']\n",
            " ZIP file found: /content/drive/My Drive/Colab Notebooks/archive.zip\n",
            "\n",
            " Extraction complete: 2684 files were extracted.\n",
            " Extracted data is available in: /content/vctk_samples\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# List contents to verify paths (optional)\n",
        "root_path = '/content/drive/My Drive/'\n",
        "print(\"Contents of 'My Drive':\", os.listdir(root_path))\n",
        "\n",
        "subfolder_path = '/content/drive/My Drive/Colab Notebooks/'\n",
        "print(\"Contents of 'Colab Notebooks':\", os.listdir(subfolder_path))\n",
        "\n",
        "# Define paths\n",
        "zip_file = \"/content/drive/My Drive/Colab Notebooks/archive.zip\"  # Path to your ZIP file\n",
        "destination_folder = \"/content/vctk_samples\"  # Where to extract selected data\n",
        "wanted_speakers = [\"p225\", \"p226\", \"p227\", \"p228\"]  # Select specific speakers\n",
        "\n",
        "# Check if the ZIP file exists\n",
        "if os.path.isfile(zip_file):\n",
        "    print(\" ZIP file found:\", zip_file)\n",
        "else:\n",
        "    raise FileNotFoundError(f\" ZIP file not found: {zip_file}\")\n",
        "\n",
        "# Create destination folder if it doesn't exist\n",
        "os.makedirs(destination_folder, exist_ok=True)\n",
        "\n",
        "# Selectively extract only desired speaker folders from the ZIP\n",
        "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "    extracted_files = 0\n",
        "    for file in zip_ref.namelist():\n",
        "        if any(f\"VCTK-Corpus/wav48/{spk}/\" in file or f\"VCTK-Corpus/txt/{spk}/\" in file for spk in wanted_speakers):\n",
        "            # Ensure directory structure is preserved\n",
        "            target_path = os.path.join(destination_folder, file)\n",
        "            os.makedirs(os.path.dirname(target_path), exist_ok=True)\n",
        "            with zip_ref.open(file) as source, open(target_path, 'wb') as target:\n",
        "                shutil.copyfileobj(source, target)\n",
        "            extracted_files += 1\n",
        "\n",
        "print(f\"\\n Extraction complete: {extracted_files} files were extracted.\")\n",
        "print(f\" Extracted data is available in: {destination_folder}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zB_QfTQcEA-G"
      },
      "source": [
        "# Installations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": true,
        "id": "nymZzYIfrL7N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2614d20-0a8e-4e5b-ff2d-8fd620046ca8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  espeak-ng-data libespeak-ng1 libpcaudio0 libsonic0\n",
            "The following NEW packages will be installed:\n",
            "  espeak-ng espeak-ng-data libespeak-ng1 libpcaudio0 libsonic0\n",
            "0 upgraded, 5 newly installed, 0 to remove and 38 not upgraded.\n",
            "Need to get 4,526 kB of archives.\n",
            "After this operation, 11.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libpcaudio0 amd64 1.1-6build2 [8,956 B]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsonic0 amd64 0.2.0-11build1 [10.3 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 espeak-ng-data amd64 1.50+dfsg-10ubuntu0.1 [3,956 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libespeak-ng1 amd64 1.50+dfsg-10ubuntu0.1 [207 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 espeak-ng amd64 1.50+dfsg-10ubuntu0.1 [343 kB]\n",
            "Fetched 4,526 kB in 2s (2,248 kB/s)\n",
            "Selecting previously unselected package libpcaudio0:amd64.\n",
            "(Reading database ... 126718 files and directories currently installed.)\n",
            "Preparing to unpack .../libpcaudio0_1.1-6build2_amd64.deb ...\n",
            "Unpacking libpcaudio0:amd64 (1.1-6build2) ...\n",
            "Selecting previously unselected package libsonic0:amd64.\n",
            "Preparing to unpack .../libsonic0_0.2.0-11build1_amd64.deb ...\n",
            "Unpacking libsonic0:amd64 (0.2.0-11build1) ...\n",
            "Selecting previously unselected package espeak-ng-data:amd64.\n",
            "Preparing to unpack .../espeak-ng-data_1.50+dfsg-10ubuntu0.1_amd64.deb ...\n",
            "Unpacking espeak-ng-data:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Selecting previously unselected package libespeak-ng1:amd64.\n",
            "Preparing to unpack .../libespeak-ng1_1.50+dfsg-10ubuntu0.1_amd64.deb ...\n",
            "Unpacking libespeak-ng1:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Selecting previously unselected package espeak-ng.\n",
            "Preparing to unpack .../espeak-ng_1.50+dfsg-10ubuntu0.1_amd64.deb ...\n",
            "Unpacking espeak-ng (1.50+dfsg-10ubuntu0.1) ...\n",
            "Setting up libpcaudio0:amd64 (1.1-6build2) ...\n",
            "Setting up libsonic0:amd64 (0.2.0-11build1) ...\n",
            "Setting up espeak-ng-data:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Setting up libespeak-ng1:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Setting up espeak-ng (1.50+dfsg-10ubuntu0.1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y espeak-ng"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcjKFlnIKvFL"
      },
      "source": [
        "# Extracting the fake audio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating Fake Audio (No Need to run now)"
      ],
      "metadata": {
        "id": "Is4mT-8ymjMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# from pathlib import Path\n",
        "# from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "# import threading\n",
        "# import torch\n",
        "\n",
        "# from TTS.api import TTS\n",
        "\n",
        "# # =========================\n",
        "# # CONFIG\n",
        "# # =========================\n",
        "# # 1) Input text files (one .txt per utterance)\n",
        "# TEXT_ROOT = Path(\"/content/vctk_samples/VCTK-Corpus/VCTK-Corpus/txt\")\n",
        "\n",
        "# # 2) Real audio folder (matching real WAVs you want to clone the voice from)\n",
        "# #    The script will try to find a matching WAV by the text filename stem inside the corresponding subfolder.\n",
        "# #    Example: If text is \".../p228/p228_065.txt\", it will try \"/.../real_audio_folder/p228/p228_065.wav\"\n",
        "# REAL_AUDIO_ROOT = Path(\"/content/vctk_samples/VCTK-Corpus/VCTK-Corpus/wav48\")  # <-- CHANGE if needed\n",
        "\n",
        "# # 3) Output folder for fake audio\n",
        "# OUT_ROOT = Path(\"/content/vctk_samples/VCTK-Corpus/VCTK-Corpus/fake_wav48_xtts\")\n",
        "# OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# # 4) Speaker metadata (for bookkeeping only; XTTS uses speaker_wav for actual voice)\n",
        "# SPEAKER_INFO = {\n",
        "#     \"p225\": (\"F\", \"22\", \"Southern England\"),\n",
        "#     \"p226\": (\"M\", \"22\", \"Surrey\"),\n",
        "#     \"p227\": (\"M\", \"38\", \"Cumbria\"),\n",
        "#     \"p228\": (\"F\", \"22\", \"Southern England\"),\n",
        "# }\n",
        "\n",
        "# # 5) Language to synthesize in (VCTK is English)\n",
        "# LANGUAGE = \"en\"\n",
        "\n",
        "# # 6) Concurrency settings:\n",
        "# #    On GPU, keep MAX_WORKERS=1 (XTTS is heavy and not thread-safe on CUDA).\n",
        "# #    On CPU, you can increase to 4 (or more if your machine can handle it).\n",
        "# GPU_AVAILABLE = torch.cuda.is_available()\n",
        "# MAX_WORKERS = 1 if GPU_AVAILABLE else 4\n",
        "\n",
        "# # =========================\n",
        "# # MODEL LOADING\n",
        "# # =========================\n",
        "# # Use XTTS v2 for voice cloning with a reference WAV.\n",
        "# # (We also lazy-load a VCTK-VITS fallback if a real WAV is missing.)\n",
        "# print(\"Loading XTTS v2 model...\")\n",
        "# tts_xtts = TTS(model_name=\"tts_models/multilingual/multi-dataset/xtts_v2\", progress_bar=True)\n",
        "# device = \"cuda\" if GPU_AVAILABLE else \"cpu\"\n",
        "# tts_xtts.to(device)\n",
        "# print(f\"XTTS is running on: {device}\")\n",
        "\n",
        "# # Fallback multi-speaker model (only used when no real WAV is found)\n",
        "# print(\"Loading VCTK-VITS fallback model...\")\n",
        "# tts_vctk = TTS(model_name=\"tts_models/en/vctk/vits\", progress_bar=False)\n",
        "# tts_vctk.to(device)\n",
        "\n",
        "# # Mutex for model calls if you insist on >1 threads on CPU (XTTS is heavy; serialize calls by default on GPU).\n",
        "# synth_lock = threading.Lock() if MAX_WORKERS > 1 else None\n",
        "\n",
        "# # =========================\n",
        "# # HELPERS\n",
        "# # =========================\n",
        "# def to_vctk_id(s: str) -> str:\n",
        "#     \"\"\"Convert '228' -> 'p228' for VCTK-style speaker IDs.\"\"\"\n",
        "#     s = s.strip()\n",
        "#     return s if s.startswith(\"p\") else f\"p{s}\"\n",
        "\n",
        "# def find_matching_real_wav(real_root: Path, subdir: str, txt_filename: str) -> Path | None:\n",
        "#     \"\"\"\n",
        "#     Try to find the real WAV that matches the text file.\n",
        "#     Strategy:\n",
        "#       1) exact same stem under REAL_AUDIO_ROOT/subdir:  <stem>.wav\n",
        "#       2) any .wav in subdir that contains the stem (fallback)\n",
        "#       3) final fallback: None\n",
        "#     \"\"\"\n",
        "#     stem = Path(txt_filename).stem  # e.g., 'p228_065' or '228_065'\n",
        "#     # Common VCTK stems look like 'p228_065'. If it's numeric-only, normalize:\n",
        "#     parts = stem.split(\"_\")\n",
        "#     if parts and not parts[0].startswith(\"p\"):\n",
        "#         parts[0] = \"p\" + parts[0]\n",
        "#     norm_stem = \"_\".join(parts)\n",
        "\n",
        "#     cand1 = real_root / subdir / f\"{norm_stem}.wav\"\n",
        "#     if cand1.exists():\n",
        "#         return cand1\n",
        "\n",
        "#     # Try exactly the original stem (if it already had 'p')\n",
        "#     cand2 = real_root / subdir / f\"{stem}.wav\"\n",
        "#     if cand2.exists():\n",
        "#         return cand2\n",
        "\n",
        "#     # Fallback: search within subdir for anything containing norm_stem or the raw stem\n",
        "#     subdir_path = real_root / subdir\n",
        "#     if subdir_path.is_dir():\n",
        "#         for fn in os.listdir(subdir_path):\n",
        "#             if not fn.lower().endswith(\".wav\"):\n",
        "#                 continue\n",
        "#             if norm_stem in fn or stem in fn:\n",
        "#                 return subdir_path / fn\n",
        "\n",
        "#     return None\n",
        "\n",
        "# def synth_xtts(text: str, speaker_wav: Path, out_path: Path, language: str = \"en\"):\n",
        "#     \"\"\"\n",
        "#     Synthesize with XTTS v2 using a reference speaker WAV. This is the key for high voice similarity.\n",
        "#     \"\"\"\n",
        "#     # Serialize heavy GPU calls if needed\n",
        "#     if synth_lock:\n",
        "#         with synth_lock:\n",
        "#             tts_xtts.tts_to_file(text=text, file_path=str(out_path), speaker_wav=str(speaker_wav), language=language)\n",
        "#     else:\n",
        "#         tts_xtts.tts_to_file(text=text, file_path=str(out_path), speaker_wav=str(speaker_wav), language=language)\n",
        "\n",
        "# def synth_vctk(text: str, speaker_id: str, out_path: Path):\n",
        "#     \"\"\"\n",
        "#     Fallback synthesis with VCTK-VITS multi-speaker model (uses 'p###' speakers).\n",
        "#     \"\"\"\n",
        "#     if synth_lock:\n",
        "#         with synth_lock:\n",
        "#             tts_vctk.tts_to_file(text=text, speaker=speaker_id, file_path=str(out_path))\n",
        "#     else:\n",
        "#         tts_vctk.tts_to_file(text=text, speaker=speaker_id, file_path=str(out_path))\n",
        "\n",
        "# def process_one(text_path: Path, out_subdir: Path):\n",
        "#     \"\"\"\n",
        "#     Process a single text file:\n",
        "#       - Read text\n",
        "#       - Resolve speaker_id from filename (for metadata/fallback)\n",
        "#       - Find matching real WAV\n",
        "#       - Prefer XTTS cloning; fallback to VCTK-VITS speaker if real WAV missing\n",
        "#     \"\"\"\n",
        "#     text = text_path.read_text(encoding=\"utf-8\").strip()\n",
        "#     if not text:\n",
        "#         return f\"[SKIP] Empty text: {text_path.name}\"\n",
        "\n",
        "#     # Resolve speaker id from the filename (e.g., 'p228' from 'p228_065.txt')\n",
        "#     raw_id = text_path.stem.split(\"_\")[0]          # 'p228' or '228'\n",
        "#     speaker_id = to_vctk_id(raw_id)                # 'p228'\n",
        "\n",
        "#     # Metadata (for filename only)\n",
        "#     gender, age, accent = SPEAKER_INFO.get(speaker_id, (\"F\", \"22\", \"Southern England\"))\n",
        "\n",
        "#     # Try to find the matching real wav in REAL_AUDIO_ROOT/<subdir>/\n",
        "#     subdir = text_path.parent.name\n",
        "#     real_wav = find_matching_real_wav(REAL_AUDIO_ROOT, subdir, text_path.name)\n",
        "\n",
        "#     # Build output path (include metadata in filename)\n",
        "#     out_name = f\"{text_path.stem}__{speaker_id}__{gender}_{age}_{accent}.wav\"\n",
        "#     out_path = out_subdir / out_name\n",
        "\n",
        "#     # Prefer XTTS cloning if real wav exists; otherwise fallback to VCTK-VITS speaker\n",
        "#     if real_wav and real_wav.exists():\n",
        "#         msg = f\"[XTTS] {text_path.name} -> clone from {real_wav.name} -> {out_name}\"\n",
        "#         synth_xtts(text=text, speaker_wav=real_wav, out_path=out_path, language=LANGUAGE)\n",
        "#         return msg\n",
        "#     else:\n",
        "#         msg = f\"[FALLBACK VCTK] {text_path.name} -> speaker={speaker_id} -> {out_name}\"\n",
        "#         synth_vctk(text=text, speaker_id=speaker_id, out_path=out_path)\n",
        "#         return msg\n",
        "\n",
        "# # =========================\n",
        "# # BUILD JOBS\n",
        "# # =========================\n",
        "# jobs = []\n",
        "# for subdir in os.listdir(TEXT_ROOT):\n",
        "#     subdir_path = TEXT_ROOT / subdir\n",
        "#     if not subdir_path.is_dir():\n",
        "#         continue\n",
        "\n",
        "#     out_subdir = OUT_ROOT / subdir\n",
        "#     out_subdir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "#     for fn in os.listdir(subdir_path):\n",
        "#         if not fn.lower().endswith(\".txt\"):\n",
        "#             continue\n",
        "#         jobs.append((subdir_path / fn, out_subdir))\n",
        "\n",
        "# print(f\"Found {len(jobs)} text files.\")\n",
        "\n",
        "# # =========================\n",
        "# # RUN\n",
        "# # =========================\n",
        "# if not jobs:\n",
        "#     print(\"No jobs found. Check TEXT_ROOT.\")\n",
        "# else:\n",
        "#     print(f\"Starting synthesis with MAX_WORKERS={MAX_WORKERS} (device={device})\")\n",
        "#     with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
        "#         futs = [ex.submit(process_one, text_path, out_dir) for (text_path, out_dir) in jobs]\n",
        "#         for fut in as_completed(futs):\n",
        "#             try:\n",
        "#                 info = fut.result()\n",
        "#                 print(info)\n",
        "#             except Exception as e:\n",
        "#                 print(\"[ERROR]\", repr(e))\n",
        "\n",
        "# print(\"Done. Fake audio saved under:\", OUT_ROOT)\n"
      ],
      "metadata": {
        "id": "BJKIKc_hmeNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Qzof7wJ8kHse",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdfafa13-15bd-4b7a-ee89-cc82d4af9b2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ZIP file found: /content/drive/My Drive/Colab Notebooks/fake_audio.zip\n",
            "Extracted 1342 files for speakers: ['p225', 'p226', 'p227', 'p228']\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "# Define paths\n",
        "zip_file = \"/content/drive/My Drive/Colab Notebooks/fake_audio.zip\"  # Path to your ZIP file\n",
        "destination_folder = \"/content/vctk_samples/VCTK-Corpus/VCTK-Corpus/fake_audio\"  # Where to extract selected data\n",
        "\n",
        "# Example: define the speakers you want to extract\n",
        "wanted_speakers = [\"p225\", \"p226\",\"p227\",\"p228\"]  # change this list as needed\n",
        "\n",
        "# Check if the ZIP file exists\n",
        "if os.path.isfile(zip_file):\n",
        "\n",
        "    print(\" ZIP file found:\", zip_file)\n",
        "else:\n",
        "    raise FileNotFoundError(f\" ZIP file not found: {zip_file}\")\n",
        "\n",
        "# Create destination folder if it doesn't exist\n",
        "os.makedirs(destination_folder, exist_ok=True)\n",
        "\n",
        "# Selectively extract only desired speaker folders from the ZIP\n",
        "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "    extracted_files = 0\n",
        "    for file in zip_ref.namelist():\n",
        "        if any(f\"{spk}/\" in file for spk in wanted_speakers):\n",
        "            target_path = os.path.join(destination_folder, file)\n",
        "\n",
        "            # If this entry is a directory ‚Üí skip it\n",
        "            if file.endswith('/'):\n",
        "                os.makedirs(target_path, exist_ok=True)\n",
        "                continue\n",
        "\n",
        "            # Ensure directory structure is preserved\n",
        "            os.makedirs(os.path.dirname(target_path), exist_ok=True)\n",
        "\n",
        "            # Copy file content\n",
        "            with zip_ref.open(file) as source, open(target_path, 'wb') as target:\n",
        "                shutil.copyfileobj(source, target)\n",
        "            extracted_files += 1\n",
        "\n",
        "print(f\"Extracted {extracted_files} files for speakers: {wanted_speakers}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HloMV1V7yZ7i"
      },
      "source": [
        "# Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "FU7_7Wq5uCOM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a6c1bce-d54d-42bb-af53-3843ea757324"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-0d68d57d-45e5-a600-6e4a-93db0e751fae)\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi -L || echo \"No GPU\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": true,
        "id": "YaBoZc46f0p_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4abcdcc3-9f4c-4c4e-f627-e3f581a42d12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Updated CLAD/requirements.txt =====\n",
            "llvmlite==0.43.0\n",
            "numba==0.60.0\n",
            "librosa>=0.11.0\n",
            "matplotlib==3.8.4\n",
            "numpy==1.26.4\n",
            "primePy==1.3\n",
            "torchcontrib\n",
            "pytorch_model_summary\n",
            "torchinfoRequirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n",
            "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1.8/1.8 MB 90.4 MB/s eta 0:00:00\n",
            "Installing collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.3\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting torch==2.3.1\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.3.1%2Bcu121-cp312-cp312-linux_x86_64.whl (780.9 MB)\n",
            "     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 780.9/780.9 MB 21.3 MB/s  0:00:17\n",
            "Collecting torchvision==0.18.1\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.18.1%2Bcu121-cp312-cp312-linux_x86_64.whl (7.0 MB)\n",
            "     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 7.0/7.0 MB 121.6 MB/s  0:00:00\n",
            "Collecting torchaudio==2.3.1\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.3.1%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
            "     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3.4/3.4 MB 51.3 MB/s  0:00:00\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (2.8.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 23.7/23.7 MB 166.5 MB/s  0:00:00\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 823.6/823.6 kB 54.6 MB/s  0:00:00\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 14.1/14.1 MB 158.2 MB/s  0:00:00\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 731.7/731.7 MB 45.3 MB/s  0:00:10\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 410.6/410.6 MB 43.0 MB/s  0:00:06\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 121.6/121.6 MB 71.8 MB/s  0:00:01\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 56.5/56.5 MB 73.8 MB/s  0:00:00\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 124.2/124.2 MB 71.7 MB/s  0:00:01\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 196.0/196.0 MB 22.1 MB/s  0:00:08\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 176.2/176.2 MB 74.7 MB/s  0:00:02\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.1)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.18.1) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.18.1) (11.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.1) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.3.1) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.3.1) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.8.0+cu126\n",
            "    Uninstalling torch-2.8.0+cu126:\n",
            "      Successfully uninstalled torch-2.8.0+cu126\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.23.0+cu126\n",
            "    Uninstalling torchvision-0.23.0+cu126:\n",
            "      Successfully uninstalled torchvision-0.23.0+cu126\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.8.0+cu126\n",
            "    Uninstalling torchaudio-2.8.0+cu126:\n",
            "      Successfully uninstalled torchaudio-2.8.0+cu126\n",
            "\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.3.1+cu121 torchaudio-2.3.1+cu121 torchvision-0.18.1+cu121\n",
            "Requirement already satisfied: llvmlite==0.43.0 in /usr/local/lib/python3.12/dist-packages (from -r CLAD/requirements.txt (line 1)) (0.43.0)\n",
            "Requirement already satisfied: numba==0.60.0 in /usr/local/lib/python3.12/dist-packages (from -r CLAD/requirements.txt (line 2)) (0.60.0)\n",
            "Requirement already satisfied: librosa>=0.11.0 in /usr/local/lib/python3.12/dist-packages (from -r CLAD/requirements.txt (line 3)) (0.11.0)\n",
            "Collecting matplotlib==3.8.4 (from -r CLAD/requirements.txt (line 4))\n",
            "  Downloading matplotlib-3.8.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.12/dist-packages (from -r CLAD/requirements.txt (line 5)) (1.26.4)\n",
            "Collecting primePy==1.3 (from -r CLAD/requirements.txt (line 6))\n",
            "  Downloading primePy-1.3-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting torchcontrib (from -r CLAD/requirements.txt (line 7))\n",
            "  Downloading torchcontrib-0.0.2.tar.gz (11 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting pytorch_model_summary (from -r CLAD/requirements.txt (line 8))\n",
            "  Downloading pytorch_model_summary-0.1.2-py3-none-any.whl.metadata (35 kB)\n",
            "Collecting torchinfo (from -r CLAD/requirements.txt (line 9))\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.4->-r CLAD/requirements.txt (line 4)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.4->-r CLAD/requirements.txt (line 4)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.4->-r CLAD/requirements.txt (line 4)) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.4->-r CLAD/requirements.txt (line 4)) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.4->-r CLAD/requirements.txt (line 4)) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.4->-r CLAD/requirements.txt (line 4)) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.4->-r CLAD/requirements.txt (line 4)) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.4->-r CLAD/requirements.txt (line 4)) (2.9.0.post0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.11.0->-r CLAD/requirements.txt (line 3)) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.11.0->-r CLAD/requirements.txt (line 3)) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.11.0->-r CLAD/requirements.txt (line 3)) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.11.0->-r CLAD/requirements.txt (line 3)) (1.5.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.11.0->-r CLAD/requirements.txt (line 3)) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.11.0->-r CLAD/requirements.txt (line 3)) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.11.0->-r CLAD/requirements.txt (line 3)) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.11.0->-r CLAD/requirements.txt (line 3)) (1.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.11.0->-r CLAD/requirements.txt (line 3)) (4.15.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.11.0->-r CLAD/requirements.txt (line 3)) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.11.0->-r CLAD/requirements.txt (line 3)) (1.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from pytorch_model_summary->-r CLAD/requirements.txt (line 8)) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from pytorch_model_summary->-r CLAD/requirements.txt (line 8)) (2.3.1+cu121)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa>=0.11.0->-r CLAD/requirements.txt (line 3)) (4.5.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa>=0.11.0->-r CLAD/requirements.txt (line 3)) (2.32.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib==3.8.4->-r CLAD/requirements.txt (line 4)) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.11.0->-r CLAD/requirements.txt (line 3)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.11.0->-r CLAD/requirements.txt (line 3)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.11.0->-r CLAD/requirements.txt (line 3)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.11.0->-r CLAD/requirements.txt (line 3)) (2025.10.5)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa>=0.11.0->-r CLAD/requirements.txt (line 3)) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->librosa>=0.11.0->-r CLAD/requirements.txt (line 3)) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa>=0.11.0->-r CLAD/requirements.txt (line 3)) (2.23)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->pytorch_model_summary->-r CLAD/requirements.txt (line 8)) (3.20.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch->pytorch_model_summary->-r CLAD/requirements.txt (line 8)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->pytorch_model_summary->-r CLAD/requirements.txt (line 8)) (2.8.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch_model_summary->-r CLAD/requirements.txt (line 8)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->pytorch_model_summary->-r CLAD/requirements.txt (line 8)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch_model_summary->-r CLAD/requirements.txt (line 8)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch_model_summary->-r CLAD/requirements.txt (line 8)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch_model_summary->-r CLAD/requirements.txt (line 8)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch_model_summary->-r CLAD/requirements.txt (line 8)) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch_model_summary->-r CLAD/requirements.txt (line 8)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch_model_summary->-r CLAD/requirements.txt (line 8)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch_model_summary->-r CLAD/requirements.txt (line 8)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch_model_summary->-r CLAD/requirements.txt (line 8)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch_model_summary->-r CLAD/requirements.txt (line 8)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch_model_summary->-r CLAD/requirements.txt (line 8)) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch_model_summary->-r CLAD/requirements.txt (line 8)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->pytorch_model_summary->-r CLAD/requirements.txt (line 8)) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->pytorch_model_summary->-r CLAD/requirements.txt (line 8)) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch->pytorch_model_summary->-r CLAD/requirements.txt (line 8)) (1.3.0)\n",
            "Downloading matplotlib-3.8.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 11.6/11.6 MB 121.7 MB/s  0:00:00\n",
            "Downloading primePy-1.3-py3-none-any.whl (4.0 kB)\n",
            "Downloading pytorch_model_summary-0.1.2-py3-none-any.whl (9.3 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Building wheels for collected packages: torchcontrib\n",
            "  Building wheel for torchcontrib (pyproject.toml): started\n",
            "  Building wheel for torchcontrib (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for torchcontrib: filename=torchcontrib-0.0.2-py3-none-any.whl size=7555 sha256=8a83604c693d6998ff58fb8f537de9a729584de333f21f64919747ab52f50fca\n",
            "  Stored in directory: /root/.cache/pip/wheels/e3/d1/1f/63f00ffea223db446943147a04ff035eb40d00cec3e87d63e5\n",
            "Successfully built torchcontrib\n",
            "Installing collected packages: torchcontrib, primePy, torchinfo, matplotlib, pytorch_model_summary\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.0\n",
            "    Uninstalling matplotlib-3.10.0:\n",
            "      Successfully uninstalled matplotlib-3.10.0\n",
            "\n",
            "Successfully installed matplotlib-3.8.4 primePy-1.3 pytorch_model_summary-0.1.2 torchcontrib-0.0.2 torchinfo-1.8.0\n",
            "Hit:1 https://cli.github.com/packages stable InRelease\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,473 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,799 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,816 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,594 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,389 kB]\n",
            "Fetched 21.5 MB in 5s (4,519 kB/s)\n",
            "Reading package lists...\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "libsndfile1 is already the newest version (1.0.31-2ubuntu0.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n",
            "===== DONE: Environment pinned for Python 3.12 =====\n",
            "Python 3.12.12\n",
            "Python: 3.12.12\n",
            "NumPy: 1.26.4\n",
            "Numba: 0.60.0\n",
            "llvmlite: 0.43.0\n",
            "Matplotlib: 3.8.4\n",
            "torch: 2.3.1+cu121\n",
            "torchvision: 0.18.1+cu121\n",
            "torchaudio: 2.3.1+cu121\n",
            "librosa: 0.11.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cloning into 'CLAD'...\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "\n",
        "# --- Clone CLAD fresh (idempotent: remove existing dir if present) ---\n",
        "# remove previous clone to ensure a clean edit of requirements\n",
        "rm -rf CLAD\n",
        "git clone https://github.com/CLAD23/CLAD.git\n",
        "\n",
        "# --- Normalize requirements for Python 3.12 (single source of truth) ---\n",
        "cp CLAD/requirements.txt CLAD/requirements.bak\n",
        "\n",
        "# 1) Remove torch lines (Torch is installed manually for the correct CUDA wheel)\n",
        "sed -i '/^torch==/d; /^torchvision==/d; /^torchaudio==/d' CLAD/requirements.txt\n",
        "\n",
        "# 2) Core pins for Py3.12 + Numba 0.60.0 (compatible with llvmlite 0.43.0 and NumPy 1.26.4)\n",
        "#    These ensure no NumPy 2.x is pulled by accident.\n",
        "if grep -q '^numpy' CLAD/requirements.txt; then\n",
        "  sed -i 's/^numpy==.*/numpy==1.26.4/' CLAD/requirements.txt\n",
        "else\n",
        "  sed -i '1i numpy==1.26.4' CLAD/requirements.txt\n",
        "fi\n",
        "\n",
        "if grep -q '^numba' CLAD/requirements.txt; then\n",
        "  sed -i 's/^numba==.*/numba==0.60.0/' CLAD/requirements.txt\n",
        "else\n",
        "  sed -i '1i numba==0.60.0' CLAD/requirements.txt\n",
        "fi\n",
        "\n",
        "if grep -q '^llvmlite' CLAD/requirements.txt; then\n",
        "  sed -i 's/^llvmlite==.*/llvmlite==0.43.0/' CLAD/requirements.txt\n",
        "else\n",
        "  sed -i '1i llvmlite==0.43.0' CLAD/requirements.txt\n",
        "fi\n",
        "\n",
        "# 3) Stable Matplotlib on Py3.12\n",
        "if grep -q '^matplotlib' CLAD/requirements.txt; then\n",
        "  sed -i 's/^matplotlib==.*/matplotlib==3.8.4/' CLAD/requirements.txt\n",
        "else\n",
        "  sed -i '1i matplotlib==3.8.4' CLAD/requirements.txt\n",
        "fi\n",
        "\n",
        "# 4) Librosa must satisfy coqui-tts (>=0.11.0); keep it permissive to avoid conflicts\n",
        "if grep -q '^librosa' CLAD/requirements.txt; then\n",
        "  sed -i 's/^librosa.*/librosa>=0.11.0/' CLAD/requirements.txt\n",
        "else\n",
        "  sed -i '1i librosa>=0.11.0' CLAD/requirements.txt\n",
        "fi\n",
        "\n",
        "# 5) Pin OpenCV to builds compatible with NumPy 1.26.x (avoid NumPy 2.x constraint)\n",
        "#    Only modify if opencv lines exist (do not add if the project doesn't use it).\n",
        "grep -q '^opencv-python' CLAD/requirements.txt && sed -i 's/^opencv-python==.*/opencv-python==4.9.0.80/' CLAD/requirements.txt || true\n",
        "grep -q '^opencv-contrib-python' CLAD/requirements.txt && sed -i 's/^opencv-contrib-python==.*/opencv-contrib-python==4.9.0.80/' CLAD/requirements.txt || true\n",
        "\n",
        "# 6) Pin spaCy/Thinc to versions that work with NumPy 1.x (only if present)\n",
        "grep -q '^thinc' CLAD/requirements.txt && sed -i 's/^thinc==.*/thinc==8.2.2/' CLAD/requirements.txt || true\n",
        "grep -q '^spacy' CLAD/requirements.txt && sed -i 's/^spacy==.*/spacy==3.7.4/' CLAD/requirements.txt || true\n",
        "\n",
        "echo \"===== Updated CLAD/requirements.txt =====\"\n",
        "sed -n '1,250p' CLAD/requirements.txt\n",
        "\n",
        "# --- Upgrade pip to avoid resolver quirks ---\n",
        "python -m pip install -U pip\n",
        "\n",
        "# --- Install PyTorch 2.3.1 CUDA 12.1 (use CPU wheels by removing the index line if no GPU) ---\n",
        "python -m pip install --index-url https://download.pytorch.org/whl/cu121 \\\n",
        "  torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1\n",
        "\n",
        "# --- Install remaining CLAD dependencies from the single normalized requirements file ---\n",
        "python -m pip install -r CLAD/requirements.txt\n",
        "\n",
        "# --- System library for soundfile/librosa WAV I/O (safe to install always) ---\n",
        "apt-get update -y\n",
        "apt-get install -y libsndfile1\n",
        "\n",
        "echo \"===== DONE: Environment pinned for Python 3.12 =====\"\n",
        "python -V\n",
        "python - <<'PY'\n",
        "import sys, numpy, numba, llvmlite, matplotlib\n",
        "import importlib\n",
        "print(\"Python:\", sys.version.split()[0])\n",
        "print(\"NumPy:\", numpy.__version__)\n",
        "print(\"Numba:\", numba.__version__)\n",
        "print(\"llvmlite:\", llvmlite.__version__)\n",
        "print(\"Matplotlib:\", matplotlib.__version__)\n",
        "for m in (\"torch\",\"torchvision\",\"torchaudio\",\"librosa\"):\n",
        "    try:\n",
        "        mod = importlib.import_module(m)\n",
        "        print(f\"{m}:\", getattr(mod,\"__version__\", \"unknown\"))\n",
        "    except Exception as e:\n",
        "        print(f\"{m}: NOT INSTALLED ({e})\")\n",
        "PY\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "FGDTDGCmYbTT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5c8210f-43ce-41de-cc0e-304aee548aca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All files copied with speaker folder structure preserved!\n",
            "  Real : /content/my_audio/real\n",
            "  Fake : /content/my_audio/fake\n",
            "  Text : /content/my_audio/txt\n"
          ]
        }
      ],
      "source": [
        "#4\n",
        "#Creating a differnet folder called my_audio so we won't destroy the data created\n",
        "\n",
        "import os, pathlib, shutil\n",
        "\n",
        "# Destination base folders\n",
        "REAL_DST = pathlib.Path(\"/content/my_audio/real\")\n",
        "TXT_DST  = pathlib.Path(\"/content/my_audio/txt\")\n",
        "FAKE_DST = pathlib.Path(\"/content/my_audio/fake\")\n",
        "\n",
        "REAL_DST.mkdir(parents=True, exist_ok=True)\n",
        "TXT_DST.mkdir(parents=True, exist_ok=True)\n",
        "FAKE_DST.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Source folders\n",
        "FAKE_SRC = pathlib.Path(\"/content/vctk_samples/VCTK-Corpus/VCTK-Corpus/fake_audio\")\n",
        "TXT_SRC  = pathlib.Path(\"/content/vctk_samples/VCTK-Corpus/VCTK-Corpus/txt\")\n",
        "REAL_SRC = pathlib.Path(\"/content/vctk_samples/VCTK-Corpus/VCTK-Corpus/wav48\")\n",
        "\n",
        "# Copy FAKE wavs while keeping speaker folders\n",
        "for folder in FAKE_SRC.glob(\"p*\"):\n",
        "    speaker_dst = FAKE_DST / folder.name\n",
        "    speaker_dst.mkdir(parents=True, exist_ok=True)\n",
        "    for wav in folder.glob(\"*.wav\"):\n",
        "        shutil.copy(wav, speaker_dst / wav.name)\n",
        "\n",
        "# Copy REAL wavs while keeping speaker folders\n",
        "for folder in REAL_SRC.glob(\"p*\"):\n",
        "    speaker_dst = REAL_DST / folder.name\n",
        "    speaker_dst.mkdir(parents=True, exist_ok=True)\n",
        "    for wav in folder.glob(\"*.wav\"):\n",
        "        shutil.copy(wav, speaker_dst / wav.name)\n",
        "\n",
        "# Copy TXT transcripts (flat, no subfolders in original)\n",
        "for folder in TXT_SRC.glob(\"p*\"):\n",
        "    speaker_dst = TXT_DST / folder.name\n",
        "    speaker_dst.mkdir(parents=True, exist_ok=True)\n",
        "    for txt in TXT_SRC.glob(\"*.txt\"):\n",
        "        shutil.copy(txt, TXT_DST / txt.name)\n",
        "\n",
        "print(\"‚úÖ All files copied with speaker folder structure preserved!\")\n",
        "print(\"  Real :\", REAL_DST)\n",
        "print(\"  Fake :\", FAKE_DST)\n",
        "print(\"  Text :\", TXT_DST)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "iHwvFTkYY4Ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "876b3d32-2911-4b9e-c0d9-296c81ab66ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eligible speakers: ['p225', 'p226', 'p227', 'p228']\n",
            "Chosen split (by speakers):\n",
            "  train: ['p225', 'p227']\n",
            "  val  : ['p226']\n",
            "  test : ['p228']\n",
            "By-file shares (train/val/test): 0.462 0.265 0.273\n",
            "\n",
            "Manifest: /content/my_audio_split/manifest.csv\n",
            "Counts per split/class:\n",
            "  train real: 620\n",
            "  train fake: 620\n",
            "  val   real: 356\n",
            "  val   fake: 356\n",
            "  test  real: 366\n",
            "  test  fake: 366\n",
            "\n",
            "Overlap checks (should be empty):\n",
            "  train ‚à© val : set()\n",
            "  train ‚à© test: set()\n",
            "  val   ‚à© test: set()\n"
          ]
        }
      ],
      "source": [
        "#7\n",
        "# Speaker-disjoint splitter with:\n",
        "# - guaranteed non-empty val\n",
        "# - easy \"switch speakers\" controls\n",
        "# - deletes old OUT folder before writing\n",
        "#\n",
        "# Works for both 4 speakers (auto 2/1/1) and 5+ speakers (targets 3/1/1 by speakers).\n",
        "\n",
        "import os, shutil, random, glob, csv, itertools\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "ROOT = Path(\"/content/my_audio\")           # your current data root (real/fake/{speaker}/*.wav)\n",
        "OUT  = Path(\"/content/my_audio_split\")     # split will be (re)created here\n",
        "AUDIO_EXTS = {\".wav\", \".flac\", \".mp3\", \".m4a\", \".aac\", \".ogg\"}  # add if needed\n",
        "SEED = 42\n",
        "USE_SYMLINKS = True                        # False = copy files instead of symlink\n",
        "REQUIRE_BOTH_CLASSES = True                # speakers must exist under BOTH real/ and fake/\n",
        "# Desired speaker counts (train/val/test)\n",
        "DESIRED_311 = (3, 1, 1)                    # prefer 3/1/1 when you have ‚â•5 speakers\n",
        "FALLBACK_211 = (2, 1, 1)                   # for 4 speakers, this is the safe split\n",
        "# >>> Force specific speakers into splits (edit these to \"switch\")\n",
        "TRAIN_FORCE = set()                        # e.g., {\"p226\",\"p227\",\"p228\"}\n",
        "VAL_FORCE   = set()                        # e.g., {\"p225\"}\n",
        "TEST_FORCE  = set()                        # e.g., {\"p229\"}\n",
        "# --------------------------------------\n",
        "\n",
        "random.seed(SEED)\n",
        "\n",
        "def list_speakers(root, cls):\n",
        "    base = root/cls\n",
        "    if not base.exists(): return []\n",
        "    return sorted([d.name for d in base.iterdir() if d.is_dir() and d.name != \"txt\"])\n",
        "\n",
        "def list_audio(dirpath):\n",
        "    return sorted([p for p in dirpath.rglob(\"*\")\n",
        "                   if p.is_file() and p.suffix.lower() in AUDIO_EXTS])\n",
        "\n",
        "# 1) Find eligible speakers (present and non-empty under both classes if required)\n",
        "real_spk = set(list_speakers(ROOT, \"real\"))\n",
        "fake_spk = set(list_speakers(ROOT, \"fake\"))\n",
        "if REQUIRE_BOTH_CLASSES:\n",
        "    eligible = sorted(real_spk & fake_spk)\n",
        "else:\n",
        "    eligible = sorted(real_spk | fake_spk)\n",
        "\n",
        "def nonempty_both(s):\n",
        "    if not REQUIRE_BOTH_CLASSES:  # just need at least one side non-empty\n",
        "        return (len(list_audio(ROOT/\"real\"/s)) + len(list_audio(ROOT/\"fake\"/s))) > 0\n",
        "    return len(list_audio(ROOT/\"real\"/s)) > 0 and len(list_audio(ROOT/\"fake\"/s)) > 0\n",
        "\n",
        "eligible = [s for s in eligible if nonempty_both(s)]\n",
        "n_spk = len(eligible)\n",
        "if n_spk < 2:\n",
        "    raise RuntimeError(f\"Need ‚â•2 eligible speakers, found {n_spk}: {eligible}\")\n",
        "\n",
        "# 2) Choose target split sizes by number of speakers\n",
        "if n_spk >= sum(DESIRED_311):\n",
        "    N_TRAIN, N_VAL, N_TEST = DESIRED_311   # 3/1/1\n",
        "else:\n",
        "    # With 4 speakers, 2/1/1 is the right shape to keep val+test non-empty\n",
        "    N_TRAIN, N_VAL, N_TEST = FALLBACK_211  # 2/1/1\n",
        "\n",
        "# 3) Validate FORCE sets and fill remaining slots\n",
        "forced = TRAIN_FORCE | VAL_FORCE | TEST_FORCE\n",
        "if forced:\n",
        "    missing = forced - set(eligible)\n",
        "    if missing:\n",
        "        raise RuntimeError(f\"Forced speakers not found/eligible: {sorted(missing)}\")\n",
        "    overlap = (TRAIN_FORCE & VAL_FORCE) | (TRAIN_FORCE & TEST_FORCE) | (VAL_FORCE & TEST_FORCE)\n",
        "    if overlap:\n",
        "        raise RuntimeError(f\"Forced sets overlap: {sorted(overlap)}\")\n",
        "\n",
        "# file counts (use 'real' side as proxy for per-speaker volume)\n",
        "spk_counts = {s: len(list_audio(ROOT/\"real\"/s)) for s in eligible}\n",
        "total_files = sum(spk_counts.values())\n",
        "\n",
        "def pick_k_closest(candidates, k, target_share):\n",
        "    \"\"\"Pick k speakers whose file-count sum is closest to target_share (in files).\"\"\"\n",
        "    if k <= 0: return set()\n",
        "    if len(candidates) <= k: return set(candidates)\n",
        "    best, gap = None, float(\"inf\")\n",
        "    for combo in itertools.combinations(candidates, k):\n",
        "        share = sum(spk_counts[s] for s in combo)\n",
        "        g = abs(share - target_share)\n",
        "        if g < gap:\n",
        "            gap, best = g, set(combo)\n",
        "    return best\n",
        "\n",
        "# Start with forced\n",
        "train_set, val_set, test_set = set(TRAIN_FORCE), set(VAL_FORCE), set(TEST_FORCE)\n",
        "remaining = [s for s in eligible if s not in (train_set | val_set | test_set)]\n",
        "\n",
        "need_train = max(0, N_TRAIN - len(train_set))\n",
        "need_val   = max(0, N_VAL   - len(val_set))\n",
        "need_test  = max(0, N_TEST  - len(test_set))\n",
        "\n",
        "# Target train file share (rough guideline): ~60% if 3/1/1, ~50% if 2/1/1\n",
        "target_train_share = 0.60*total_files if (N_TRAIN, N_VAL, N_TEST) == DESIRED_311 else 0.50*total_files\n",
        "\n",
        "# Fill TRAIN first to hit the share as best as possible\n",
        "if need_train > 0:\n",
        "    add = pick_k_closest(remaining, need_train, target_train_share - sum(spk_counts[s] for s in train_set))\n",
        "    train_set |= add\n",
        "    remaining = [s for s in remaining if s not in add]\n",
        "\n",
        "# Fill VAL with lighter speakers (to keep val/test similar size)\n",
        "if need_val > 0:\n",
        "    remaining.sort(key=lambda s: spk_counts[s])  # lightest first\n",
        "    add = set(remaining[:need_val])\n",
        "    val_set |= add\n",
        "    remaining = remaining[need_val:]\n",
        "\n",
        "# Fill TEST with the rest needed\n",
        "if need_test > 0:\n",
        "    add = set(remaining[:need_test])\n",
        "    test_set |= add\n",
        "    remaining = remaining[need_test:]\n",
        "\n",
        "# Final sanity: exact sizes, disjointness\n",
        "if not (len(train_set) == N_TRAIN and len(val_set) == N_VAL and len(test_set) == N_TEST):\n",
        "    raise RuntimeError(f\"Final sizes must be {N_TRAIN}/{N_VAL}/{N_TEST}, got {len(train_set)}/{len(val_set)}/{len(test_set)}\")\n",
        "if not (train_set.isdisjoint(val_set) and train_set.isdisjoint(test_set) and val_set.isdisjoint(test_set)):\n",
        "    raise RuntimeError(\"Splits are not disjoint by speakers.\")\n",
        "\n",
        "print(\"Eligible speakers:\", eligible)\n",
        "print(\"Chosen split (by speakers):\")\n",
        "print(\"  train:\", sorted(train_set))\n",
        "print(\"  val  :\", sorted(val_set))\n",
        "print(\"  test :\", sorted(test_set))\n",
        "print(\"By-file shares (train/val/test):\",\n",
        "      round(sum(spk_counts[s] for s in train_set)/total_files, 3),\n",
        "      round(sum(spk_counts[s] for s in val_set)/total_files, 3),\n",
        "      round(sum(spk_counts[s] for s in test_set)/total_files, 3))\n",
        "\n",
        "# 4) DELETE OLD OUT (so the previous no-val split is removed), then rebuild\n",
        "if OUT.exists():\n",
        "    shutil.rmtree(OUT)\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "    for cls in [\"real\", \"fake\"]:\n",
        "        (OUT/split/cls).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def which_split(speaker):\n",
        "    if speaker in train_set: return \"train\"\n",
        "    if speaker in val_set:   return \"val\"\n",
        "    return \"test\"\n",
        "\n",
        "# 5) Materialize split (symlink/copy) + manifest\n",
        "rows, counts = [], defaultdict(int)\n",
        "for cls in [\"real\", \"fake\"]:\n",
        "    base = ROOT/cls\n",
        "    for sp in (train_set | val_set | test_set):\n",
        "        src_dir = base/sp\n",
        "        dst_dir = OUT/which_split(sp)/cls/sp\n",
        "        dst_dir.mkdir(parents=True, exist_ok=True)\n",
        "        for src in list_audio(src_dir):\n",
        "            dst = dst_dir/src.name\n",
        "            if dst.exists():\n",
        "                try: dst.unlink()\n",
        "                except: pass\n",
        "            if USE_SYMLINKS:\n",
        "                try:\n",
        "                    os.symlink(src.resolve(), dst)\n",
        "                except FileExistsError:\n",
        "                    pass\n",
        "            else:\n",
        "                shutil.copy2(src, dst)\n",
        "            rows.append({\n",
        "                \"split\": which_split(sp),\n",
        "                \"speaker\": sp,\n",
        "                \"label\": 0 if cls==\"real\" else 1,\n",
        "                \"src_path\": str(src.resolve()),\n",
        "                \"dst_path\": str(dst.resolve())\n",
        "            })\n",
        "            counts[(which_split(sp), cls)] += 1\n",
        "\n",
        "manifest_csv = OUT/\"manifest.csv\"\n",
        "with open(manifest_csv, \"w\", newline=\"\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=[\"split\",\"speaker\",\"label\",\"src_path\",\"dst_path\"])\n",
        "    writer.writeheader(); writer.writerows(rows)\n",
        "\n",
        "print(\"\\nManifest:\", manifest_csv)\n",
        "print(\"Counts per split/class:\")\n",
        "for split in [\"train\",\"val\",\"test\"]:\n",
        "    for cls in [\"real\",\"fake\"]:\n",
        "        print(f\"  {split:5s} {cls:4s}: {counts[(split, cls)]}\")\n",
        "\n",
        "# Extra safety: show speaker overlap (should be empty)\n",
        "print(\"\\nOverlap checks (should be empty):\")\n",
        "print(\"  train ‚à© val :\", train_set & val_set)\n",
        "print(\"  train ‚à© test:\", train_set & test_set)\n",
        "print(\"  val   ‚à© test:\", val_set & test_set)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "SQIRMktSh-TU"
      },
      "outputs": [],
      "source": [
        "#6\n",
        "# Choose ONE of the two options below:\n",
        "\n",
        "USE_LIBROSA = True   # set to False to use scipy.io.wavfile only\n",
        "\n",
        "if USE_LIBROSA:\n",
        "    # Librosa path: convenient resample-to-16k + mono in one call\n",
        "    !pip -q install librosa soundfile\n",
        "else:\n",
        "    # Scipy path: no extra system libs; we will do a small numpy resample\n",
        "    !pip -q install scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "wNTVqDPYiGZ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90596b3e-1688-46b3-cd3f-b522afe71ede"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
            "  def dispatcher(\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:337: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.save_with_torchcodec` under the hood. Some parameters like format, encoding, bits_per_sample, buffer_size, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's encoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.encoders.AudioEncoder\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:247: UserWarning: torio.io._streaming_media_encoder.StreamingMediaEncoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  s = torchaudio.io.StreamWriter(uri, format=muxer, buffer_size=buffer_size)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All audio preprocessed to 16kHz and padded/clipped to 64600 samples (torchaudio)\n"
          ]
        }
      ],
      "source": [
        "# =========================================\n",
        "# Preprocess audio with torchaudio: 16kHz mono + pad/trim to 64600\n",
        "# =========================================\n",
        "from pathlib import Path\n",
        "import torchaudio, torch, torch.nn.functional as F\n",
        "\n",
        "TARGET_SR = 16000\n",
        "TARGET_LEN = 64600  # ~4 seconds at 16kHz\n",
        "\n",
        "def preprocess_wav_torch(in_path: Path):\n",
        "    wav, sr = torchaudio.load(str(in_path))   # [C,T]\n",
        "    wav = wav.float()\n",
        "    if wav.shape[0] > 1:\n",
        "        wav = wav.mean(dim=0, keepdim=True)   # [1,T]\n",
        "    if sr != TARGET_SR:\n",
        "        res = torchaudio.transforms.Resample(orig_freq=sr, new_freq=TARGET_SR)\n",
        "        wav = res(wav)                         # [1,T']\n",
        "    T = wav.shape[-1]\n",
        "    if T < TARGET_LEN:\n",
        "        wav = F.pad(wav, (0, TARGET_LEN - T))\n",
        "    else:\n",
        "        wav = wav[..., :TARGET_LEN]\n",
        "    return wav.squeeze(0), TARGET_SR          # [T], 16000\n",
        "\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "    split_dir = Path(\"/content/my_audio_split\") / split\n",
        "    for cls in [\"real\", \"fake\"]:\n",
        "        for wav_path in (split_dir/cls).rglob(\"*.wav\"):\n",
        "            wav, sr = preprocess_wav_torch(wav_path)\n",
        "            torchaudio.save(str(wav_path), wav.unsqueeze(0), sr)  # overwrite in place\n",
        "\n",
        "print(\"‚úÖ All audio preprocessed to 16kHz and padded/clipped to 64600 samples (torchaudio)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Fast resampler (no resampy needed) ===\n",
        "# Comments are in English only.\n",
        "import numpy as np\n",
        "from math import gcd\n",
        "\n",
        "try:\n",
        "    from scipy.signal import resample_poly\n",
        "    _HAVE_SCIPY = True\n",
        "except Exception:\n",
        "    _HAVE_SCIPY = False\n",
        "\n",
        "def fast_resample(x: np.ndarray, orig_sr: int, target_sr: int) -> np.ndarray:\n",
        "    \"\"\"Resample using scipy.signal.resample_poly; falls back to librosa if SciPy missing.\"\"\"\n",
        "    x = np.asarray(x, dtype=np.float32)\n",
        "    if orig_sr == target_sr:\n",
        "        return x\n",
        "    if _HAVE_SCIPY:\n",
        "        g = gcd(int(orig_sr), int(target_sr))\n",
        "        up = int(target_sr // g)\n",
        "        down = int(orig_sr // g)\n",
        "        y = resample_poly(x, up, down).astype(np.float32)\n",
        "        return y\n",
        "    else:\n",
        "        import librosa  # will need resampy if this path is used\n",
        "        return librosa.resample(x, orig_sr=orig_sr, target_sr=target_sr, res_type=\"kaiser_best\").astype(np.float32)\n"
      ],
      "metadata": {
        "id": "L5yMnSA-DpFh"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Audio globals and basic utilities\n",
        "# Fixed format: mono, 16 kHz, 4 sec (64,600 samples)\n",
        "# ================================\n",
        "import numpy as np\n",
        "import librosa\n",
        "\n",
        "TARGET_SR  = 16000\n",
        "TARGET_LEN = 64600  # ~4 seconds at 16 kHz\n",
        "EPS = 1e-9\n",
        "\n",
        "# Deterministic generator for reproducibility; adjust seed if needed\n",
        "_rng = np.random.default_rng(42)\n",
        "\n",
        "def pad_trim(wav: np.ndarray, target_len: int = TARGET_LEN) -> np.ndarray:\n",
        "    \"\"\"Pad with zeros or trim to exactly target_len samples.\"\"\"\n",
        "    if wav.shape[0] < target_len:\n",
        "        wav = np.pad(wav, (0, target_len - wav.shape[0]))\n",
        "    else:\n",
        "        wav = wav[:target_len]\n",
        "    return wav\n",
        "\n",
        "def resample_to(wav: np.ndarray, sr: int, target_sr: int = TARGET_SR) -> np.ndarray:\n",
        "    \"\"\"Resample to target_sr using high-quality Kaiser filter.\"\"\"\n",
        "    if sr == target_sr:\n",
        "        return wav\n",
        "    return librosa.resample(wav, orig_sr=sr, target_sr=target_sr, res_type=\"kaiser_best\")\n",
        "\n",
        "def post_fix(wav: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Return to the fixed format after any manipulation: length=64600 and clipped to [-1, 1].\"\"\"\n",
        "    wav = pad_trim(wav, TARGET_LEN)\n",
        "    wav = np.clip(wav, -1.0, 1.0)\n",
        "    return wav.astype(np.float32)\n"
      ],
      "metadata": {
        "id": "ixmliIrEykcq"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Audio globals and basic utilities\n",
        "# Fixed format: mono, 16 kHz, 4 sec (64,600 samples)\n",
        "# ================================\n",
        "import numpy as np\n",
        "import librosa\n",
        "\n",
        "TARGET_SR  = 16000\n",
        "TARGET_LEN = 64600  # ~4 seconds at 16 kHz\n",
        "EPS = 1e-9\n",
        "\n",
        "# Deterministic generator for reproducibility; adjust seed if needed\n",
        "_rng = np.random.default_rng(42)\n",
        "\n",
        "def pad_trim(wav: np.ndarray, target_len: int = TARGET_LEN) -> np.ndarray:\n",
        "    \"\"\"Pad with zeros or trim to exactly target_len samples.\"\"\"\n",
        "    if wav.shape[0] < target_len:\n",
        "        wav = np.pad(wav, (0, target_len - wav.shape[0]))\n",
        "    else:\n",
        "        wav = wav[:target_len]\n",
        "    return wav\n",
        "\n",
        "def resample_to(wav: np.ndarray, sr: int, target_sr: int = TARGET_SR) -> np.ndarray:\n",
        "    \"\"\"Resample to target_sr using high-quality Kaiser filter.\"\"\"\n",
        "    if sr == target_sr:\n",
        "        return wav\n",
        "    return librosa.resample(wav, orig_sr=sr, target_sr=target_sr, res_type=\"kaiser_best\")\n",
        "\n",
        "def post_fix(wav: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Return to the fixed format after any manipulation: length=64600 and clipped to [-1, 1].\"\"\"\n",
        "    wav = pad_trim(wav, TARGET_LEN)\n",
        "    wav = np.clip(wav, -1.0, 1.0)\n",
        "    return wav.astype(np.float32)\n"
      ],
      "metadata": {
        "id": "q8_Hbmgo1qQe"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manipulations"
      ],
      "metadata": {
        "id": "pcyj99GfwB5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Manipulation Policy & Parameter Ranges (paper-aligned) ===\n",
        "# All ops assume mono float32 in [-1, 1] and return fixed-length 64600 samples.\n",
        "# Values are aligned with the paper's evaluation grid.\n",
        "\n",
        "# Volume Control factors\n",
        "VC_FACTORS = [0.5, 0.1]  # paper uses 0.5 and 0.1\n",
        "\n",
        "# White-Noise SNRs in dB\n",
        "WN_SNRS_DB = [15.0, 20.0, 25.0]  # paper: 15/20/25 dB\n",
        "\n",
        "# Environmental noise SNR (if provided)\n",
        "ENV_SNR_DB = 20.0  # paper uses 20 dB for ESC-50 env noises\n",
        "\n",
        "# Fade ratios and shapes (paper: linear at 0.1/0.3/0.5, plus multiple shapes at 0.5)\n",
        "FADE_RATIOS = [0.1, 0.3, 0.5]\n",
        "FADE_SHAPES = [\"linear\", \"half_sin\", \"quarter_sin\", \"raised_sin\", \"exp\"]  # keep 'linear' for default\n",
        "\n",
        "# Time-Stretch factors (paper: 0.9, 0.95, 1.05, 1.1)\n",
        "TS_FACTORS = [0.9, 0.95, 1.05, 1.1]\n",
        "\n",
        "# Resample offsets around 16kHz (paper: 15k, 15.5k, 16.5k, 17k ‚Üí -1000, -500, +500, +1000)\n",
        "RS_OFFSETS_HZ = [-1000, -500, +500, +1000]\n",
        "\n",
        "# Time-Shift (in samples) at 16kHz (paper: 1,600; 16,000; 32,000)\n",
        "SHIFT_SAMPLES = [1600, 16000, 32000]\n",
        "\n",
        "# Echo parameters (paper: delay 1000/2000; attenuation 0.2/0.5)\n",
        "ECHO_PARAMS = [(1000, 0.2), (1000, 0.5), (2000, 0.2), (2000, 0.5)]\n",
        "\n",
        "# Optional environmental noise clip (16k mono float32). Keep None unless you load one.\n",
        "ENV_NOISE = None\n"
      ],
      "metadata": {
        "id": "tuxyWsmfwGRD"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Core Manipulation Ops (waveform-in, waveform-out) ===\n",
        "import numpy as np\n",
        "import librosa\n",
        "\n",
        "def change_volume(wav: np.ndarray, factor: float) -> np.ndarray:\n",
        "    \"\"\"Multiply amplitude by a gain factor, then post-fix.\"\"\"\n",
        "    out = wav * float(factor)\n",
        "    return post_fix(out)\n",
        "\n",
        "def add_white_noise_snr(wav: np.ndarray, snr_db: float) -> np.ndarray:\n",
        "    \"\"\"Add white noise at a target SNR (dB) relative to signal RMS.\"\"\"\n",
        "    sig_pow = max(float(np.mean(wav**2)), EPS)\n",
        "    snr_lin = 10.0 ** (float(snr_db) / 10.0)\n",
        "    noise_pow = sig_pow / snr_lin\n",
        "    noise = np.random.normal(0.0, np.sqrt(noise_pow), size=wav.shape).astype(np.float32)\n",
        "    return post_fix(wav + noise)\n",
        "\n",
        "def add_env_noise(wav: np.ndarray, env_noise: np.ndarray, snr_db: float) -> np.ndarray:\n",
        "    \"\"\"Mix an external environmental noise clip at target SNR (dB).\"\"\"\n",
        "    env = env_noise.astype(np.float32)\n",
        "    if env.shape[0] < wav.shape[0]:\n",
        "        reps = int(np.ceil(wav.shape[0] / env.shape[0]))\n",
        "        env = np.tile(env, reps)[:wav.shape[0]]\n",
        "    else:\n",
        "        env = env[:wav.shape[0]]\n",
        "    sig_pow = max(float(np.mean(wav**2)), EPS)\n",
        "    snr_lin = 10.0 ** (float(snr_db) / 10.0)\n",
        "    noise_pow = sig_pow / snr_lin\n",
        "    cur_pow = max(float(np.mean(env**2)), EPS)\n",
        "    env = env * np.sqrt(noise_pow / cur_pow)\n",
        "    return post_fix(wav + env)\n",
        "\n",
        "def apply_fade(wav: np.ndarray, ratio: float, shape: str = \"linear\") -> np.ndarray:\n",
        "    \"\"\"Apply fade-in/out over a ratio at both ends using the selected shape.\"\"\"\n",
        "    n = wav.shape[0]\n",
        "    k = int(min(0.5, max(0.0, float(ratio))) * n)\n",
        "    if k == 0:\n",
        "        return post_fix(wav)\n",
        "    if shape == \"half_sin\":\n",
        "        ramp = np.sin(np.linspace(0, np.pi/2, k, endpoint=True))**2\n",
        "    elif shape == \"quarter_sin\":\n",
        "        ramp = np.sin(np.linspace(0, np.pi/4, k, endpoint=True))**2\n",
        "    elif shape == \"raised_sin\":\n",
        "        ramp = (np.sin(np.linspace(0, np.pi/2, k, endpoint=True))**2) * 0.5 + 0.5\n",
        "    elif shape == \"exp\":\n",
        "        ramp = np.linspace(0, 1, k, endpoint=True)**2\n",
        "    else:  # linear\n",
        "        ramp = np.linspace(0, 1, k, endpoint=True)\n",
        "    mask = np.ones(n, dtype=np.float32)\n",
        "    mask[:k] *= ramp\n",
        "    mask[-k:] *= ramp[::-1]\n",
        "    return post_fix(wav * mask)\n",
        "\n",
        "def time_stretch_fixed(wav: np.ndarray, factor: float) -> np.ndarray:\n",
        "    \"\"\"Time-stretch while preserving pitch (librosa effects).\"\"\"\n",
        "    stretched = librosa.effects.time_stretch(wav.astype(np.float32), rate=float(factor))\n",
        "    return post_fix(stretched)\n",
        "\n",
        "def resample_variant(wav: np.ndarray, base_sr: int, offset_hz: int) -> np.ndarray:\n",
        "    \"\"\"Resample to base_sr+offset, then back to base_sr to simulate pitch/duration change.\"\"\"\n",
        "    target_sr = max(2000, int(base_sr) + int(offset_hz))\n",
        "    tmp = fast_resample(wav, orig_sr=base_sr, target_sr=target_sr)\n",
        "    back = fast_resample(tmp, orig_sr=target_sr, target_sr=base_sr)\n",
        "    return post_fix(back)\n",
        "\n",
        "def time_shift(wav: np.ndarray, shift_samples: int) -> np.ndarray:\n",
        "    \"\"\"Roll the waveform by +/- shift_samples.\"\"\"\n",
        "    s = int(shift_samples)\n",
        "    return post_fix(np.roll(wav, s))\n",
        "\n",
        "def add_echo(wav: np.ndarray, delay_samples: int, attenuation: float) -> np.ndarray:\n",
        "    \"\"\"Single-tap echo with given delay and attenuation.\"\"\"\n",
        "    delay = int(max(1, delay_samples))\n",
        "    att = float(attenuation)\n",
        "    out = wav.copy()\n",
        "    if delay < wav.shape[0]:\n",
        "        out[delay:] += att * wav[:-delay]\n",
        "    return post_fix(out)\n"
      ],
      "metadata": {
        "id": "HkHILZA7wlSQ"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Manual Selection Utilities: print menu, read choice, and apply that op ===\n",
        "# Requires: constants from Cell 1, ops from Cell 2, plus pad_trim/post_fix/TARGET_SR/TARGET_LEN/EPS/_rng.\n",
        "\n",
        "OP_MENU = {\n",
        "    \"1\": (\"VC\",  \"Volume Control\"),\n",
        "    \"2\": (\"WN\",  \"White-Noise SNR\"),\n",
        "    \"3\": (\"ENV\", \"Environmental Noise (if ENV_NOISE is provided)\"),\n",
        "    \"4\": (\"FD\",  \"Fade In/Out\"),\n",
        "    \"5\": (\"TS\",  \"Time Stretch\"),\n",
        "    \"6\": (\"RS\",  \"Resample Variant\"),\n",
        "    \"7\": (\"SF\",  \"Time Shift\"),\n",
        "    \"8\": (\"EC\",  \"Echo\"),\n",
        "}\n",
        "\n",
        "def print_op_menu():\n",
        "    \"\"\"Print available single-op manipulations.\"\"\"\n",
        "    print(\"Choose one manipulation to apply (type the number):\")\n",
        "    for k, (code, label) in OP_MENU.items():\n",
        "        print(f\"  {k}) {label}  [{code}]\")\n",
        "    print()\n",
        "\n",
        "def apply_selected_manipulation(wav, op_code: str, env_noise=None):\n",
        "    \"\"\"Apply the selected manipulation by short code; returns (wav, op_code).\"\"\"\n",
        "    op_code = op_code.upper().strip()\n",
        "    if op_code == \"VC\":\n",
        "        return change_volume(wav, float(_rng.choice(VC_FACTORS))), \"VC\"\n",
        "    if op_code == \"WN\":\n",
        "        return add_white_noise_snr(wav, float(_rng.choice(WN_SNRS_DB))), \"WN\"\n",
        "    if op_code == \"ENV\":\n",
        "        if env_noise is None:\n",
        "            raise ValueError(\"ENV selected but ENV_NOISE is None.\")\n",
        "        return add_env_noise(wav, env_noise, snr_db=float(ENV_SNR_DB)), \"ENV\"\n",
        "    if op_code == \"FD\":\n",
        "        return apply_fade(wav, ratio=float(_rng.choice(FADE_RATIOS)),\n",
        "                          shape=str(_rng.choice(FADE_SHAPES))), \"FD\"\n",
        "    if op_code == \"TS\":\n",
        "        return time_stretch_fixed(wav, float(_rng.choice(TS_FACTORS))), \"TS\"\n",
        "    if op_code == \"RS\":\n",
        "        return resample_variant(wav, base_sr=TARGET_SR,\n",
        "                                offset_hz=int(_rng.choice(RS_OFFSETS_HZ))), \"RS\"\n",
        "    if op_code == \"SF\":\n",
        "        return time_shift(wav, int(_rng.choice(SHIFT_SAMPLES))), \"SF\"\n",
        "    if op_code == \"EC\":\n",
        "        delay, att = _rng.choice(ECHO_PARAMS)\n",
        "        return add_echo(wav, delay_samples=int(delay), attenuation=float(att)), \"EC\"\n",
        "    raise ValueError(f\"Unknown op_code: {op_code}\")\n",
        "\n",
        "def ask_and_apply_once(wav, env_noise=None):\n",
        "    \"\"\"Interactive path: print menu, read user choice, apply op; returns (wav, op_code).\"\"\"\n",
        "    print_op_menu()\n",
        "    choice = input(\"Enter number (1-8): \").strip()\n",
        "    if choice not in OP_MENU:\n",
        "        raise ValueError(f\"Invalid choice: {choice}. Expected one of {list(OP_MENU.keys())}.\")\n",
        "    op_code = OP_MENU[choice][0]\n",
        "    out, code = apply_selected_manipulation(wav, op_code, env_noise=env_noise)\n",
        "    print(f\"Applied: {OP_MENU[choice][1]} [{code}]\")\n",
        "    return out, code\n"
      ],
      "metadata": {
        "id": "cc57XfCkwoT6"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Environmental Noise Presets: build ENV_NOISE (traffic / crowd / hvac / cafe) ===\n",
        "# Drop-in replacement for the previous \"ENV_NOISE loader\". Comments are English only.\n",
        "\n",
        "import numpy as np\n",
        "import librosa\n",
        "\n",
        "# You can switch the preset below to: \"traffic\", \"crowd\", \"hvac\", or \"cafe\"\n",
        "ENV_PRESET   = \"traffic\"   # choose: \"traffic\" | \"crowd\" | \"hvac\" | \"cafe\"\n",
        "ENV_SECONDS  = 180         # duration to synthesize (seconds)\n",
        "ENV_SNR_DB   = 25.0        # keep this subtle (higher dB = softer noise when mixed later)\n",
        "\n",
        "def _fft_shape(x: np.ndarray, sr: int, shape_mag: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Apply magnitude shaping in the frequency domain and return time-domain signal.\"\"\"\n",
        "    n = x.size\n",
        "    X = np.fft.rfft(x.astype(np.float64), n=n)\n",
        "    # Ensure shape matches bins\n",
        "    if shape_mag.size != X.size:\n",
        "        raise ValueError(\"shape_mag size mismatch.\")\n",
        "    y = np.fft.irfft(X * shape_mag, n=n)\n",
        "    return y.astype(np.float32)\n",
        "\n",
        "def _make_shape(sr: int, n: int, preset: str) -> np.ndarray:\n",
        "    \"\"\"Construct a magnitude response per preset.\"\"\"\n",
        "    freqs = np.fft.rfftfreq(n, d=1.0/sr)\n",
        "    eps = 1e-9\n",
        "    # Start flat\n",
        "    mag = np.ones_like(freqs, dtype=np.float64)\n",
        "\n",
        "    if preset == \"traffic\":\n",
        "        # Emphasize 80‚Äì800 Hz (engine/road), roll-off above 3 kHz\n",
        "        low  = 1.0 / np.sqrt(np.maximum(freqs, 80.0))        # pink-ish\n",
        "        band = np.clip((freqs >= 80.0) & (freqs <= 800.0), 0, 1).astype(np.float64)\n",
        "        high_cut = 1.0 / (1.0 + (freqs/3000.0)**2)           # gentle low-pass ~3 kHz\n",
        "        mag = (0.6*low + 0.8*band + 0.2) * high_cut\n",
        "\n",
        "    elif preset == \"crowd\":\n",
        "        # Emphasize 200‚Äì2kHz (voices blur), attenuate <100 Hz and >4 kHz\n",
        "        band = np.clip((freqs >= 200.0) & (freqs <= 2000.0), 0, 1).astype(np.float64)\n",
        "        low_cut = (freqs / 100.0)\n",
        "        low_cut = np.clip(low_cut, 0.0, 1.0)\n",
        "        high_cut = 1.0 / (1.0 + (freqs/4000.0)**2)\n",
        "        mag = (0.5*band + 0.2) * low_cut * high_cut\n",
        "\n",
        "    elif preset == \"hvac\":\n",
        "        # Low hum (50‚Äì200 Hz) + broadband low-passed\n",
        "        low_bump = 1.0 / np.sqrt(np.maximum(freqs, 50.0))\n",
        "        low_pass = 1.0 / (1.0 + (freqs/1500.0)**2)\n",
        "        mag = 0.8*low_bump * low_pass\n",
        "\n",
        "    elif preset == \"cafe\":\n",
        "        # Mid-band babble + clatter hints, soft highs\n",
        "        band = np.clip((freqs >= 300.0) & (freqs <= 3000.0), 0, 1).astype(np.float64)\n",
        "        high_cut = 1.0 / (1.0 + (freqs/5000.0)**2)\n",
        "        mag = (0.6*band + 0.3) * high_cut\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown ENV_PRESET: {preset}\")\n",
        "\n",
        "    # Avoid DC explosion\n",
        "    mag[0] = mag[1] if mag.size > 1 else 1.0\n",
        "    # Smooth a little to avoid sharp edges\n",
        "    from scipy.ndimage import gaussian_filter1d  # available in Colab; if not, comment out smoothing\n",
        "    mag = gaussian_filter1d(mag, sigma=2.0)\n",
        "    return mag\n",
        "\n",
        "def _simple_reverb_ir(sr: int, rt60: float = 0.3) -> np.ndarray:\n",
        "    \"\"\"Make a short noise-based decay IR (very light reverb).\"\"\"\n",
        "    length = int(sr * rt60)\n",
        "    if length < 32:\n",
        "        length = 32\n",
        "    ir = np.random.normal(0.0, 1.0, size=length).astype(np.float32)\n",
        "    # Exponential decay envelope\n",
        "    t = np.linspace(0.0, 1.0, length, endpoint=False)\n",
        "    decay = np.exp(-4.0 * t)  # adjust factor for decay speed\n",
        "    ir *= decay\n",
        "    # Normalize IR energy\n",
        "    ir /= (np.sqrt(np.maximum(np.mean(ir**2), 1e-12)) + 1e-8)\n",
        "    return ir\n",
        "\n",
        "def _convolve(x: np.ndarray, h: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Fast convolution via FFT (same length as x).\"\"\"\n",
        "    n = x.size + h.size - 1\n",
        "    N = 1 << (n - 1).bit_length()  # next pow2\n",
        "    X = np.fft.rfft(x.astype(np.float64), n=N)\n",
        "    H = np.fft.rfft(h.astype(np.float64), n=N)\n",
        "    y = np.fft.irfft(X * H, n=N)\n",
        "    y = y[:x.size]\n",
        "    return y.astype(np.float32)\n",
        "\n",
        "def synth_env_noise(preset: str = \"traffic\", seconds: int = 180, sr: int = TARGET_SR) -> np.ndarray:\n",
        "    n = int(seconds * sr)\n",
        "    # Base: white noise\n",
        "    x = np.random.normal(0.0, 1.0, size=n).astype(np.float32)\n",
        "    # Spectral shaping\n",
        "    shape = _make_shape(sr, n, preset=preset)\n",
        "    y = _fft_shape(x, sr, shape_mag=shape)\n",
        "    # Distant/room feel: light low-pass already in shape; add mild reverb\n",
        "    ir = _simple_reverb_ir(sr, rt60=0.25 if preset in (\"traffic\", \"hvac\") else 0.35)\n",
        "    y = _convolve(y, ir)\n",
        "    # Normalize RMS\n",
        "    rms = np.sqrt(np.maximum(np.mean(y**2), 1e-12))\n",
        "    y = (y / (rms + 1e-8)).astype(np.float32)\n",
        "    return y\n",
        "\n",
        "# Build ENV_NOISE according to the chosen preset\n",
        "ENV_NOISE = synth_env_noise(ENV_PRESET, seconds=ENV_SECONDS, sr=TARGET_SR)\n",
        "print(f\"[READY] ENV_NOISE preset='{ENV_PRESET}', length={ENV_NOISE.shape[0]} samples @ {TARGET_SR} Hz\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OstCR4o0_cER",
        "outputId": "a9a7f98c-4066-40c8-c1fc-d4f8858188d3"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[READY] ENV_NOISE preset='traffic', length=2880000 samples @ 16000 Hz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Install resampy (required by librosa.resample) ===\n",
        "# Comments are in English only.\n",
        "import sys, subprocess\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"resampy==0.4.3\"])\n",
        "print(\"resampy installed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4e2pp0VBm-s",
        "outputId": "8f06a0d7-bfe8-4dd8-b54f-576b6045e068"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resampy installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Batch Runner (force full overwrite): choose once ‚Üí clean output ‚Üí process ALL files ===\n",
        "# Comments are in English only.\n",
        "\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "# ---- Expected from previous cells ----\n",
        "# print_op_menu(), OP_MENU, apply_selected_manipulation()\n",
        "# TARGET_SR, TARGET_LEN, pad_trim, ENV_NOISE\n",
        "\n",
        "# ---- Config ----\n",
        "BASE_DIR    = Path(\"/content/my_audio_split\")   # make sure this is the ORIGINAL dataset root\n",
        "SPLITS      = [\"train\", \"val\", \"test\"]\n",
        "CLASSES     = [\"real\", \"fake\"]\n",
        "AUDIO_EXTS  = (\".wav\", \".flac\", \".mp3\", \".m4a\", \".ogg\")\n",
        "\n",
        "OUTPUT_BASE = Path(\"/content/tmp/augmented\")\n",
        "\n",
        "# Behavior: force-clean and overwrite everything on each run\n",
        "CLEAN_WHOLE_OUTPUT_ROOT = True   # delete OUTPUT_BASE entirely before writing\n",
        "SKIP_INPUTS_WITH_DUNDER = False  # process inputs even if their name already contains \"__\"\n",
        "OVERWRITE_OUTPUT        = True   # if file exists (shouldn't after clean), overwrite anyway\n",
        "\n",
        "# ---- Helpers ----\n",
        "def safe_read_audio(path: Path):\n",
        "    \"\"\"Read audio as (waveform float32, sample_rate int).\"\"\"\n",
        "    try:\n",
        "        wav, sr = sf.read(str(path), always_2d=False)\n",
        "        wav = np.asarray(wav).squeeze()\n",
        "        return wav.astype(np.float32), int(sr)\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] soundfile failed on {path.name}: {e} ‚Üí falling back to librosa.load\")\n",
        "        wav, sr = librosa.load(str(path), sr=None, mono=False)\n",
        "        if np.ndim(wav) > 1:\n",
        "            wav = np.mean(wav, axis=0)\n",
        "        return wav.astype(np.float32), int(sr)\n",
        "\n",
        "def list_audio_recursive(folder: Path):\n",
        "    \"\"\"Return all audio files (matching AUDIO_EXTS) recursively under 'folder'.\"\"\"\n",
        "    return sorted([p for p in folder.rglob(\"*\") if p.is_file() and p.suffix.lower() in AUDIO_EXTS])\n",
        "\n",
        "# ---- Choose the manipulation once ----\n",
        "print_op_menu()\n",
        "choice = input(\"Enter number (1-8) to apply to ALL files in ALL splits/classes: \").strip()\n",
        "if choice not in OP_MENU:\n",
        "    raise ValueError(f\"Invalid choice: {choice}. Expected one of {list(OP_MENU.keys())}.\")\n",
        "OP_CODE = OP_MENU[choice][0]\n",
        "print(f\"[INFO] Applying [{OP_MENU[choice][1]} | {OP_CODE}] to ALL files\")\n",
        "\n",
        "# ---- Clean the entire output tree first ----\n",
        "if CLEAN_WHOLE_OUTPUT_ROOT and OUTPUT_BASE.exists():\n",
        "    print(f\"[CLEAN] Removing entire output root: {OUTPUT_BASE}\")\n",
        "    shutil.rmtree(OUTPUT_BASE)\n",
        "OUTPUT_BASE.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---- Process everything ----\n",
        "total_written = 0\n",
        "for split in SPLITS:\n",
        "    for cls in CLASSES:\n",
        "        root_cls = BASE_DIR / split / cls\n",
        "        if not root_cls.exists():\n",
        "            print(f\"[WARN] Missing class folder: {root_cls} ‚Üí skipping\")\n",
        "            continue\n",
        "\n",
        "        p_folders = sorted([d for d in root_cls.iterdir() if d.is_dir() and d.name.startswith(\"p\")])\n",
        "        if not p_folders:\n",
        "            print(f\"[WARN] No p-folders under: {root_cls}\")\n",
        "            continue\n",
        "\n",
        "        for p_folder in p_folders:\n",
        "            files = list_audio_recursive(p_folder)\n",
        "            if not files:\n",
        "                print(f\"[WARN] {split}/{cls}/{p_folder.name}: 0 audio files ‚Üí skipped\")\n",
        "                continue\n",
        "\n",
        "            # Optional filtering (we do NOT skip dunder here)\n",
        "            to_process = files if not SKIP_INPUTS_WITH_DUNDER else [p for p in files if \"__\" not in p.stem]\n",
        "\n",
        "            out_p = OUTPUT_BASE / split / cls / p_folder.name\n",
        "            out_p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            print(f\"[INFO] {split}/{cls}/{p_folder.name}: processing {len(to_process)} files\")\n",
        "\n",
        "            for path in to_process:\n",
        "                out_path = out_p / f\"{path.stem}__{OP_CODE}.wav\"\n",
        "                if out_path.exists() and not OVERWRITE_OUTPUT:\n",
        "                    continue  # not expected after clean, but kept for safety\n",
        "\n",
        "                wav, sr = safe_read_audio(path)\n",
        "                if np.ndim(wav) > 1:\n",
        "                    wav = np.mean(wav, axis=0)\n",
        "                if sr != TARGET_SR:\n",
        "                    wav = librosa.resample(wav, orig_sr=sr, target_sr=TARGET_SR)\n",
        "                wav = pad_trim(wav, TARGET_LEN)\n",
        "\n",
        "                aug_wav, _ = apply_selected_manipulation(wav, OP_CODE, env_noise=ENV_NOISE)\n",
        "                sf.write(str(out_path), aug_wav, TARGET_SR)\n",
        "                total_written += 1\n",
        "\n",
        "print(\"\\n[DONE] Full overwrite run completed.\")\n",
        "print(f\"Total written: {total_written} ‚Üí {OUTPUT_BASE}\")\n"
      ],
      "metadata": {
        "id": "lMdf_eJPwrxR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0ec31f3-40f5-4e72-c046-0c139cda343b"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Choose one manipulation to apply (type the number):\n",
            "  1) Volume Control  [VC]\n",
            "  2) White-Noise SNR  [WN]\n",
            "  3) Environmental Noise (if ENV_NOISE is provided)  [ENV]\n",
            "  4) Fade In/Out  [FD]\n",
            "  5) Time Stretch  [TS]\n",
            "  6) Resample Variant  [RS]\n",
            "  7) Time Shift  [SF]\n",
            "  8) Echo  [EC]\n",
            "\n",
            "Enter number (1-8) to apply to ALL files in ALL splits/classes: 8\n",
            "[INFO] Applying [Echo | EC] to ALL files\n",
            "[CLEAN] Removing entire output root: /content/tmp/augmented\n",
            "[INFO] train/real/p225: processing 231 files\n",
            "[INFO] train/real/p227: processing 389 files\n",
            "[INFO] train/fake/p225: processing 231 files\n",
            "[INFO] train/fake/p227: processing 389 files\n",
            "[INFO] val/real/p226: processing 356 files\n",
            "[INFO] val/fake/p226: processing 356 files\n",
            "[INFO] test/real/p228: processing 366 files\n",
            "[INFO] test/fake/p228: processing 366 files\n",
            "\n",
            "[DONE] Full overwrite run completed.\n",
            "Total written: 2684 ‚Üí /content/tmp/augmented\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Dataset Fixed-Op Wrapper: force a specific manipulation for train only ===\n",
        "# Use this to train with ONE chosen manipulation applied to every training item.\n",
        "# It wraps your existing dataset without modifying it.\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class ManualAugDataset(Dataset):\n",
        "    \"\"\"Wrap an existing dataset and apply a fixed op in __getitem__ for split=='train'.\"\"\"\n",
        "    def __init__(self, base_dataset: Dataset, fixed_op_code: str, env_noise=None):\n",
        "        super().__init__()\n",
        "        self.base = base_dataset\n",
        "        self.fixed_op_code = fixed_op_code.upper().strip()\n",
        "        self.env_noise = env_noise\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        out = self.base[idx]\n",
        "        # Expecting base to return (waveform_tensor, label, maybe_meta)\n",
        "        if isinstance(out, (list, tuple)) and len(out) >= 2:\n",
        "            wav_t, label = out[0], out[1]\n",
        "        else:\n",
        "            raise RuntimeError(\"Base dataset must return at least (waveform_tensor, label).\")\n",
        "\n",
        "        wav = wav_t.numpy().astype(np.float32)\n",
        "        wav = pad_trim(wav, TARGET_LEN)\n",
        "\n",
        "        wav_fixed, code = apply_selected_manipulation(wav, self.fixed_op_code, env_noise=self.env_noise)\n",
        "        return torch.from_numpy(wav_fixed).float(), label, code\n"
      ],
      "metadata": {
        "id": "yEAr77-cwv2v"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CHECKPOINT SETUP"
      ],
      "metadata": {
        "id": "T1RDMw-EwDDY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "4KfMrNLgbP-x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "outputId": "a61f767d-6347-433f-b703-6547ed48a63c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
            "  def dispatcher(\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Dimension D inferred: 1024\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "type object 'GuardSource' has no attribute 'LOCAL_NN_MODULE'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1671355221.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;31m# üõë Optimize BOTH encoder and classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m optimizer = torch.optim.AdamW(\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, maximize, foreach, capturable, differentiable, fused)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Invalid beta parameter at index 0: {betas[0]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Invalid beta parameter at index 1: {betas[1]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused, decoupled_weight_decay)\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0;31m# Lazy state initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m    398\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlist_backends\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregister_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallback_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_compile_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_compile_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcode_context\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcode_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_traceback\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mformat_traceback_short\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace_rules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCompilerFn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbytecode_analysis\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mremove_dead_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_pointless_jumps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/trace_rules.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgetfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhashable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNP_SUPPORTED_MODULES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munwrap_if_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m from .variables import (\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0mBuiltinVariable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mFunctorchHigherOrderVariable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# mypy: ignore-errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariableTracker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbuiltin\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBuiltinVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConstantVariable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEnumVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_scope_id\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcurrent_scope_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0munimplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAttrSource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSource\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mistype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/source.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# so those cases are omitted intentionally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m _GUARD_SOURCE_NN_MODULE = {\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mGuardSource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLOCAL\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mGuardSource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLOCAL_NN_MODULE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mGuardSource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGLOBAL\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mGuardSource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGLOBAL_NN_MODULE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mGuardSource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLOCAL_NN_MODULE\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mGuardSource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLOCAL_NN_MODULE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: type object 'GuardSource' has no attribute 'LOCAL_NN_MODULE'"
          ]
        }
      ],
      "source": [
        "# üõ†Ô∏è CHECKPOINT SETUP\n",
        "CHECKPOINT_PATH = \"/content/best_clad_model.pth\"\n",
        "best_val_acc = 0.0\n",
        "\n",
        "#1 =========================================================\n",
        "# Full End-to-End Fine-tuning of RawNet Encoder + Classifier\n",
        "# (Training from Scratch since checkpoint is incompatible)\n",
        "# =========================================================\n",
        "from pathlib import Path\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchaudio\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Change directory to CLAD to import modules\n",
        "os.chdir('/content/CLAD')\n",
        "\n",
        "# We only need the RawNet Encoder for classification.\n",
        "from Model import RawNetEncoderBaseline # Keep only the encoder import\n",
        "\n",
        "# --- ARCHITECTURE CONFIG (Matching the incompatible checkpoint's width) ---\n",
        "# This configuration is used for the RawNet model architecture.\n",
        "d_args = {\n",
        "    \"in_channels\": 1,\n",
        "    \"first_conv\": 251,\n",
        "    \"filts\": [\n",
        "        32,           # Narrower filter set to align with common RawNet variants\n",
        "        [32, 32],\n",
        "        [32, 64],\n",
        "        [64, 64]\n",
        "    ],\n",
        "    \"nb_fc_node\": 1024,\n",
        "    \"gru_node\": 1024,\n",
        "    \"nb_gru_layer\": 3,\n",
        "    \"nb_classes\": 2\n",
        "}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- MODEL INSTANTIATION ---\n",
        "# 1. Create a single RawNet encoder (no MoCo wrapper needed for classification)\n",
        "encoder = RawNetEncoderBaseline(d_args, device)\n",
        "encoder.to(device)\n",
        "\n",
        "# üõë NO CHECKPOINT LOADING: We are training from random initialization.\n",
        "\n",
        "# -------- Hyperparameters --------\n",
        "TARGET_SR  = 16000\n",
        "TARGET_LEN = 64600\n",
        "LABELS     = {\"real\": 0, \"fake\": 1}\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS     = 15 # Increased epochs slightly for training from scratch\n",
        "LR         = 5e-5 # Low learning rate for complex architecture training\n",
        "# ---------------------------------\n",
        "\n",
        "# -------- Dataset (No change to class definition is needed) --------\n",
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, root_dir: str):\n",
        "        self.samples = []\n",
        "        root = Path(root_dir)\n",
        "        for cls in (\"real\", \"fake\"):\n",
        "            base = root / cls\n",
        "            if not base.exists():\n",
        "                # Removed the raise to allow running if a folder is empty,\n",
        "                # though it should exist after split cell 17.\n",
        "                continue\n",
        "            for p in base.rglob(\"*.wav\"):\n",
        "                if p.is_file():\n",
        "                    self.samples.append((p, LABELS[cls]))\n",
        "        if len(self.samples) == 0:\n",
        "            raise ValueError(f\"No .wav files found under {root_dir}.\")\n",
        "        self._resamplers = {}\n",
        "\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        # Since audio is preprocessed to 16kHz/64600 in cell 19,\n",
        "        # we can simplify this function to rely on that.\n",
        "        wav, sr = torchaudio.load(str(path)) # [C, T]\n",
        "        return wav.squeeze(0), label         # -> [T], label\n",
        "\n",
        "# -------- DataLoaders --------\n",
        "train_ds = AudioDataset(\"/content/my_audio_split/train\")\n",
        "val_ds   = AudioDataset(\"/content/my_audio_split/val\")\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
        "\n",
        "\n",
        "# --- Infer feature dimension D and define Classifier ---\n",
        "# Inference on one batch to set classifier dimension\n",
        "with torch.no_grad():\n",
        "    wavs0, _ = next(iter(train_loader))     # wavs0: [B, T]\n",
        "    x0 = wavs0.to(device).float()           # [B, T]\n",
        "    feat0 = encoder(x0)                     # RawNetEncoderBaseline(d_args) expects [B, T]\n",
        "\n",
        "    # Pool any extra time/freq dims\n",
        "    if feat0.dim() == 3:\n",
        "        feat0 = feat0.mean(dim=2)            # pool time -> [B, D]\n",
        "    elif feat0.dim() > 3:\n",
        "        feat0 = feat0.mean(dim=tuple(range(2, feat0.dim())))\n",
        "    D = feat0.shape[1] # D should be 1024\n",
        "\n",
        "print(f\"Feature Dimension D inferred: {D}\")\n",
        "# -----------------------------------------------------\n",
        "\n",
        "# -------- Classifier, Loss, Optimizer (Optimizing the whole system) --------\n",
        "classifier = nn.Sequential(\n",
        "    nn.Linear(D, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(64, 2)\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# üõë Optimize BOTH encoder and classifier\n",
        "optimizer = torch.optim.AdamW(\n",
        "    list(encoder.parameters()) + list(classifier.parameters()),\n",
        "    lr=LR,\n",
        "    weight_decay=1e-4\n",
        ")\n",
        "\n",
        "# üõ†Ô∏è MISSING/FIXED: Define the Learning Rate Scheduler\n",
        "# T_max is the total number of updates (epochs * steps_per_epoch)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS * len(train_loader))\n",
        "\n",
        "\n",
        "# -------- Train + Validate --------\n",
        "print(f\"Starting End-to-End Training (Encoder + Classifier) for {EPOCHS} epochs.\")\n",
        "\n",
        "# A simple loop for demonstration (removed the single-batch overfit check to clean up)\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    # ---- Train ----\n",
        "    encoder.train()\n",
        "    classifier.train()\n",
        "    tr_loss = tr_correct = tr_total = 0.0\n",
        "\n",
        "    for wavs, labels in train_loader:\n",
        "        x = wavs.to(device).float()          # [B, T]\n",
        "        y = labels.to(device).long()         # 0=real, 1=fake\n",
        "\n",
        "        # Encoder forward (WITH grad)\n",
        "        f = encoder(x) # (B, D) or (B, D, T, ...)\n",
        "        if f.dim() == 3:\n",
        "            f = f.mean(dim=2)            # pool time -> (B, D)\n",
        "        elif f.dim() > 3:\n",
        "            f = f.mean(dim=tuple(range(2, f.dim())))\n",
        "\n",
        "        # Head forward\n",
        "        logits = classifier(f)\n",
        "        loss = criterion(logits, y)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        tr_loss   += loss.item() * x.size(0)\n",
        "        tr_correct += (logits.argmax(dim=1) == y).sum().item()\n",
        "        tr_total  += x.size(0)\n",
        "\n",
        "   # ---- Val ----\n",
        "    classifier.eval()\n",
        "    encoder.eval() # Ensure encoder is also in eval mode\n",
        "    va_loss, va_correct, va_total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "      # ... (Existing validation loop to calculate va_loss and va_correct/va_total) ...\n",
        "      for wavs, labels in val_loader:\n",
        "          x = wavs.to(device).float()\n",
        "          y = labels.to(device).long()\n",
        "\n",
        "          f = encoder(x)\n",
        "          if f.dim() == 3:\n",
        "              f = f.mean(dim=2)\n",
        "          elif f.dim() > 3:\n",
        "              f = f.mean(dim=tuple(range(2, f.dim())))\n",
        "\n",
        "          logits = classifier(f)\n",
        "          va_loss   += criterion(logits, y).item() * x.size(0)\n",
        "          va_correct += (logits.argmax(dim=1) == y).sum().item()\n",
        "          va_total  += x.size(0)\n",
        "\n",
        "    current_val_acc = va_correct / va_total\n",
        "\n",
        "    print(f\"Epoch {epoch}/{EPOCHS} | \"\n",
        "          f\"Train Loss: {tr_loss/tr_total:.4f}, Acc: {tr_correct/tr_total:.3f} | \"\n",
        "          f\"Val Loss: {va_loss/va_total:.4f}, Acc: {current_val_acc:.3f}\")\n",
        "\n",
        "    # üõ†Ô∏è CHECKPOINTING / EARLY STOPPING LOGIC\n",
        "    global best_val_acc\n",
        "\n",
        "    if current_val_acc > best_val_acc:\n",
        "        print(f\"  --> New best model found (Acc: {current_val_acc:.4f}). Saving checkpoint to {CHECKPOINT_PATH}\")\n",
        "        best_val_acc = current_val_acc\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'encoder_state_dict': encoder.state_dict(),\n",
        "            'classifier_state_dict': classifier.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'best_val_acc': best_val_acc,\n",
        "        }, CHECKPOINT_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYNs_SB4L1b1"
      },
      "outputs": [],
      "source": [
        "# ===============================================\n",
        "# üîÑ Load Best Model State from Checkpoint\n",
        "# ===============================================\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "CHECKPOINT_PATH = \"/content/best_clad_model.pth\"\n",
        "\n",
        "if Path(CHECKPOINT_PATH).exists():\n",
        "    print(f\"Loading best checkpoint from: {CHECKPOINT_PATH}\")\n",
        "    checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n",
        "\n",
        "    # Load state dictionaries into the previously defined models\n",
        "    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
        "    classifier.load_state_dict(checkpoint['classifier_state_dict'])\n",
        "\n",
        "    # Restore optimizer state if needed for continued training (optional here)\n",
        "    # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    print(f\"‚úÖ Successfully loaded model from Epoch {checkpoint['epoch']} (Best Val Acc: {checkpoint['best_val_acc']:.4f})\")\n",
        "\n",
        "    # Set models to evaluation mode before proceeding\n",
        "    encoder.eval()\n",
        "    classifier.eval()\n",
        "\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Checkpoint file not found. Proceeding with the final state of the training run.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjMcDM3hbTXb"
      },
      "outputs": [],
      "source": [
        "#2 =========================================\n",
        "# Evaluate model on validation/test sets\n",
        "# =========================================\n",
        "from sklearn.metrics import classification_report\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader\n",
        "# Assuming AudioDataset is defined and model/classifier are trained from cell 21\n",
        "\n",
        "def evaluate(loader, split_name=\"val\", encoder=None, classifier=None, device=None):\n",
        "    if encoder is None or classifier is None or device is None:\n",
        "        raise ValueError(\"encoder, classifier, and device must be provided to evaluate function.\")\n",
        "\n",
        "    encoder.to(device).eval()\n",
        "    classifier.to(device).eval()\n",
        "\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for wavs, labels in loader:\n",
        "            wavs, labels = wavs.to(device).float(), labels.to(device).long()\n",
        "\n",
        "            # Use the single encoder\n",
        "            feats = encoder(wavs)\n",
        "            # Pool any extra time/freq dims so we have (B,D)\n",
        "            if feats.dim() == 3:\n",
        "                feats = feats.mean(dim=2)\n",
        "            elif feats.dim() > 3:\n",
        "                feats = feats.mean(dim=tuple(range(2, feats.dim())))\n",
        "\n",
        "            preds = classifier(feats)\n",
        "            all_preds.extend(torch.argmax(preds, dim=1).cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    print(f\"\\n=== {split_name.upper()} RESULTS ===\\n\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=[\"real\",\"fake\"], digits=4))\n",
        "\n",
        "# Define the test loader (ensure AudioDataset is defined in cell 21)\n",
        "test_ds = AudioDataset(\"/content/my_audio_split/test\")\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Run evaluation on Validation and Test sets\n",
        "evaluate(val_loader, \"val\", encoder=encoder, classifier=classifier, device=device)\n",
        "evaluate(test_loader, \"test\", encoder=encoder, classifier=classifier, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvNxJXpebWE5"
      },
      "outputs": [],
      "source": [
        "#13 =========================================\n",
        "# Prediction and Embedding Extraction\n",
        "# =========================================\n",
        "import torch\n",
        "from pathlib import Path\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'encoder', 'classifier', and 'device' are defined and trained\n",
        "if 'encoder' not in globals() or 'classifier' not in globals() or 'device' not in globals():\n",
        "     raise RuntimeError(\"Encoder, classifier, or device not found. Please run the training cells first.\")\n",
        "\n",
        "# Ensure encoder and classifier are on the correct device and in eval mode\n",
        "encoder.to(device).eval()\n",
        "classifier.to(device).eval()\n",
        "\n",
        "TARGET_SR  = 16000\n",
        "TARGET_LEN = 64600\n",
        "\n",
        "# Function to load audio as 16k mono and preprocess\n",
        "def load_and_preprocess_audio(path: Path):\n",
        "    \"\"\"Load an audio file, resample to 16kHz mono, pad/trim to TARGET_LEN.\"\"\"\n",
        "    # Since audio files are already preprocessed in cell 19, we load directly\n",
        "    wav, sr = torchaudio.load(str(path))   # [C,T]\n",
        "    return wav.squeeze(0) # -> [T]\n",
        "\n",
        "# Function to run prediction\n",
        "def predict_file(path: Path, encoder, classifier, device):\n",
        "    \"\"\"Run encoder + classifier on one WAV file. Returns (pred_label, fake_conf, real_conf, embedding).\"\"\"\n",
        "    try:\n",
        "        wav = load_and_preprocess_audio(path) # [T]\n",
        "        x = wav.unsqueeze(0).to(device).float()       # [B=1, T]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            feats = encoder(x) # RawNet expects [B, T]\n",
        "\n",
        "            # Pool any extra time/freq dims if necessary\n",
        "            if feats.dim() == 3:\n",
        "                feats = feats.mean(dim=2)\n",
        "            elif feats.dim() > 3:\n",
        "                 feats = feats.mean(dim=tuple(range(2, feats.dim())))\n",
        "\n",
        "            # Pass features through the classifier\n",
        "            logits = classifier(feats)        # [B=1, 2]\n",
        "            probs  = F.softmax(logits, dim=1)[0].detach().cpu().numpy()\n",
        "\n",
        "        label = \"fake\" if int(np.argmax(probs)) == 1 else \"real\"\n",
        "        fake_conf = float(probs[1])\n",
        "        real_conf = float(probs[0])\n",
        "        # Return the embedding from the encoder (for t-SNE)\n",
        "        return label, fake_conf, real_conf, feats.squeeze(0).cpu().numpy()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {path}: {e}\")\n",
        "        return \"error\", 0.0, 0.0, None # Return None for embedding on error\n",
        "\n",
        "\n",
        "REAL_DIR = Path(\"/content/my_audio/real\")\n",
        "FAKE_DIR = Path(\"/content/my_audio/fake\")\n",
        "OUT_DIR  = Path(\"/content/my_audio_results\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "real_files = sorted(REAL_DIR.rglob(\"*.wav\"))\n",
        "fake_files = sorted(FAKE_DIR.rglob(\"*.wav\"))\n",
        "\n",
        "print(f\"Found {len(real_files)} real and {len(fake_files)} fake WAVs.\")\n",
        "\n",
        "rows = []\n",
        "all_embeddings = [] # List to collect embeddings for t-SNE\n",
        "\n",
        "for f in real_files + fake_files:\n",
        "    p = Path(f)\n",
        "    pred, fake_conf, real_conf, emb = predict_file(p, encoder, classifier, device)\n",
        "\n",
        "    if emb is not None:\n",
        "        all_embeddings.append(emb)\n",
        "        emb_list = emb.tolist()\n",
        "        emb_dim = len(emb)\n",
        "    else:\n",
        "        emb_list = None\n",
        "        emb_dim = None\n",
        "\n",
        "    rows.append({\n",
        "        \"file\": str(p),\n",
        "        \"pred\": pred,\n",
        "        \"fake_conf\": fake_conf,\n",
        "        \"real_conf\": real_conf,\n",
        "        \"embedding_dim\": emb_dim,\n",
        "        \"embedding\": emb_list\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "csv_path = OUT_DIR / \"clad_results.csv\"\n",
        "df.to_csv(csv_path, index=False)\n",
        "print(\"Saved CSV:\", csv_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XS7d1xz3bYOt"
      },
      "outputs": [],
      "source": [
        "#14 =========================================\n",
        "# t-SNE Visualization of Embeddings\n",
        "# =========================================\n",
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# The 'rows' list is assumed to be available from the previous cell's execution\n",
        "emb_rows = [r for r in rows if r[\"embedding\"] is not None]\n",
        "\n",
        "if len(emb_rows) >= 2:\n",
        "    # Separate embeddings and predicted labels\n",
        "    X = np.vstack([np.array(r[\"embedding\"], dtype=np.float32) for r in emb_rows])\n",
        "    y = np.array([r[\"pred\"] for r in emb_rows])\n",
        "\n",
        "    # Run t-SNE\n",
        "    # Perplexity should be < N_samples.\n",
        "    # Use 15 for your small dataset of ~2700 total samples (real+fake).\n",
        "    perp = min(30, len(emb_rows)-1)\n",
        "    print(f\"Running t-SNE with perplexity={perp}...\")\n",
        "\n",
        "    # Trigger image generation for better visualization\n",
        "\n",
        "    X2d = TSNE(n_components=2, random_state=0, perplexity=perp, init='random', learning_rate='auto').fit_transform(X)\n",
        "\n",
        "    plt.figure(figsize=(6,5))\n",
        "    for cls, marker, color in [(\"real\", \"o\", \"blue\"), (\"fake\", \"x\", \"red\")]:\n",
        "        mask = (y == cls)\n",
        "        plt.scatter(X2d[mask,0], X2d[mask,1], label=cls, marker=marker, alpha=0.85, color=color)\n",
        "\n",
        "    plt.title(\"CLAD Embeddings (t-SNE) - Classification by Model\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No embeddings available (encoder not exposed or too few samples).\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "cell_execution_strategy": "setup",
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}